name: L2H4-eval-loss-fn
method: grid
command: ["python", "icl/experiments/sweep_over_final.py", "wandb"]
project: icl-llc
entity: devinterp
parameters:
  sampler_config:
    parameters: 
      num_chains:
        value: 10
      num_draws:
        value: 1000
      sampling_method:
        value: sgld
      grad_batch_origin:
        value: "eval-dataset"
      grad_batch_size:
        value: 1024
      # 
      gradient_scale:
        value: 0.00001
      localization_scale:
        value: 0.003
      noise_scale:
        value: 0.00003 
      # 
      eval_method:
        value: 'grad-minibatch'      
      eval_batch_size:
        value: 1024
      eval_dataset_size:
        value: 1048576
      eval_metrics:
        value: ["likelihood-derived", "batch-loss"]
      eval_online:
        value: True
      eval_loss_fn:
        values: ["subsequence-mse", "mse"]

  plotting_config:
    parameters:
      include_loss_trace: 
        value: True
      include_weights_pca: 
        value: False


  task_config: 
    parameters:
      task_size:
        value: 4
      max_examples:
        value: 8
      num_tasks:
        value: 'inf'
      noise_variance:
        value: 0.125
      embed_size:
        value: 64
      mlp_size:
        value: 64
      num_heads:
        value: 4
      num_layers:
        value: 2
      model_seed:
        values: [0] #, 1, 2, 3, 4]
      pretrain_seed:
        value: null
      true_seed:
        value: null
      sampling_seed:
        value: null
      layer_norm:
        values: [true, false]
      include_output: 
        value: true
        
  optimizer_config:
    parameters:
      lr:
        value: 0.003

  step:
    values: [10101, 499999]