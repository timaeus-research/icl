name: L2H4-eval-fixed-batch
method: grid
command: ["python", "icl/experiments/sweep_over_final.py", "wandb"]
project: icl-llc
entity: devinterp
parameters:
  sampler_config:
    parameters: 
      num_chains:
        value: 8
      num_draws:
        value: 2000
      sampling_method:
        value: sgld
      grad_batch_origin:
        value: "eval-dataset"
      grad_batch_size:
        value: 1024
      #
      gradient_scale:
        value: 0.01
      localization_scale:
        value: 0.001
      noise_scale:
        value: 0.0003
      #
      eval_method:
        value: 'fixed-minibatch'
      eval_batch_size:
        values: [1024, 2048, 4096, 8192, 16384]
      eval_dataset_size:
        value: 1048576
      eval_metrics:
        value: ["likelihood-derived", "batch-loss", "weights"]
      eval_online:
        value: True
      eval_loss_fn:
        value: "mse"
        # values: ["subsequence-mse", "mse"]

  plotting_config:
    parameters:
      include_loss_trace: 
        value: True
      include_weights_pca: 
        value: True
      num_components: 
        value: 3
      num_points: 
        value: 10

  task_config: 
    parameters:
      task_size:
        value: 4
      max_examples:
        value: 8
      num_tasks:
        value: 'inf'
      noise_variance:
        value: 0.125
      embed_size:
        value: 64
      mlp_size:
        value: 64
      num_heads:
        value: 4
      num_layers:
        value: 2
      model_seed:
        values: [0] #, 1, 2, 3, 4]
      pretrain_seed:
        value: null
      true_seed:
        value: null
      sampling_seed:
        value: null
      layer_norm:
        values: [true]
      include_output: 
        value: true
        
  optimizer_config:
    parameters:
      lr:
        value: 0.003

  step:
    values:  [507, 1124, 9375, 499999, 3247, 27072, 59968, 121212]