{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $M\\to\\infty$ Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# import sys\n",
    "# del sys.modules['icl.figures.colors']\n",
    "# del sys.modules['icl.figures.notation']\n",
    "\n",
    "from icl.analysis.utils import get_unique_run\n",
    "from icl.constants import ANALYSIS, FIGURES, SWEEPS\n",
    "from icl.figures.notation import str_d_dlogt, str_d_dt, str_dlog_dlogt\n",
    "from icl.figures.colors import plot_transitions, gen_transition_colors, get_transition_type, PRIMARY, SECONDARY, TERTIARY, BRED, BBLUE, BRED, BGREEN\n",
    "from icl.setup import DEVICE\n",
    "\n",
    "MODEL_ID = \"L2H4Minf\"\n",
    "LLC_RUN_ID = \"5e0cq1db\"  # TODO: Placeholder from m=20\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TASKS = 2 ** 20  # TODO: Placeholder\n",
    "NUM_LAYERS = 2\n",
    "MAX_LR = 0.01\n",
    "\n",
    "# shorthands\n",
    "BATCH_SIZE = 8192\n",
    "K = 8\n",
    "D = 4\n",
    "\n",
    "run = get_unique_run(\n",
    "    str(SWEEPS / \"training-runs/small-L-2.yaml\"), \n",
    "    task_config={\"num_tasks\": NUM_TASKS, \"num_layers\": NUM_LAYERS},\n",
    "    optimizer_config={\"lr\": MAX_LR}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy \n",
    "# Let's generate these same plots and also look at their evolution. \n",
    "models = []\n",
    "optimizer_state_dicts = []\n",
    "\n",
    "steps = run.checkpointer.file_ids\n",
    "\n",
    "for checkpoint in tqdm.tqdm(run.checkpointer):\n",
    "    m = deepcopy(run.model)\n",
    "    m.load_state_dict(checkpoint[\"model\"])\n",
    "    models.append(m)\n",
    "    optimizer_state_dicts.append(checkpoint[\"optimizer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at how the curvature of the llc changes over time. \n",
    "\n",
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "llc_run = api.run(f\"devinterp/icl/{LLC_RUN_ID}\")\n",
    "history_df = llc_run.history()\n",
    "\n",
    "llc_steps = history_df[\"_step\"]\n",
    "llcs = history_df[\"llc/mean\"]\n",
    "llcs_std = history_df[\"llc/std\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.figures.derivatives import d_dt, d_dlogt, dlog_dlogt\n",
    "\n",
    "weight_norms = [(sum(torch.norm(p) ** 2 for p in model.parameters()) ** 0.5).item() for model in models]\n",
    "\n",
    "d_llc_dt = d_dt(steps, llcs)\n",
    "d_llc_dlogt = d_dlogt(steps, llcs)\n",
    "\n",
    "d_weight_norm_dt = d_dt(run.checkpointer.file_ids, weight_norms)\n",
    "d_weight_norm_dlogt = d_dlogt(run.checkpointer.file_ids, weight_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.evals import ICLEvaluator\n",
    "\n",
    "FORCE_REEVAL = True\n",
    "\n",
    "evaluator = ICLEvaluator(\n",
    "    pretrain_dist=run.pretrain_dist,\n",
    "    true_dist=run.true_dist,\n",
    "    max_examples=run.config.task_config.max_examples,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    seed=run.config.task_config.true_seed,   \n",
    ")\n",
    "\n",
    "if os.path.exists(ANALYSIS / f\"{MODEL_ID}_evals_over_time.csv\") and not FORCE_REEVAL:\n",
    "    evals_over_time_df = pd.read_csv(ANALYSIS / f\"{MODEL_ID}_evals_over_time.csv\")\n",
    "    evals_over_time = evals_over_time_df.to_dict(\"records\")\n",
    "else:\n",
    "    evals_over_time = [evaluator(model) for model in models]\n",
    "    evals_over_time_df = pd.DataFrame(evals_over_time)\n",
    "    evals_over_time_df.to_csv(ANALYSIS / f\"{MODEL_ID}_evals_over_time.csv\")\n",
    "\n",
    "evals_over_time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_over_time_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "# TRANSITIONS = [\n",
    "#     (30, 1200, 'A1'),\n",
    "#     (1200, 12500, 'A2'),\n",
    "#     (12500, 60000, 'B1'),\n",
    "#     (60000, 110000, 'B2'),\n",
    "#     (110000, 180000, 'B3'),\n",
    "#     (180000, 280000, \"B4\"),\n",
    "#     (280000, 320000, \"B5\"),\n",
    "#     (320000, 500000, \"B6\")\n",
    "# ]\n",
    "\n",
    "\n",
    "TRANSITIONS = [\n",
    "    (30, 800, 'A1'),\n",
    "    (800, 10000, 'A2'),\n",
    "    (10000, 28000, 'B1'),\n",
    "    (28000, 300000, 'B2'),\n",
    "    # (110000, 180000, 'B3'),\n",
    "    # (180000, 280000, \"B4\"),\n",
    "    # (280000, 320000, \"B5\"),\n",
    "    # (320000, 500000, \"B6\")\n",
    "]\n",
    "\n",
    "# def gen_transition_colors(types):\n",
    "#     \"\"\"Generates a palette for transition colors. Orange-flavored for Type A. Blue-flavored for Type B.\"\"\"\n",
    "#     num_type_a = sum([t == \"A\" for t in types])\n",
    "#     num_type_b = sum([t == \"B\" for t in types])\n",
    "#     num_other = sum([t == \"Other\" for t in types])\n",
    "\n",
    "#     type_a_palette = sns.color_palette(\"Oranges_r\", num_type_a)\n",
    "#     type_b_palette = sns.color_palette(\"Blues_r\", num_type_b)\n",
    "#     other_palette = sns.color_palette(\"Greys_r\", num_other)\n",
    "\n",
    "#     palette = []\n",
    "\n",
    "#     for t in types:\n",
    "#         if t == \"A\":\n",
    "#             palette.append(type_a_palette.pop())\n",
    "#         elif t == \"B\":\n",
    "#             palette.append(type_b_palette.pop())\n",
    "#         else:\n",
    "#             palette.append(other_palette.pop())\n",
    "\n",
    "#     return palette\n",
    "\n",
    "\n",
    "def increase_saturation(rgb, saturation_factor):\n",
    "    # Convert RGB to HSV\n",
    "    hsv = colorsys.rgb_to_hsv(*rgb)\n",
    "    \n",
    "    # Increase saturation by the given factor, making sure it stays in [0, 1]\n",
    "    new_s = min(max(hsv[1] * saturation_factor, 0), 1)\n",
    "    \n",
    "    # Convert back to RGB\n",
    "    new_rgb = colorsys.hsv_to_rgb(hsv[0], new_s, hsv[2])\n",
    "    return new_rgb\n",
    "\n",
    "\n",
    "def increase_contrast(rgb, contrast_factor):\n",
    "    # Midpoint\n",
    "    midpoint = 128.0 / 255\n",
    "    \n",
    "    # Increase contrast\n",
    "    new_rgb = [(0.5 + contrast_factor * (component - 0.5)) for component in rgb]\n",
    "    \n",
    "    # Clip to the range [0, 1]\n",
    "    new_rgb = [min(max(component, 0), 1) for component in new_rgb]\n",
    "    return new_rgb\n",
    "\n",
    "\n",
    "\n",
    "transition_types = [get_transition_type(t) for t in TRANSITIONS]\n",
    "transition_colors = gen_transition_colors(transition_types)\n",
    "\n",
    "transition_colors = [increase_saturation(rgb, 2) for rgb in transition_colors]\n",
    "transition_colors = [increase_contrast(rgb, 2) for rgb in transition_colors]\n",
    "\n",
    "transitions_cmap = LinearSegmentedColormap.from_list(\"transitions\", transition_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_plot = [\n",
    "    # (r\"L_{\\mathcal{T}}(t)\", evals_over_time_df[\"pretrain/mse_subsequence\"], {\"logy\": True}, ),\n",
    "    # (r\"L_\\mathcal{G}(t)\", evals_over_time_df[\"true/mse\"], {\"logy\": False}),\n",
    "    # (r\"\\hat \\lambda(t)\", llcs, {}),\n",
    "    (r\"|\\theta(t)|\", weight_norms, {\"derivative\": \"dlog_dlogt\", \"logy\": True}),\n",
    "] \n",
    "\n",
    "fig, axes = plt.subplots(2, len(metrics_to_plot), figsize=(12, 6))\n",
    "\n",
    "axes = axes.reshape(2, len(metrics_to_plot))\n",
    "\n",
    "for i, (metric_name, metric_values, kwargs) in enumerate(metrics_to_plot):\n",
    "    axes[0, i].plot(run.checkpointer.file_ids, metric_values, label=metric_name, marker='.')\n",
    "    axes[0, i].set_title(f\"${metric_name}$\")\n",
    "    axes[0, i].set_xlabel('Step, $t$')\n",
    "    axes[0, i].set_ylabel(metric_name)\n",
    "\n",
    "    if kwargs.get(\"logy\", False):\n",
    "        axes[0, i].set_yscale('log')\n",
    "\n",
    "    slope_type = kwargs.get(\"derivative\", \"d_dlogt\")\n",
    "\n",
    "    if slope_type == \"d_dlogt\":\n",
    "        slope = d_dlogt(run.checkpointer.file_ids, metric_values)\n",
    "        slope_name = str_d_dlogt(metric_name)\n",
    "    elif slope_type == \"d_dt\":\n",
    "        slope = d_dt(run.checkpointer.file_ids, metric_values)\n",
    "        slope_name = str_d_dt(metric_name)\n",
    "    elif slope_type == \"dlog_dlogt\":\n",
    "        slope = dlog_dlogt(run.checkpointer.file_ids, metric_values)\n",
    "        slope_name = str_dlog_dlogt(metric_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown slope type {slope_type}\")\n",
    "\n",
    "    axes[1, i].plot(run.checkpointer.file_ids, slope, label=metric_name + \" Slope\", marker='.')\n",
    "    axes[1, i].set_title(slope_name)\n",
    "    axes[1, i].set_xlabel('Step, $t$')\n",
    "    axes[1, i].set_ylabel(slope_name)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(30, 500_000)\n",
    "\n",
    "\n",
    "# axes[0, 1].set_ylim(0, 100)\n",
    "# axes[1, 1].set_ylim(0, 200)\n",
    "# axes[1,1].set_ylim(-2, 2)\n",
    "\n",
    "patch_list = plot_transitions(axes, TRANSITIONS, limit=True)\n",
    "\n",
    "milestone_labels = [label for _, _, label in TRANSITIONS]\n",
    "fig.legend(patch_list, milestone_labels, loc='upper center', bbox_to_anchor=(0.5, -0.025), ncol=len(TRANSITIONS))\n",
    "\n",
    "fig.set_facecolor(\"white\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Prediction Norm, OOD Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait if my layer norm theory is right. Then we should see a sudden improvement in the ability of the model to make predictions for out-of-distribution xs/ys (not ws). \n",
    "from devinfra.utils.seed import set_seed\n",
    "from icl.tasks import apply_transformations\n",
    "from devinfra.utils.iterables import flatten_dict\n",
    "\n",
    "pretrain_dist_noiseless = run.config.task_config.pretrain_dist_factory().to(\n",
    "    DEVICE\n",
    ")\n",
    "pretrain_dist_noiseless.std = 0.\n",
    "\n",
    "set_seed(run.config.task_config.pretrain_seed)\n",
    "\n",
    "\n",
    "# sample a batch of random tasks\n",
    "ws = pretrain_dist_noiseless.task_distribution.sample_tasks(BATCH_SIZE) # -> B D\n",
    "\n",
    "# sample i.i.d. inputs and outputs for each task according to the\n",
    "# regression model\n",
    "xs = torch.normal(\n",
    "    mean=0.,\n",
    "    std=1.,\n",
    "    size=(BATCH_SIZE, K, D,),\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "OOD_MULTIPLIER = 3\n",
    "\n",
    "ood_xs = 100 * xs\n",
    "ys = apply_transformations(ws, xs, 0.125, DEVICE)\n",
    "ood_ys = apply_transformations(ws, ood_xs, 0.125, DEVICE)\n",
    "\n",
    "def eval_loss(yhats, ys):\n",
    "    losses = ((yhats - ys) ** 2).mean(dim=0)[:, 0]\n",
    "    return [loss.item() for loss in losses] + [losses.mean().item()]\n",
    "\n",
    "losses_over_time = []\n",
    "\n",
    "for step, model in zip(run.checkpointer.file_ids, models):\n",
    "    losses = eval_loss(model(xs, ys), ys)\n",
    "    ood_losses = eval_loss(model(ood_xs, ood_ys), ood_ys)\n",
    "    losses_0s = eval_loss(model(xs, ys), torch.zeros_like(ys))\n",
    "\n",
    "    for i in range(9):\n",
    "        losses_over_time.append({\n",
    "            \"step\": step,\n",
    "            \"loss\": losses[i],\n",
    "            \"ood_loss\": ood_losses[i],\n",
    "            \"loss_0\": losses_0s[i],\n",
    "            # \"token\": f\"$\\hat y_{i+1}$\" if i < 8 else \"$\\overline{\\hat y}$\"\n",
    "            \"token\": i + 1 if i < 8 else \"$\\overline{\\hat y}$\"\n",
    "        })\n",
    "\n",
    "losses_over_time = pd.DataFrame(losses_over_time)\n",
    "losses_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-context learning score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "\n",
    "LINE_PALETTE=\"viridis\"\n",
    "ALPHA=1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "sns.lineplot(data=losses_over_time, x=\"step\", y=\"loss\", hue=\"token\", palette=LINE_PALETTE, alpha=ALPHA, ax=ax)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Step, $t$\")\n",
    "ax.set_ylabel(\"$L_\\mathrm{val}$\")\n",
    "ax.legend().remove()\n",
    "\n",
    "plot_transitions(ax, TRANSITIONS, limit=True) #, alpha=0.25)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.7])  # Adjust as necessary for position and size\n",
    "\n",
    "# custom_cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", LINE_PALETTE)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=1, vmax=8), )\n",
    "sm._A = []  # Dummy array for the ScalarMappable. \n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "tick_positions = range(1, len(LINE_PALETTE)+1)  # Positions for each color\n",
    "tick_labels = [f\"${i}$\" for i in range(1, len(LINE_PALETTE) + 1)] # Replace with your labels\n",
    "cbar.set_ticks(tick_positions)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "cbar.set_label(\"$k$\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.91, 1])  # Adjust layout to make room for colorbar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "\n",
    "LINE_PALETTE=\"viridis\"\n",
    "ALPHA=1\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "icl_score = losses_over_time.loc[losses_over_time.token == 8, \"loss\"].values - losses_over_time.loc[losses_over_time.token == 1, \"loss\"].values\n",
    "sns.lineplot(x=steps, y=icl_score, alpha=ALPHA, ax=ax)\n",
    "    \n",
    "# ax.set_yscale('log')\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Step, $t$\")\n",
    "ax.set_ylabel(\"$L_\\mathrm{val}$\")\n",
    "ax.legend().remove()\n",
    "\n",
    "plot_transitions(ax, TRANSITIONS, limit=True) #, alpha=0.25)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.7])  # Adjust as necessary for position and size\n",
    "\n",
    "# custom_cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", LINE_PALETTE)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=1, vmax=8), )\n",
    "sm._A = []  # Dummy array for the ScalarMappable. \n",
    "cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "\n",
    "tick_positions = range(1, len(LINE_PALETTE)+1)  # Positions for each color\n",
    "tick_labels = [f\"${i}$\" for i in range(1, len(LINE_PALETTE) + 1)] # Replace with your labels\n",
    "cbar.set_ticks(tick_positions)\n",
    "cbar.set_ticklabels(tick_labels)\n",
    "cbar.set_label(\"$k$\")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.91, 1])  # Adjust layout to make room for colorbar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Essential Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the PCAs again. \n",
    "from typing import Dict, Iterable, Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "from devinterp.mechinterp.hooks import hook\n",
    "import numpy as np\n",
    "from icl.analysis.utils import map_evals_over_checkpoints, get_unique_run\n",
    "from icl.train import Run\n",
    "from devinfra.utils.tensors import convert_tensor, ReturnTensor\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "\n",
    "def extract_activations_over_checkpoints(models: Iterable[nn.Module], xs, ys, *paths, return_type: ReturnTensor=\"np\"):\n",
    "    def eval_activations(model):\n",
    "        model.to(DEVICE)\n",
    "        xs.to(model.device)\n",
    "        ys.to(model.device)\n",
    "        hooked_model = hook(model, *paths)\n",
    "        outputs, activations = hooked_model.run_with_cache(xs, ys)\n",
    "        activations[\"\"] = outputs\n",
    "        return {k: convert_tensor(v, return_type) for k, v in activations.items() if (k in paths or k == \"\") and v is not None}\n",
    "    \n",
    "    for model in models:\n",
    "        yield eval_activations(model)\n",
    "\n",
    "\n",
    "def get_vectorized_activations_trace(models: Iterable[nn.Module], xs, ys, *paths, normalize=False):\n",
    "    evals: Dict[str, list] = defaultdict(list)\n",
    "    \n",
    "    for activations in extract_activations_over_checkpoints(models, xs, ys, *paths):\n",
    "        for path, activation in activations.items():\n",
    "            if normalize:\n",
    "                activation = activation / np.linalg.norm(activation)\n",
    "\n",
    "            evals[path].append(activation)\n",
    "\n",
    "    return {\n",
    "        k: np.array(v).reshape(len(v), -1) for k, v in evals.items()\n",
    "    }\n",
    "\n",
    "def get_pca_activations_trace(models: Iterable[nn.Module], xs, ys, *paths, num_components=3, normalize=False) -> Dict[str, Tuple[PCA, np.ndarray]]:\n",
    "    results = {}\n",
    "\n",
    "    for path, activations in get_vectorized_activations_trace(models, xs, ys, *paths, normalize=normalize).items():\n",
    "        pca = PCA(n_components=num_components)\n",
    "        activations_reduced = pca.fit_transform(activations)\n",
    "        results[path] = pca, activations_reduced\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "tab10 = plt.cm.get_cmap('tab10', 10)  # Get the tab10 colormap\n",
    "colors = tab10.colors[:len(TRANSITIONS)]  # Get the first 6 colors \n",
    "# Add an extra gray to this np array for extra colors\n",
    "colors = np.vstack((colors, np.array([0.8, 0.8, 0.8, 1.0])))\n",
    "\n",
    "# Create a new colormap from the extracted colors\n",
    "custom_cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "# plot_multiple_slices(steps, demo_logits_reduced_3, demo_logits_pca_3, title=demo.config.to_latex(), connect_dots=True)\n",
    "train_xs_1, train_ys_1 = pretrain_dist_noiseless.get_batch(8, 1024)\n",
    "\n",
    "traces = get_pca_activations_trace(\n",
    "    models,\n",
    "    train_xs_1, \n",
    "    train_ys_1, \n",
    "    \"token_sequence_transformer\",\n",
    "    # \"token_sequence_transformer.blocks.1\",\n",
    "    # \"token_sequence_transformer.token_embedding\",\n",
    "    num_components=3,\n",
    "    normalize=False\n",
    ") \n",
    "\n",
    "# traces_small = get_pca_activations_trace(\n",
    "#     models,\n",
    "#     train_xs_1[:1024], \n",
    "#     train_ys_1[:1024], \n",
    "#     \"\",\n",
    "#     \"token_sequence_transformer.blocks.1\",\n",
    "#     # \"token_sequence_transformer.token_embedding\",\n",
    "#     num_components=3,\n",
    "#     normalize=False\n",
    "# ) \n",
    "\n",
    "pca_outputs, logits_outputs = traces[\"\"]\n",
    "pca_logits, logits_reduced = traces[\"token_sequence_transformer\"]\n",
    "# pca_internal, activations_reduced = traces_small[\"token_sequence_transformer.blocks.1\"]\n",
    "\n",
    "traces_normalized = get_pca_activations_trace(\n",
    "    models,\n",
    "    train_xs_1, \n",
    "    train_ys_1, \n",
    "    \"token_sequence_transformer\",\n",
    "    # \"token_sequence_transformer.blocks.1\",\n",
    "    # \"token_sequence_transformer.token_embedding\",\n",
    "    num_components=3,\n",
    "    normalize=True\n",
    ") \n",
    "\n",
    "# traces_small_normalized = get_pca_activations_trace(\n",
    "#     models,\n",
    "#     train_xs_1[:1024], \n",
    "#     train_ys_1[:1024], \n",
    "#     # \"token_sequence_transformer\",\n",
    "#     \"token_sequence_transformer.blocks.1\",\n",
    "#     # \"token_sequence_transformer.token_embedding\",\n",
    "#     num_components=3,\n",
    "#     normalize=True\n",
    "# ) \n",
    "\n",
    "pca_outputs_normalized, activations_outputs_normalized = traces_normalized[\"\"]\n",
    "pca_internal_normalized, activations_reduced_normalized = traces_normalized[\"token_sequence_transformer\"]\n",
    "# pca_internal, activations_reduced = traces_small[\"token_sequence_transformer.blocks.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "def get_transition_indices(steps, transitions):\n",
    "    transition_indices = []\n",
    "    for step in steps:\n",
    "        # Find the index of the transition that the current step falls into\n",
    "        index = next((i for i, transition in enumerate(transitions) if transition[0] <= step < transition[1]), None)\n",
    "        transition_indices.append(index if index is not None else -1)\n",
    "\n",
    "    return transition_indices\n",
    "\n",
    "def get_nearest_step(step):\n",
    "    idx = np.argmin(np.abs(np.array(steps) - step))\n",
    "    return steps[idx]\n",
    "\n",
    "def plot_explained_variance(pca, title=\"Explained Variance\", ax: Optional[plt.Axes] = None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    ax.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)\n",
    "\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        ax.text(i, ratio, f\"{ratio:.2f}\", fontsize=12, ha='center', va='bottom')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('PC')\n",
    "    ax.set_ylabel('Explained Variance')\n",
    "\n",
    "    ax.set_xticks(range(len(pca.explained_variance_ratio_)), range(1, len(pca.explained_variance_ratio_) + 1))\n",
    "\n",
    "\n",
    "def plot_multiple_slices(steps, samples, pca, highlighted_steps, transition_idxs, ave: Optional[str] = None, connect_dots=False, cmap=custom_cmap, alpha=1, save=False, line_color=\"auto\"):\n",
    "    num_pca_components = samples.shape[-1]\n",
    "    \n",
    "    # Create a single row of subplots\n",
    "    num_pca_combos = (num_pca_components * (num_pca_components-1)) // 2\n",
    "    fig, axes = plt.subplots(1, num_pca_combos + 1, figsize=(20, 4))\n",
    "    # fig.suptitle(title)\n",
    "\n",
    "    # Ensure ax is iterable by converting to a list if there's only one subplot\n",
    "    if num_pca_components == 2:\n",
    "        axes = [axes]\n",
    "\n",
    "\n",
    "    I = 0\n",
    "    for i in range(1, num_pca_components):\n",
    "        for j in range(i):\n",
    "\n",
    "            if connect_dots:\n",
    "                axes[I].plot(samples[:, i], samples[:, j], c='black', alpha=0.2)\n",
    "\n",
    "            sc = axes[I].scatter(samples[:, i], samples[:, j], c=transition_idxs, cmap=cmap, s=50, alpha=alpha)\n",
    "            axes[I].set_xlabel(f'PC {i}')\n",
    "            axes[I].set_ylabel(f'PC {j}')\n",
    "            axes[I].set_title(f'PC {i} vs PC {j}')\n",
    "\n",
    "            # Label some points\n",
    "            total_samples = len(samples)\n",
    "            for step in highlighted_steps:\n",
    "                k = steps.index(step)  # Find the index of the highlighted step\n",
    "                axes[I].text(samples[k, i], samples[k, j], str(step), fontsize=8, ha='right', va='bottom', alpha=0.5)\n",
    "\n",
    "            I += 1\n",
    "\n",
    "    plot_explained_variance(pca, ax=axes[-1])\n",
    "    # for I in range( num_pca_combos):\n",
    "    #     axes[I].axis('off')\n",
    "            \n",
    "    # Colorbar for the last plot\n",
    "    # cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Adjust as necessary\n",
    "        # plt.colorbar(sc, cax=cbar_ax, label='Milestones')\n",
    "\n",
    "    # Plot the legend on the first subplot on the left\n",
    "    legend_ax = axes[0]\n",
    "    scatter_proxy = [plt.Line2D([0], [0], linestyle='none', marker='o', alpha=alpha, color=cmap(i / len(TRANSITIONS))) for i in range(len(transition_idxs))]\n",
    "    legend_labels = [label for _, _, label in TRANSITIONS]\n",
    "    legend_ax.legend(scatter_proxy, legend_labels, loc='center', ncol=1, frameon=False, bbox_to_anchor=(-0.5, 0.5), title='Developmental Stages')\n",
    "    # legend_ax.set_title()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust the right side to make room for the colorbar\n",
    "\n",
    "    if save:\n",
    "        parent_dir = os.path.dirname(save)\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "        plt.savefig(save)\n",
    "\n",
    "\n",
    "    fig.set_facecolor('white')\n",
    "    \n",
    "# Usage of the function\n",
    "# Call the function with your data and the list of highlighted steps\n",
    "# plot_multiple_slices(steps, samples, pca, highlighted_steps=[100, 1000, 10000], title=\"Your Title\", num_points_to_label=10, save=\"path/to/save.png\", connect_dots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_of_steps = get_transition_indices(steps, TRANSITIONS)\n",
    "highlight_steps = list(map(get_nearest_step, [t[0] for t in TRANSITIONS][1:]))\n",
    "\n",
    "# plot_multiple_slices(\n",
    "#     steps, \n",
    "#     logits_outputs, \n",
    "#     pca_outputs, \n",
    "#     highlight_steps,\n",
    "#     transitions_of_steps,\n",
    "#     connect_dots=True, \n",
    "#     save=None,\n",
    "#     cmap=transitions_cmap\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# plot_multiple_slices(\n",
    "#     steps, \n",
    "#     activations_outputs_normalized, \n",
    "#     pca_outputs_normalized, \n",
    "#     highlight_steps,\n",
    "#     transitions_of_steps,\n",
    "#     connect_dots=True, \n",
    "#     save=None,\n",
    "#     cmap=transitions_cmap\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    logits_reduced, \n",
    "    pca_logits, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    "    cmap=transitions_cmap\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plot_multiple_slices(\n",
    "#     steps, \n",
    "#     activations_reduced_normalized, \n",
    "#     pca_internal_normalized, \n",
    "#     highlight_steps,\n",
    "#     transitions_of_steps,\n",
    "#     connect_dots=True, \n",
    "#     save=None,\n",
    "#     cmap=transitions_cmap\n",
    "# )\n",
    "# plt.show()\n",
    "\n",
    "LAST_CHECKPOINT_IDXS = -50\n",
    "last_steps = steps[LAST_CHECKPOINT_IDXS:]\n",
    "last_highlight_steps = [step for step in highlight_steps if step in last_steps]\n",
    "\n",
    "plot_multiple_slices(\n",
    "    last_steps, \n",
    "    activations_reduced_normalized[LAST_CHECKPOINT_IDXS:], \n",
    "    pca_internal_normalized, \n",
    "    last_highlight_steps,\n",
    "    transitions_of_steps[LAST_CHECKPOINT_IDXS:],\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    "    cmap=transitions_cmap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    activations_reduced, \n",
    "    pca_internal, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    activations_reduced_normalized, \n",
    "    pca_internal_normalized,\n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "token_losses_over_time = losses_over_time.loc[losses_over_time.token != \"$\\overline{\\hat y}$\"]\n",
    "mean_losses_over_time = losses_over_time.loc[losses_over_time.token == \"$\\overline{\\hat y}$\"]\n",
    "\n",
    "sns.lineplot(data=token_losses_over_time, x=\"step\", y=f\"loss\", hue=\"token\", palette=\"viridis\", ax=axes[0], alpha=0.5)\n",
    "sns.lineplot(data=token_losses_over_time, x=\"step\", y=f\"loss_0\", hue=\"token\", palette=\"viridis\", ax=axes[1], alpha=0.5)\n",
    "sns.lineplot(data=token_losses_over_time, x=\"step\", y=f\"ood_loss\", hue=\"token\", palette=\"viridis\", ax=axes[2], alpha=0.5)\n",
    "\n",
    "sns.lineplot(data=mean_losses_over_time, x=\"step\", y=\"loss\", label=\"Mean\", ax=axes[0], color=BRED)\n",
    "sns.lineplot(data=mean_losses_over_time, x=\"step\", y=f\"loss_0\", label=\"Mean\", ax=axes[1], color=BRED)\n",
    "sns.lineplot(data=mean_losses_over_time, x=\"step\", y=\"ood_loss\", label=\"Mean\", ax=axes[2], color=BRED)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Step, $t$\")\n",
    "    ax.set_yscale('log')\n",
    "    legend = ax.legend()\n",
    "    legend.remove()\n",
    "    ax.set_xlim(100, 500_000)\n",
    "\n",
    "legend = axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=6)\n",
    "legend.set_title(\"Per-Token Losses\")\n",
    "\n",
    "# Move legend to be likee fig.legend(patch_list, milestone_labels, loc='upper center', bbox_to_anchor=(0.5, -0.025), ncol=len(TRANSITIONS))\n",
    "\n",
    "axes[0].set_title(\"MSE on in-distribution inputs over time\")\n",
    "axes[0].set_ylabel(\"MSE\")\n",
    "\n",
    "axes[1].set_title(\"Mean squared prediction over time\")\n",
    "axes[1].set_ylabel(\"$\\|\\hat y_k\\|^2$\")\n",
    "\n",
    "axes[2].set_title(\"MSE on out-of-distribution inputs over time\")\n",
    "axes[2].set_ylabel(\"MSE\")\n",
    "\n",
    "plot_transitions(axes, TRANSITIONS)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Add color bar on the far right\n",
    "\n",
    "fig.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "gradient_stats = []\n",
    "\n",
    "xs, ys = run.evaluator.pretrain_xs, run.evaluator.pretrain_ys\n",
    "xs, ys = xs.to(DEVICE), ys.to(DEVICE)\n",
    "\n",
    "\n",
    "for step, model in zip(steps, models):\n",
    "    model.to(DEVICE)\n",
    "    model.zero_grad()\n",
    "\n",
    "    yhats = model(xs, ys)\n",
    "\n",
    "    loss = F.mse_loss(yhats, ys)\n",
    "    loss.backward()\n",
    "\n",
    "    for n, p in model.named_parameters():\n",
    "\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "\n",
    "        grad_sq_mean = (p.grad ** 2).mean().item()\n",
    "        grad_sq_std = (p.grad ** 2).std().item()\n",
    "\n",
    "        gradient_stats.append({\n",
    "            \"step\": step,\n",
    "            \"layer\": n,\n",
    "            \"grad/norm\": grad_sq_mean ** 0.5,\n",
    "            \"grad_sq/mean\": grad_sq_mean,\n",
    "            \"grad_sq/std\": grad_sq_std,\n",
    "            \"numel\": p.numel(),\n",
    "            \"loss\": loss.item(),\n",
    "        })          \n",
    "\n",
    "        p.grad = None \n",
    "\n",
    "gradient_stats = pd.DataFrame(gradient_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "\n",
    "avg_gradients = gradient_stats.groupby(['step']).mean()\n",
    "\n",
    "grad_norm_thresholded = gradient_stats['grad/norm'].values + 0.00001\n",
    "\n",
    "sns.lineplot(data=gradient_stats, x='step', y=grad_norm_thresholded, hue=\"layer\", ax=ax, legend=False, alpha=0.5)\n",
    "sns.lineplot(data=avg_gradients, x='step', y='grad/norm', ax=ax, legend=False, color=BRED)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Step, $t$\")\n",
    "ax.set_ylabel(\"Gradient norm, $\\|w_t\\|$\")\n",
    "\n",
    "_ = plot_transitions(ax, TRANSITIONS, limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the optimizer state\n",
    "\n",
    "names = [n for n, _ in run.model.named_parameters()]\n",
    "\n",
    "optimizer_stats = []\n",
    "\n",
    "for step, optimizer_state_dict in zip(steps, optimizer_state_dicts):\n",
    "    for layer, g in optimizer_state_dict[\"state\"].items():\n",
    "        optimizer_stats.append({\n",
    "            \"step\": step,\n",
    "            \"layer\": layer,\n",
    "            \"layer_name\": list(model.state_dict().keys())[layer],\n",
    "            \"exp_avg_sq_norm\": g[\"exp_avg_sq\"].norm().item() + 0.0000001\n",
    "        })\n",
    "\n",
    "optimizer_stats = pd.DataFrame(optimizer_stats)\n",
    "avg_optimizer_stats = optimizer_stats.groupby('step').mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sns.lineplot(data=optimizer_stats, x=\"step\", y=\"exp_avg_sq_norm\", hue=\"layer\", palette=\"viridis\", ax=ax, alpha=0.5)\n",
    "sns.lineplot(data=avg_optimizer_stats, x=\"step\", y=\"exp_avg_sq_norm\", palette=\"viridis\", ax=ax, color=BRED)\n",
    "ax.set_ylabel(\"Gradient Norm\")\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_title(\"Gradient Norms by Layer and Step\")\n",
    "ax.legend().remove()\n",
    "\n",
    "plot_transitions(ax, TRANSITIONS, limit=True)\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to see in what order the layers reach \"zero\"\n",
    "# 1. Figure out the earliest step for each layer where exp_avg_sq_norm < 1e-5\n",
    "# 2. Order by this earliest step\n",
    "# 3. List the names\n",
    "\n",
    "threshold = 3e-7\n",
    "\n",
    "# Find the earliest step where exp_avg_sq_norm < threshold for each layer\n",
    "earliest_zero_step = optimizer_stats[optimizer_stats['exp_avg_sq_norm'] < threshold] \\\n",
    "    .groupby('layer_name') \\\n",
    "    .agg(earliest_step=('step', 'min'))\n",
    "\n",
    "# Now, sort the layers by the earliest step where their norm goes below the threshold\n",
    "sorted_layers_by_earliest_zero_step = earliest_zero_step.sort_values(by='earliest_step')\n",
    "sorted_layers_by_earliest_zero_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient essential dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "try:\n",
    "    del gradients_over_time\n",
    "    del gradients_reduced\n",
    "    del gradients_reduced_normalized\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "PERPLEXITY = 15\n",
    "\n",
    "for layer_name in [\n",
    "    \"token_sequence_transformer.token_embedding\",\n",
    "    \"token_sequence_transformer.unembedding.0\",\n",
    "    \"token_sequence_transformer.blocks.0.compute.0\",\n",
    "    \"token_sequence_transformer.blocks.0.compute.2\",\n",
    "    \"token_sequence_transformer.blocks.1.compute.0\",\n",
    "    \"token_sequence_transformer.blocks.1.compute.2\"\n",
    "]:\n",
    "    layer_path = layer_name.split(\".\")\n",
    "    last_state_dict = models[-1].state_dict()\n",
    "    num_params = 0\n",
    "    \n",
    "    for subset in (\"weight\", \"bias\"):\n",
    "        subset_full_name = layer_name + \".\" + subset\n",
    "        if subset_full_name in last_state_dict:\n",
    "            num_params += models[-1].state_dict()[subset_full_name].numel()\n",
    "            \n",
    "    gradients_over_time = np.zeros((len(steps), num_params))\n",
    "\n",
    "    def get_params(model, layer_path):\n",
    "        m = model\n",
    "        for part in layer_path:\n",
    "            m = getattr(m, part)\n",
    "        \n",
    "        return m\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # model.train()\n",
    "        model.to(DEVICE)\n",
    "        model.zero_grad()\n",
    "\n",
    "        yhats = model(xs, ys)\n",
    "\n",
    "        loss = F.mse_loss(yhats, ys)\n",
    "        loss.backward()\n",
    "\n",
    "        layer = get_params(model, layer_path)\n",
    "\n",
    "        n = 0\n",
    "        for subset in (\"weight\", \"bias\"):\n",
    "            if layer and hasattr(layer, subset):\n",
    "                param = getattr(layer, subset)\n",
    "                if param is None:\n",
    "                    continue\n",
    "\n",
    "                numel = param.numel()\n",
    "                gradients_over_time[i, n:n+numel] = param.grad.flatten().cpu().numpy()\n",
    "                n += numel\n",
    "\n",
    "    pca = PCA(n_components=50)\n",
    "    gradients_reduced = pca.fit_transform(gradients_over_time)\n",
    "\n",
    "    norms = np.linalg.norm(gradients_over_time, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1\n",
    "\n",
    "    gradients_reduced_normalized = pca.fit_transform(gradients_over_time / norms)\n",
    "\n",
    "    plot_multiple_slices(\n",
    "        steps, \n",
    "        gradients_reduced[:, :3], \n",
    "        pca, \n",
    "        highlight_steps,\n",
    "        transitions_of_steps,\n",
    "        connect_dots=True, \n",
    "        save=None,\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=PERPLEXITY, n_iter=1000, random_state=0)\n",
    "    gradients_tsne = tsne.fit_transform(gradients_reduced)\n",
    "\n",
    "    plt.scatter(gradients_tsne[:, 0], gradients_tsne[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "    tsne_normalized = TSNE(n_components=2, perplexity=PERPLEXITY, n_iter=1000, random_state=0)\n",
    "    gradients_tsne_normalized = tsne.fit_transform(gradients_reduced_normalized)\n",
    "\n",
    "    plt.scatter(gradients_tsne_normalized[:, 0], gradients_tsne_normalized[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "try:\n",
    "    del gradients_over_time\n",
    "    del gradients_reduced\n",
    "    del gradients_reduced_normalized\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "PERPLEXITY = 10\n",
    "\n",
    "def get_all_params_as_array(model):\n",
    "    return np.concatenate([p.cpu().numpy().flatten() for p in model.parameters()])\n",
    "\n",
    "\n",
    "def get_all_gradients_as_array(model):\n",
    "    return np.concatenate([p.grad.cpu().numpy().flatten() for p in model.parameters() if p.grad is not None])\n",
    "\n",
    "gradients_over_time = []\n",
    "gradients_normalized_over_time = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    # model.train()\n",
    "    model.to(DEVICE)\n",
    "    model.zero_grad()\n",
    "\n",
    "    yhats = model(xs, ys)\n",
    "\n",
    "    loss = F.mse_loss(yhats, ys)\n",
    "    loss.backward()\n",
    "\n",
    "    layer = get_params(model, layer_path)\n",
    "\n",
    "    gradients = get_all_gradients_as_array(model)\n",
    "    gradients_over_time.append(gradients)\n",
    "    gradients_normalized_over_time.append(gradients / np.linalg.norm(gradients))\n",
    "\n",
    "gradients_over_time = np.array(gradients_over_time)\n",
    "gradients_normalized_over_time = np.array(gradients_normalized_over_time)\n",
    "\n",
    "pca_1 = PCA(n_components=50)\n",
    "gradients_reduced = pca_1.fit_transform(gradients_over_time)\n",
    "\n",
    "pca_2 = PCA(n_components=50)\n",
    "gradients_reduced_normalized = pca_2.fit_transform(gradients_normalized_over_time)\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    gradients_reduced[:, :3], \n",
    "    pca_1, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    ")\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    gradients_reduced_normalized[:, :3], \n",
    "    pca_2, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Full gradients\")\n",
    "print(\"Unnormalized\")\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=PERPLEXITY, n_iter=1000, random_state=0)\n",
    "gradients_tsne = tsne.fit_transform(gradients_reduced)\n",
    "\n",
    "plt.scatter(gradients_tsne[:, 0], gradients_tsne[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "print(\"Normalized\")\n",
    "\n",
    "tsne_normalized = TSNE(n_components=2, perplexity=PERPLEXITY, n_iter=1000, random_state=0)\n",
    "gradients_tsne_normalized = tsne.fit_transform(gradients_reduced_normalized)\n",
    "\n",
    "plt.scatter(gradients_tsne_normalized[:, 0], gradients_tsne_normalized[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "try:\n",
    "    del exp_avg_grads_over_time\n",
    "    del exp_avg_grads_reduced\n",
    "    del exp_avg_grads_reduced_normalized\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "PERPLEXITY = 15\n",
    "\n",
    "exp_avg_grads_over_time = []\n",
    "exp_avg_grads_normalized_over_time = []\n",
    "\n",
    "def get_exp_avg_sq_grads(optimizer_state_dict):\n",
    "    return np.concatenate([g[\"exp_avg_sq\"].cpu().numpy().flatten() for g in optimizer_state_dict[\"state\"].values()])\n",
    "\n",
    "def get_exp_avg_grads(optimizer_state_dict):\n",
    "    return np.concatenate([g[\"exp_avg\"].cpu().numpy().flatten() for g in optimizer_state_dict[\"state\"].values()])\n",
    "\n",
    "\n",
    "for i, opt_state in enumerate(optimizer_state_dicts):\n",
    "    # model.train()\n",
    "    exp_avg_grads = get_exp_avg_grads(opt_state)\n",
    "    exp_avg_grads_over_time.append(exp_avg_grads)\n",
    "    exp_avg_grads_normalized_over_time.append(exp_avg_grads / np.linalg.norm(exp_avg_grads))\n",
    "\n",
    "exp_avg_grads_over_time = np.array(exp_avg_grads_over_time)\n",
    "exp_avg_grads_normalized_over_time = np.array(exp_avg_grads_normalized_over_time)\n",
    "\n",
    "\n",
    "pca_1 = PCA(n_components=50)\n",
    "exp_avg_grads_reduced = pca_1.fit_transform(exp_avg_grads_over_time)\n",
    "\n",
    "pca_2 = PCA(n_components=50)\n",
    "exp_avg_grads_reduced_normalized = pca_2.fit_transform(exp_avg_grads_normalized_over_time)\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    exp_avg_grads_reduced[:, :3], \n",
    "    pca_1, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    ")\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    exp_avg_grads_reduced_normalized[:, :3], \n",
    "    pca_2, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Exponentially averaged gradients\")\n",
    "print(\"Unnormalized\")\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=PERPLEXITY, n_iter=1000, random_state=0)\n",
    "exp_avg_grads_tsne = tsne.fit_transform(exp_avg_grads_reduced)\n",
    "\n",
    "plt.scatter(exp_avg_grads_tsne[:, 0], exp_avg_grads_tsne[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "print(\"Normalized\")\n",
    "\n",
    "tsne_normalized = TSNE(n_components=2, perplexity=PERPLEXITY, n_iter=1000, random_state=0)\n",
    "exp_avg_grads_tsne_normalized = tsne.fit_transform(exp_avg_grads_reduced_normalized)\n",
    "\n",
    "plt.scatter(exp_avg_grads_tsne_normalized[:, 0], exp_avg_grads_tsne_normalized[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "try:\n",
    "    del exp_avg_grads_over_time\n",
    "    del exp_avg_grads_reduced\n",
    "    del exp_avg_grads_reduced_normalized\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "PERPLEXITY = 15\n",
    "\n",
    "exp_avg_grads_over_time = []\n",
    "exp_avg_grads_normalized_over_time = []\n",
    "\n",
    "def get_exp_avg_sq_grads(optimizer_state_dict):\n",
    "    return np.concatenate([g[\"exp_avg_sq\"].cpu().numpy().flatten() for g in optimizer_state_dict[\"state\"].values()])\n",
    "\n",
    "def get_exp_avg_grads(optimizer_state_dict):\n",
    "    return np.concatenate([g[\"exp_avg\"].cpu().numpy().flatten() for g in optimizer_state_dict[\"state\"].values()])\n",
    "\n",
    "\n",
    "for i, opt_state in enumerate(optimizer_state_dicts):\n",
    "    # model.train()\n",
    "    exp_avg_grads = get_exp_avg_sq_grads(opt_state)\n",
    "    exp_avg_grads_over_time.append(exp_avg_grads)\n",
    "    exp_avg_grads_normalized_over_time.append(exp_avg_grads / np.linalg.norm(exp_avg_grads))\n",
    "\n",
    "exp_avg_grads_over_time = np.array(exp_avg_grads_over_time)\n",
    "exp_avg_grads_normalized_over_time = np.array(exp_avg_grads_normalized_over_time)\n",
    "\n",
    "pca_1 = PCA(n_components=50)\n",
    "exp_avg_grads_reduced = pca_1.fit_transform(exp_avg_grads_over_time)\n",
    "\n",
    "pca_2 = PCA(n_components=50)\n",
    "exp_avg_grads_reduced_normalized = pca_2.fit_transform(exp_avg_grads_normalized_over_time)\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    exp_avg_grads_reduced[:, :3], \n",
    "    pca_1, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    ")\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    exp_avg_grads_reduced_normalized[:, :3], \n",
    "    pca_2, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Exponentially averaged square gradients\")\n",
    "print(\"Unnormalized\")\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=PERPLEXITY, n_iter=1000, random_state=0)\n",
    "exp_avg_grads_tsne = tsne.fit_transform(exp_avg_grads_reduced)\n",
    "\n",
    "plt.scatter(exp_avg_grads_tsne[:, 0], exp_avg_grads_tsne[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "print(\"Normalized\")\n",
    "tsne_normalized = TSNE(n_components=2, perplexity=PERPLEXITY, n_iter=1000, random_state=0)\n",
    "exp_avg_grads_tsne_normalized = tsne.fit_transform(exp_avg_grads_reduced_normalized)\n",
    "\n",
    "plt.scatter(exp_avg_grads_tsne_normalized[:, 0], exp_avg_grads_tsne_normalized[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=0)\n",
    "gradients_tsne = tsne.fit_transform(gradients_reduced)\n",
    "\n",
    "plt.scatter(gradients_tsne[:, 0], gradients_tsne[:, 1], c=transitions_of_steps, cmap=custom_cmap, s=50, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGLD PCAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pp\n",
    "import sys\n",
    "\n",
    "from icl.analysis.slt import ExpectedBatchLossEstimator\n",
    "from icl.analysis.weights import WeightsTrace \n",
    "del sys.modules['icl.analysis.sample']\n",
    "del sys.modules['icl.analysis.slt']\n",
    "del sys.modules['icl.analysis.estimators']\n",
    "\n",
    "import yaml\n",
    "from icl.analysis.sample import SamplerConfig\n",
    "\n",
    "os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')\n",
    "\n",
    "CORES = 1\n",
    "NUM_CHAINS = 25\n",
    "NUM_DRAWS = 1000\n",
    "NUM_SAMPLES = 1024\n",
    "DATASET_SIZE = 2 ** 14\n",
    "\n",
    "sampler_config: SamplerConfig = SamplerConfig(\n",
    "    num_chains=NUM_CHAINS,\n",
    "    num_draws=NUM_DRAWS,\n",
    "    sampling_method='sgld',\n",
    "    grad_batch_origin='eval-dataset',\n",
    "    grad_batch_size=NUM_SAMPLES,\n",
    "    noise_scale=5e-3,    \n",
    "    localization_scale=5e-2,\n",
    "    gradient_scale=1e-5 * 1024. / (2 * np.log(1024)), \n",
    "    # noise_scale=5e-4,    \n",
    "    # localization_scale=1e-1,\n",
    "    # gradient_scale=1e-6 * 1024. / (2 * np.log(1024)), \n",
    "    # eval_method='fixed-minibatch',\n",
    "    eval_method='grad-minibatch',\n",
    "    eval_metrics=['likelihood-derived', 'batch-loss', 'weights'],\n",
    "    # eval_batch_size=8192,\n",
    "    eval_dataset_size=DATASET_SIZE,\n",
    "    device='cpu',\n",
    "    cores=CORES,\n",
    "    eval_loss_fn='mse',\n",
    "    eval_online=True\n",
    ")\n",
    "\n",
    "print(yaml.dump(sampler_config.model_dump()))\n",
    "\n",
    "run.model = models[-1]\n",
    "run.model.to('cpu')\n",
    "# log_fn = lambda data, step=None: print(f\"Step: {step}\\n\", yaml.dump(data))\n",
    "sampler = sampler_config.to_sampler(run, log_fn=None)\n",
    "print(\"INIT LOSS\", sampler.init_loss)\n",
    "\n",
    "results = sampler.eval(run.model)\n",
    "\n",
    "pp(sampler.callbacks[0].estimate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llc_callback = sampler.callbacks[0]\n",
    "sgld_estimates_df = llc_callback.estimates()\n",
    "\n",
    "sgld_estimates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "batch_losses = sampler.batch_loss.estimates()\n",
    "batch_losses['mean'] = [float(x) for x in batch_losses['mean']]\n",
    "\n",
    "#llc_callback.expected_loss_estimator.\n",
    "sns.lineplot(data=batch_losses, x=\"draw\", y=\"mean\", hue=\"chain\", palette=\"gray\", ax=ax, alpha=0.5)\n",
    "\n",
    "twin_ax = ax.twinx()\n",
    "sns.lineplot(data=sgld_estimates_df, x=\"draw\", y=\"llc/mean\", ax=twin_ax, alpha=0.5, color=PRIMARY)\n",
    "\n",
    "ax.set_ylabel(r\"Batch Loss. $L^{(\\tau)}_m$\")\n",
    "twin_ax.set_ylabel(r\"LLC, $\\hat\\lambda_\\tau$\", color=PRIMARY)\n",
    "for label in twin_ax.get_yticklabels():\n",
    "    label.set_color(PRIMARY)\n",
    "\n",
    "ax.set_xlabel(r\"Draw, $\\tau$\")\n",
    "ax.legend().remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights_np = np.concatenate([p.view(-1).detach().cpu().numpy() for p in run.model.parameters()])\n",
    "\n",
    "weights_np = sampler.callbacks[-1].weights.detach().cpu().numpy()[:, :, :]\n",
    "# del sampler.callbacks[-1].weights\n",
    "\n",
    "weights_flat = weights_np.reshape(-1, weights_np.shape[-1]) - init_weights_np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "weights_reduced = pca.fit_transform(weights_flat)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=0)\n",
    "weights_tsne = tsne.fit_transform(weights_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "plot_explained_variance(pca, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights_np = np.concatenate([p.view(-1).detach().cpu().numpy() for p in run.model.parameters()])\n",
    "\n",
    "weights_np = sampler.callbacks[-1].weights.detach().cpu().numpy()[:, :, :]\n",
    "# del sampler.callbacks[-1].weights\n",
    "\n",
    "weights_flat = weights_np.reshape(-1, weights_np.shape[-1]) - init_weights_np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "weights_reduced = pca.fit_transform(weights_flat)\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=0)\n",
    "weights_tsne = tsne.fit_transform(weights_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chain in range(NUM_CHAINS):\n",
    "    _weights = weights_tsne[chain * NUM_DRAWS:(chain + 1) * NUM_DRAWS] \n",
    "    sns.scatterplot(x=_weights[:, 0], y=_weights[:, 1], s=50, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_weights_trace_fn(model, deltas, xs, ys, device='cpu', num_components=3, num_points=10):\n",
    "    model.to(device)\n",
    "    xs.to(device)\n",
    "    ys.to(device)\n",
    "\n",
    "    num_chains = deltas.shape[0]\n",
    "    num_draws = deltas.shape[1]\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=num_components)\n",
    "\n",
    "    weights_reduced = pca.fit_transform(deltas.reshape(num_chains * num_draws, -1))\n",
    "\n",
    "    def get_pc_landscape(pca, fn, pc1, pc2, pc1_lim: Tuple[int, int], pc2_lim: Tuple[int, int], num_points=100, ax=None):\n",
    "        xx, yy = np.meshgrid(np.linspace(*pc1_lim, num_points), np.linspace(*pc2_lim, num_points))\n",
    "\n",
    "        # Compute function values for the grid\n",
    "        Z = np.zeros(xx.shape)\n",
    "        for i in tqdm.tqdm(range(xx.shape[0]), \"Iterating over rows\"):\n",
    "            for j in range(xx.shape[1]):\n",
    "                u = xx[i, j] * pc1 + yy[i, j] * pc2\n",
    "                Z[i, j] = fn(u)\n",
    "\n",
    "        # Plot the density map\n",
    "        Z = (Z - Z.min()) / (Z.max() - Z.min()) # rescale\n",
    "        Z = np.log(1e-3 + Z)\n",
    "        \n",
    "        im = ax.imshow(Z, interpolation='bilinear', origin='lower',\n",
    "            extent=(*pc1_lim, *pc2_lim), cmap='Blues', alpha=1., aspect='auto')\n",
    "        \n",
    "        return Z\n",
    "\n",
    "    def weights_to_model(weights):\n",
    "        m = deepcopy(model)\n",
    "        m.to(device)\n",
    "\n",
    "        i = 0\n",
    "        for n, p in m.named_parameters():\n",
    "            p.data += torch.from_numpy(weights[i:i+p.numel()]).view(p.shape).to(device)\n",
    "            i += p.numel()\n",
    "        \n",
    "        return m\n",
    "\n",
    "\n",
    "    def weights_to_loss(weights):\n",
    "        m = weights_to_model(weights)\n",
    "        yhats = m(xs, ys)\n",
    "        return F.mse_loss(yhats, ys).item()\n",
    "\n",
    "    xs.to(device)\n",
    "    ys.to(device)\n",
    "\n",
    "    pc_combos = list(itertools.combinations(range(num_components), 2))\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(pc_combos) + 1, figsize=(20, 5))\n",
    "\n",
    "    for ax,  (pc1_idx, pc2_idx) in zip(axes, pc_combos):\n",
    "        pc1 = pca.components_[pc1_idx]\n",
    "        pc2 = pca.components_[pc2_idx]\n",
    "\n",
    "        min_pc1, max_pc1 = weights_reduced[:, pc1_idx].min(), weights_reduced[:, pc1_idx].max()\n",
    "        min_pc2, max_pc2 = weights_reduced[:, pc2_idx].min(), weights_reduced[:, pc2_idx].max()\n",
    "\n",
    "        pc1_lims = (min_pc1 - 0.1 * (max_pc1 - min_pc1), max_pc1 + 0.1 * (max_pc1 - min_pc1))\n",
    "        pc2_lims = (min_pc2 - 0.1 * (max_pc2 - min_pc2), max_pc2 + 0.1 * (max_pc2 - min_pc2))\n",
    "\n",
    "        get_pc_landscape(pca, weights_to_loss, pc1, pc2, pc1_lims, pc2_lims, num_points=num_points, ax=ax)\n",
    "\n",
    "        for chain in range(num_chains):\n",
    "            # _weights = pca.transform(deltas[chain])\n",
    "            _weights = weights_reduced[chain * num_draws:(chain + 1) * num_draws] \n",
    "            sns.scatterplot(x=_weights[:, pc1_idx], y=_weights[:, pc2_idx], ax=ax, s=2, alpha=0.2)\n",
    "\n",
    "        ax.set_xlim(*pc1_lims)\n",
    "        ax.set_ylim(*pc2_lims)\n",
    "\n",
    "        ax.set_title(f\"PC {pc1_idx + 1} vs PC {pc2_idx + 1}\")\n",
    "        ax.set_xlabel(f\"PC {pc1_idx + 1}\")\n",
    "        ax.set_ylabel(f\"PC {pc2_idx + 1}\")\n",
    "\n",
    "    # Plot explained variance\n",
    "    plot_explained_variance(pca, title=\"Explained Variance\", ax=axes[-1])        \n",
    "\n",
    "\n",
    "plot_weights_trace_fn(run.model, sampler.weights.deltas(), xs=xs, ys=ys, device=DEVICE, num_components=4, num_points=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.current_allocated_memory() / 1e9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-token LLCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-token Essential Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhessian\n",
    "from pyhessian import hessian # Hessian computation\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs[0], inputs[1])\n",
    "    \n",
    "ref_model = ModelWrapper(deepcopy(models[-1]).to('cpu'))\n",
    "hessian_comp = hessian(ref_model, F.mse_loss, data=((xs, ys), ys), cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_eig_wrapper(*args, eigenvectors=True, **kwargs):\n",
    "    # Call the new torch.linalg.eig function\n",
    "    eigenvalues, eigenvecs = torch.linalg.eig(*args, **kwargs)\n",
    "    \n",
    "    # Format the output to mimic the old torch.eig\n",
    "    # torch.eig used to return a tensor with [real, imaginary] parts for eigenvalues\n",
    "    # torch.linalg.eig returns a tensor of complex numbers for eigenvalues\n",
    "    eigenvalues_real = eigenvalues.real\n",
    "    eigenvalues_imag = eigenvalues.imag\n",
    "\n",
    "    eigenvalues_combined = torch.stack((eigenvalues_real, eigenvalues_imag), dim=-1)\n",
    "    return eigenvalues_combined, eigenvecs\n",
    "\n",
    "# Monkey patch the torch.eig function in the pyhessian module\n",
    "torch.eig = torch_eig_wrapper \n",
    "\n",
    "density_eigen, density_weight = hessian_comp.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-161-72e34e31aabb>:25: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  density_output[i, j] = np.sum(tmp_result * weights[i, :])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAABOXUlEQVR4nO2dd5hkZZX/P5U7z0x3T2JyfGlyWIKAmBADriuirhhWXDGs4pp+7rquYXUNuKIruoguumJOGFgEhQUMZHBkZgjNOzNMTj3TOVe8vz9u3apbVfdW3equ6qruPp/n4emqG0/VNPfbJ7zn+AzDQBAEQRDqDX+tDRAEQRAEJ0SgBEEQhLpEBEoQBEGoS0SgBEEQhLpEBEoQBEGoS4K1NqCWbN261YhEIrU2o6ZEo1Hm+ncwHz6jUBr5PahfxsfHe88+++zF+dvntUBFIhG6urpqbUZN6e7unvPfwXz4jEJp5PegftmyZcs+p+0S4hMEQRDqEhEoQRAEoS4RgRIEQRDqEhEoQRAEoS4RgRIEQRDqEhEoQRAEoS4RgRIEQRDqEhGoecytWw/RfWyy1mYIgiA4Mq8X6s5n7nzqKO/7yVYATj9pExsWt9TWIEEQhDzEg5qnfO+hvYSDfgI++N6De2ttjiAIQgEiUPOQ0WiCR/f089YL13LmCY3cv6u31iYJgiAUIAI1D9l2YJB40uDCDZ2cvqyRZ4+PcWxEclGCINQXIlDzkKcODwFwyooFbO40uzs/fXi4liYJgiAUIAI1D3nq8DDLFzTQ3hxm3aIwAProSI2tEgRByEUEah6y69gom5e2AtAaCbC0LSICJQhC3SECNc8wDIN9feOs62zObNuwuIU9fWM1tEoQBKEQEah5Rt9YjNFogjUdTZltazqaONA/XkOrBEEQChGBmmfsS3tKazuyHtSq9iZ6R2OMRRO1MksQBKEAEah5xoH+CQBWtTdmtq1uN72pAwPiRQmCUD+IQM0zjgyZ652WL8gK1MpFpkAdTIuXIAhCPSACNc84MjRBW0OQ5ki2DePSNnMt1LGRaK3MEgRBKEAEap5xeHCSExY25mzrbIng8yHdJARBqCtEoOYZR4cnWL6gIWdbKOCnvSlMz7B4UIIg1A8iUPOMnuEoS1obCrYvaWvguHhQgiDUESJQ84hUyqB/LEZna7hg35LWiOSgBEGoK0Sg5hFDE3GSKYOO5kjBviWtEXqGxYMSBKF+EIGaR/SNxQDoaHHwoNoi9I7GSKaMmTZLEATBERGoeUTfqBnC62xx8qAaSKZDgIIgCPWACNQ8opgHlV0LJWE+QRDqAxGoeYTlQTnloBanK/uOSam5IAh1QrD0IbMDpdRy4HrgCDCitf5YjU2qO3pHY/h8sKgpVLBvSat4UIIg1BdzyYN6J3CT1vp9wGql1Noa21N39I1FWdQUJhgo/Ge3wn79Y/GZNksQBMGRuSRQy4G96dcHgRW1M6U+6RuN0dFcmH8CaAwFCAf9DI5LkYQgCPXBXBKofcCq9OuVwKEa2lKX9I3GaHcRKJ/PR3tTWKr4BEGoG+aSQP038Hal1A3ALq313hrbU3f0jkUdS8wtFjaFGBiXEJ8gCPVBXRdJKKV8wHeAJ7XW16W3XQZ8HogA24G3aa2Htda9wJXlXD8ajdLd3V1hq+uXY0MTnNweyPnMk5OTmfcR4hzuG5xz34n9MwrzF/k9mH3UrUAppbqAG4DzgSfT2xZjCtaFWuudSqkvANcC757KPSKRCF1dXRWyuL5JJFOMxnazfuVSuro2Z7Z3d3dnvoOVf5mg++jwnPtO7J9RmL/I70H9smXLFsft9Rziew+mGP3Mtu1S4DGt9c70+xuBN6Y9LaEII5MJwAzjubGwKcSghPgEQagT6taD0lpfA6CUepFt8yrggO39QaANaAWGZ8662cfQhCk8bQ3uArWoKczgeIxUysDvF80XBKG21LMH5YSbvckZtWIWYgnUgsYiAtUcJmVkvS1BEIRaMtsEaj/meieLFcCA1nqsRvbMGjICVSTEZ3WY6Je1UIIg1AGzTaDuAs5XSm1Kv38XcGsN7Zk1DE968KCazDVSAyJQgiDUAbNKoLTWx4C3ArcopbqBU4EP1daq2YGXHJRVQCHdJARBqAfqtkjCQmt9Vd77O4A7amPN7MVLDsrqMiH9+ARBqAdmlQclTJ2hiTjhgJ+GkPs/+cJ0iE88KEEQ6gERqHnC8ESCtsYgPp97+XhbQ5CA3yc5KEEQ6gIRqHnC8ESctiLhPTAbxi5slMW6giDUByJQ84ShiXjR/JPFgsYQgxMiUIIg1B5PRRJKqTbgecA5wBIgBRwF/gzcq7WWMax1ztBEPDOUsBgLmkIMi0AJglAHFBUopdRG4COYXcL7gaeBPkzPazNmv7wWpdQPgOtsPfKEOmN4Ms66zuaSxy1sDNE7KjkoQRBqj6tAKaU+A7wauBk4W2v9jMtxClPAfquU+pnW+qPVMFSYHl5DfAubwuw6PjoDFgmCIBSnmAd1ADhVa120z53WWgP/ppT6HHB1JY0TKkMqZTBcTg5KiiQEQagDXAVKa/3Nci6ktY4BX5+2RULFGY0lSBnQ1lg65bigMcTIZIJkyiAgHc0FQaghnjtJpCfZfgDYiFkw8XZgv9b6v6tkm1Ahhj20ObKw2h0NT8RZ1Fy6qEIQBKFaeCozV0q9Cfg+8CfMKr4AcAT4slLqA9UzT6gEY1EzSttahkBJqbkgCLXG6zqofwbeqbX+NOnZS1rrG4CrgH+sjmlCpRiNmmLTHAmUPHZho7Q7EgShPvAqUBsw1zzlsxVYVjFrhKowmvagWiIeclDiQQmCUCd4FagngMts7430z78HtlfUIqHijEXNCbnNXgSqMZuDEgRBqCVeiyQ+BNyulHoREAY+kR4aeCbwimoZJ1SG0fQIdy8e1MJGayaUCJQgCLXFkweltb4fUJie1P8CC4H7gS6t9e+rZp1QEUaj3gVqgQiUIAh1gucyc631UeATVbRFqBLlhPiCAT+tkSCDE1IkIQhCbSnW6ug+srmmomitL66YRULFGY0lCAf8hIPeUo5tjSGGxIMSBKHGFPuT+u4Zs0KoKqOTCVoaPDvLtDeH6Zcyc0EQakyxVkefmklDhOoxFk14WgNl0d4cpn9MBEoQhNridR5UC/Au4GTMLhIAPiACnKm13lQd84RKMBpN0hz27kF1NIfZdUw6mguCUFu8roP6FvBPmIJ0Jdl5UK/BbIEk1DFj0QStZYT4OlrC9I1Fq2iRIAhCabwK1EuBN2it34A5tPA/tdbnAf8JnFot44TKMBpNeKrgs2hvjjAZTzEeS1TRKkEQhOJ4FagIsCP9+inM0e8A3wCkgq/OGStToDrSXcz7ZLKuIAg1xKtAPQ28OP36SeC56deLMMVLqGNGowlayslBtaQFSgolBEGoIV6fWp8EblFKBTBzTk8rpX4LnAL8rlrGCZVhLFpemfnStgYAjg5NwKqFVbJKEAShOF5bHf0GOBG4V2t9ALgI05O6AbNhrFCnpFIGY7FkWSG+FQsbATg0OFktswRBEEriNcQHcAKwHEBrvQ0YA/6gtR6vhmFCZRiLWX34vK+DWtgUojEU4PDgRLXMEgRBKInXibpvBe4ht2JvNXCvUupvq2GYUBmsabrleFA+n48TFjaIQAmCUFO8elD/CrxVa/11a4PW+u+BqwHpOFHHWNN0vXQyt7NyURP7+sQ5FgShdngVqOXAFoftj2F6UkKdUs40XTsnndDGzmMjRBPJapglCIJQEq8C9RjwfqWUL2/7e4BtlTVJqCTljNqwc+qKBcSTBvroSDXMEgRBKInXp9YHMLubv1wptTW97XSghdxR8EKdUc6wQjtnrV4EwO+fOc5pKxdW2ixBEISSeC0zfxyz9951wCFgD/BFYKPW+rHqmSdMl3LGvdtZtqCBizZ28uNH9zM8KbOhBEGYeTyXmWut+7TWN2itrwE+A3xLaz1cPdOESmCVmZcb4gP4wIs3c3w0yru+v4WJmOSiBEGYWYoKlFLqlUqpO5RSK9Lv1yql/gwcBwaVUp9zyEvVBKXUZqXUj5VSX1ZK/UAp1VBrm+qBqYb4AM5es4gvvuY0Htrdxzu+/2diiVSlzRMEQXDFVaCUUq8BfgEcA6zZCz8F1gGvBF6S/vn+6promSXAR7XWHwQGAVVbc+qDsWiCgN9HQ6icNdlZXn3WSr5wxWnct7OXz97+dIWtEwRBcKfYn9UfBD6stf4KgFLqLMwu5p/UWt+e3vavwLWYYzdmDKXUVcBVtk03aq1/mt53OeBLd7uY94xOJmgOB/D5pu7ovu6vVrHj6Ajfun8PZ69t55Wnn1BBCwVBEJwpJlCnAW+2vX8JYAC32rZtB9ZW3qziaK1vBm62b1NKBYHPAbu11u+ZaZvqldFockrhvXz++WUnsvXAIB/5xXY2Lm7hpBPaKmCdIAiCO8XiPglyBewS4IjWerttWydQL7PBP445WPGidA7q5FobVA+UOwvKjVDAz3+94SxaG4JcedPDPLa3vwLWCYIguFPsyfUA8HrgU0qpzZiDCW/MO+Ya4JFKGJIutvgO8KTW+rr0tsuAz2POnNoOvM2tclBr/UnMsSCeiUajdHd3T8vueqenf4iAkXL9nJOTk2V9B9desoSP3X2U13/zId5xTgd/fWLbtMKHM0G5n1GYm8jvweyjmEB9DLMZ7BWY7YyOYYbQUEpdCrwPeCHw/OkaoZTqwhzdcT7mGA+UUosxBetCrfVOpdQXMPNd757u/SwikQhdXV2VulxdYtzbz+KWoOvn7O7uLus76ALuOLWLD/50Kzc+eoxIWzvvv2RzhaytDuV+RmFuIr8H9cuWLU6d9IqE+NKLc08C/huzWewZWuuj6d1nYoYAX6C1roQH9R5MMfqZbdulwGNa653p9zcCb6yXsvbZwlg0QXMZ03S9sKAxxE1/91e8+qwVXH/PTh7e3VfR6wuCIECJVkda6yPAfzls/0IljUgv/kUp9SLb5lXAAdv7g0Ab0ArIAmGPjJY5Tdcrfr+Pf/+bU9iyb4B/umU7d33gYhpC3mdOCYIglGJqi2NmBjfbpKVBGYxGExWp4nOiORLks686lf3949z84N6q3EMQhPlLPQvUftITfNOsAAa01mM1smfWYRgGo9EErVXwoCwu2tTJC09cwg337qJvNFr6BEEQBI/Us0DdBZyvlNqUfv8uctdgCSUYjyUxjKn14SuHj778RMbjSa6/Z2fpgwVBEDxStwKltT4GvBW4RSnVjTlu/kO1tWp2MTaNPnzlsHFJK284dzU/fGQ/D+7q9XRONJHEMIyq2iUIwuzG05NLKfUdzC4S+RhADDgC/FJr/eR0jNFaX5X3/g7gjulccz4zkhaoaob4LD506WYe2dPHm779CNe8YCPvfdEmQoHCv396hie59rfPcNu2w3S2RPj6m87KzJ4SBEGw49WDGgHeApwIDGA2Y92A6eEsAc4FHlVKvaIKNgpTJDNNt8Jl5k4sbArzi3+4gMvPXMlX793F+3+ylWQq+zdNLJHim398lhde9wduf+IIV567mmDAx7t/8JdMx3VBEAQ7Xp9cG4HPpLs1ZFBK/QvwHK31K5RSVwP/DvymwjYKUyQzrHAGPCiA1oYQX3rd6Zy4rJXP3tHNwqYQn3nVKdy3s5d/u+0pdh8f45KuJXz8FSexpqOZLfv6ueLGh/jJo/u5+rnrZ8RGQRBmD16fXM/D7ByRzy3AJ9Kv7wK+WgmjhMowMkM5qHzefvF6eseifPOPu7n3mWMcGZpkbUcT37nqHF5w4pLMcWevaeectYv40SP7edtF6+q+ZZIgCDOL1xDfs8DrHLa/BrMcHMyR8McrYZRQGWaqSMKJj7z0RD57+SmctLyNj778RO78wMU54mRx+Zkr2d07xlOHZe21IAi5eH1y/T/gVqXUy4DHAB9wNvBXwGuUUmdgDjO8rhpGClMjM013hkJ8dnw+H288bw1vPG9N0eMuPXkpH/3VE/xxx3FOWbFghqwTBGE24MmD0lrfBZwM/AkzH7UG+COg0sML48Bbtdafr5ahQvmMTNbOg/JKZ0uEruVtPOCxPF0QhPmD5yeX1nq3UurzwCYgAOzSWg+k9z0FPFUdE4WpMhZNEAr4iATrdrkbABdt7OC7D+5jIpakMSz9/IRcJmJJAn4f4Tr/PRYqj6d/caVURCn1NaAPM8T3CNCjlPquUipcTQOFqTOaHlZY78UHF27sJJZMzashiIZh8OShIWKJVK1NqXu6PvE7Lv/6A7U2Q6gBXv8kuQ54GfDXwEKgHXgVcAHpGVFC/TE6Wb1GsZXk3HXtBP2+eTW244bf7+IVX7uf137zoZz1YoIzUkQzP/H69Ho98Bqt9R9t2+5QSo0DP8EsohDqjGp2Mq8kTeEgp65cwKN7yvOgRqMJrr97B13L23j1WSurZF3lGZ6Mc8PvnwVg24FB7nrqKC87dXmJswShvjk+EqUlEqxomN6rB+UHnLLYfUBLxawRKspsESiA89Z1sO3gIJNx79NUPn3bU9x03x4++LNt/P6ZY1W0rrLc+eRRJuJJfvXuC+hsCfO7p46WPkkQ6pxzPns3f/vfD1X0ml4F6h7gC0qpTB2wUmoh8Hng3opaJFSMag0rrAbnrWsnnjT4y/6BzLZkyuBXjx/k1q2HSOWFwXpHo/zq8UO84bzVrOts9tRJPZ6sj3zP/bt66WyJcMaqhVy8aTF/2nG84PMJwmxk+8Ghil7Pq0B9AHMh7iGl1Fal1FbMCbcrgGsqapFQMWaTB3X22kX4ffDw7myY79rfdvOBn27jfT/Zyrfu351z/B/0ceJJgzecu5o3n7+GrQcGeeaoe57i7qd7UB/7Le/50V9q2kXdMAweeraPCzd24PP5OG99OwPjcfb1j9fMJkGoV7yugzqEuQ7qDcCPgG8DlwNnaa33FztXqB2zpUgCoK0hxJmrF3HvMz0APLqnn2/dv4fXn7OK521ezA2/fzYn/PenHcdZ3Brh5BPaeOUZJ+D3we3bjzhe2zAMPv2bp0kZ5jEPPVu7Yoye4SjHRqKcuWohAKeuMH9uPzhYM5vqGSkgmd94XligtY5rrf9Xa/0fWuuvaa3/D1itlPpgFe0TpoFa1sppKxfW2gzPvPikpTx5aJhtBwb58C3bWLWoiY+/4iTe+bz1DE3Eubu7J3Ps4wcGOGftInw+H50tEZ6zoYPbtx9x9I529cfY3z/OZ151Cs3hALc/4SxkM4ElRKeuNKPlm5a2EA76efJQZUMjM8WhwQn+5ZdPlJU7LId6CcsKtWG6K99OBL5YCUOEyvP9t53HG85bXWszPPPas1fSGArwNzc8wMGBCa577ek0R4Kcu7ad1oZgpttE32iUA/0TnG4T35efupzdvWN0HxkpuO6WQ+OZY567aXFNCyqePjKMzwddy9sACAX8bFzcwo6e0ZrZNB0+f0c3P350f9kVmF5JiAc1r5Gl2ULd0NES4frXn8FzN3Xy1defybnr2gEIBvycv76DB3aZobntaW/D7h2+9ORl+H1wh4N3tKM3yrrOZtqbw5y3vp3DQ5McHZqs/gdyYPfxMU5Y0EiTbUbX5qUt7OwpFNbZwETM9JzGY9WZ6RWvwULmnzy6n7UfuV3mlNUBIlBCXXHpycv4/tvO47LTctcFXbSxk/394xzoH2f7gSF8vmyYDExxu2BDJ7c/URjm29EX5bT0saencz9bDwxW9XO4sbt3lPWLm3O2bVrayuGhSUYm4zWxaTpYHs54rEohvtTMC9Q3/2QW5PQM1+aPmKkwFk3UtPinWohACbOCCzd2AGaJ9raDg2xc3FJQAHLZacvZ0zvG00ey1Xw9w5P0jScz3tZJy9sI+H01yfkYhsGe42NsWJy7dHDTEvP9s8fHZtym6ZKsskAlkjP/0LXyXkF/fbcIszg2PMnJn7yTm+7bXfrgWYZriZdS6tMezt9QQVsEwZUNi1tY0hrh/l29bNk3wEtOXlpwzEtOXsYnbn2Sn//5ICe/0vSYtqU9pTNWme8bQgHWdDSx69jM53yOjUQZiyUdPSiAHT0jnJH28GYLVpvHaoX4aiFQs40j6XD1b7Yf4R0Xz61HcrEa5Od6vMafKmGIIBTD5/NxwYYOfr31MADnr+8oOKa9Ocxfn3YCP/vzAT5wyWYWNIXYfnAIvw9OWp4NB25a0sLOYzOf83n2uCmK6zpzBWp1exPhoH9GRPPXjx+idzTK1c9dX5HrhQJmEGYs6uxBGYbB4HicRc1T6yldixBfIO05zZYSd3/6r4TUHAzxuQqU1voFM2mIIJTistNOyAjUBRs6HY+5+rnr+eXjh/jRo/v5h+dvYNvBQdYuDOf0B9u0pJW7u48RS6RmdITD7nQIb31eiC/g97FhcQs7ZqBQ4v0/3QrAm5+zhkgw+508eWiIP+44zrufv6Gs7vdWDirmUg5+2/Yj/OOPH+c3773IcSDlQ8/2sbAplKlqzKcWHTZm2wPf+ueqgZZXHdf/O5VS71ZKef6/VykVUkpJVwmhalzStYQ3nLear/ztGSxb0OB4zEkntHHhxg5ufnAPsUSKJw4NsbkzknPMpqUtJFMGe/tmNuez+/gYDSE/y9sKbTcr+XI9qL/sH+Ce7p6q/CWfX8X4r79+ki/eqdnTW953YlXZJVwEyirpt+cF7Vx508O87Pr7XK9v/+gzVQRgpZ5mS4l7RqBmiaCWQzEBWgo8rZT6qFLqJLeDlFJdSqlPAc+kzxGEquDz+fjc5afyqjNXFD3u6ueup2c4ypfu0gyOxzlxca5AbUwXJeQLQrXZ3TvKus4W/A7J901LWjg0OJGpxrr2t8/w6q8/yNu++2c++b9PVuT+dhEZmsitGHz6sFk0cniwvMo1q6Ag7pIrshbwNoSm1uHa/tCdqZCb5UHOlhCfj9lRzDEVioX4PqmU+j7wYeBRpdQQ0I3Z1TwAdACnAk3AD4GXaa13VN9kQSjO8zcvRi1tzZQLn7cyN+ezYXELPh/pPFR1xlwcG55kR88oz9nQkclp7OkdcwxzQbZQ4pmjI9y/s5dv/PFZrjx3NfFkih8+sp93XryBVe1NZdnwpbs0jeEA737+RgCOj0Yz+wbHnUvaR6PllbpbAuUW4rP+uo9OsdNEjkAZhvcR4MBt2w7z3h8/ztOffknOurNSWH8/uAnUrx4/yM0P7OXWay4qwxphKhQN4Wmtd2mt3wksA94B3Ic5YuMoZhfztwCdWut3iDgJ9YLP5+Pf022N3nbROhY25v713hAKsHJRY9WKEnqGJ3n5V+/nTd9+hE/d9hQA0USSA/3jbMgrkLA4Z207fh985vanuf6eHVx+5go+d/kpfOjSzQD86vFDZdkQTST52r27+I/f6cw2e1gv34OydGBksrxqvFjac3IL8QX95iNmwkGgvITs7IeU69F85W7zkXRoYKKs87I5KOf9H/jpNrYdHKobD8ugPuyoBp7+rNBajwK3p/8ThLrn3HXtPPXplwLQ3d1dsH9tRzP7+qrTQfymP+1mYDzGOWsX8f2H93H1ReuZTCRJGYUFEhbtzWEu3NjJfTt72bC4mc+86hR8Ph/LFzRyygkLuH9nL//4ok2ebbCL0WQ8SUMokLPwdDBfoNI/y+2eUCrEZ3mPTr36vOR47AJVrh5Y4jjVXFIpAY0mkmV5ZkL5yEJdYV6yrrOZvX1jRR9CsUSKL975DG/61iP8ccdxT9dNJFP84i8Heekpy/jqlWfi9/n4yWP7eeaoWaG3OR3Kc+JLrz2dD79E8YOrz6PZtgj5go0d/GX/QFkNWQdsITwrnGcXLXvXCsMwMqG00TI9qFIhPuu6TgIW89DGyB7iK7cIYLrFA6XOmoM1CXWHCJQwL1nT0czIZIL+sZjrMZ+9/Wlu+P2zPHFoiHd8788c8DCz6c/7BhgYj3PZqctZvqCRCzd28r/bDvPMkWGznHyJc4gPYElbA+95wUaWL2jM2X7qigUkUkZZIcmB8eznsj7j0eEooUDao7F1fkikjGyIr1wPqkQVn3Vdp67k0TIFyiizjNoKwU21iKCUANVb1Vw5ywNmCyJQwrxkXadZcLDXJcz37PFRvvfwPq66YC2/e/9zMQz4nwf2lLzuo3v68fngok3mOq1Xnn4CBwcm+P7D+1jf2Zyz9sgrJy4zvS591Ps6qUGbQFmve4YnWdLaQGMowKRNHOxCUe7YjHjK3UOCrEg4dYTw5kHZX5cnCNa9p+xBlTivFimoeDLF4cHycmqzGU8CpZS6XCk1taXgglCHrOkwPZm9Lut+fvzIfkJ+P9e80PRoXnDiYm7bdqTkwtGtBwbZsLiFtoYQQKYl08hkgheeuGRKtq7taCYc9BedGJzPwFg2hNefFqijQ5MsW9BAYziQ6UIOuRV2XkTDTjYH5XyeJX5OHSGiidJiaBeJcvUgmT53qsUMpUN8U1eowfEYdz/dU/rAPD7+6ye54Np7502nda8e1NeAHqXU/yilLilnAa8g1COrFjXh98E+h8W6hmFwxxNHuHhzJ50t5hqqF5+0jN7RKDuLhNkMw2DrgcGcfnqtDSE+dlkXy9oaeN05q6ZkazDgZ9OSlkweywv2Iggrr9QznBaoUCCnqi7q4k15wQrxuQmUtd3Jg3LzuuzYj0iU2Sph+h5U8f3T8aDe9YMtXP29P9NrK/33wr3phc/l5gpnK16FZhXwN8A48APgsFLqa0qpC6pmmSBUkXDQz4pFjexxCPE9e3yUw0OTXNKVXXd+7lpzNtWje90H8x3on6B/LMaZqxfmbL/6uet5+KMvKuhiXg5qWWtZIb4JW/PW8VgSwzA4OjzJsrYGIiF/jkDZvabyPajiIb5YkRxVTn7JRQ3sHuu5n72H7z+8j//z6HmkpilQpc6bTg5qf/r3rtyQ6mxrwzRdPAmU1trQWv9Ja30NcALweiAJ/J9Saq9S6vNKKe81sIJQB5il5oUe1MO7TRE6z9aQdlV7I50t4Ux3dCcePzAAUJWO5F3L2jg2Ei0o6tiyb4AHdvUWPODHY0laG8xKwIl4kpFogvFYkmVtpgcVrYAHZRhGpnqvlAcVd3A37A9Zt1Lw/M0f//WTvP17f3b8d8sn2xGi5KGOuGlAJVoLWd1Eyu2fNwfrIIpSVqhOKdUIXAH8A/BW4DCmR7UE2KKU+mDFLSwTpdR/KKX+rdZ2CPXP2o5m9vQWlpr/Zd8AnS0R1nZkOzf4fD5OXNZW1IvpPjJCKOBDFSklnyqbllrtmbL3v3XrIa648UHe+K1H+PmfD+YcPxFP0tYQIuD3MR5LZErMlzqG+Jxfl8IuKq4eVNLdg7LnhtzyRG6e1fCE9xDX1HNQzudZGjEdJ8byhCqxyHYuO1OeVpkppV4DvA54OTAM/BR4sdb6UdsxjwFfAL5cBTs9oZR6BxCq1f2F2cXaTrPUfGA8TrttHMRTh4c5ZUVbQdmuWtbKDx/ZRzJlZBag2tnTO8qajmaCgcqnaK3+gbuOj3Le+g4m40k+fdvTnLZyAbFEiuvv2clr/2plxuaJWJKmcICmUIDxWDITUlrd3kRDKJAzvymaF+J78Nlezl3bXvJz2L0mNw8qkek04eBB2U6JJ1OO/frctKXYGI5LvvxHVi5qzHgbUy1mcPegfGBbOzala8/h7g+VxOsy6JuAX2Lmoe7VWjt9u1uAL1bKsGIopa4CrrJtuhFTOJuAW4Hnz4QdwuzG8pD29I5lBGoynmTX8VEuOamw4k4tbWUynuLgwHimCtDO7uNjBbOeKsUJCxppDAUya6Fu23aYvrEYX7vyTPb1j/Mvv3yCZ4+PZYRsPC1QVsXe/n67QPnpH7OF9eLm68ZQgEf29POGmx7hvS/cyIcuVUVtsntNbl5KZqFuiRCfqwfl8iAvlivbdWyUXcdGWdVuridLVlqg0j/rpNPRnMarQP0z8D2tdU6rY6VUM/A2rfVXtdaPAY9V2kAntNY3Azfn2fJzzD6BLwROUEr9Smu9bSbsEWYna9Nisq9vjLPXLALMAolkynCcT2Qdv7evUKCSKYN9feNTLiUvhd/vY+MSc2aUYRjc/OBeNi9t4TkbOjKjR/6ybyAjUBPp9kZNYdOD+sOO4yxra2BRU4hw0J/T+SGWNMN6rQ3BTOjPS0GG3WtyE5jsOiiHEJ9ROkToJgJe5kT5p9mVvJSXM50y87ncgbySFBv5vgSwyo5uBB5XSvXlHXYGZljvq1Wxrgy01q8FUEo9H3i+iJNQCqvU3L4Wam+v6Wms7yysuFud7ia+v28MWJyz7/DgBLFkqmCceyU5ZcUCfrP9MI/u6eepw8N87vJT8fl8GYHqsxVQTMSSdLaEaQwHOTI0wWN7B/jHF23C5/MRCvhzxMXyoFobghwbMcuevYzH8CRQRVodpTzkoNzCaF766001V+Qr0Sw2Gzos77qVZL44b6VGvv+c7HfxCLnfi/Xvf3MlDFFK+YDvAE9qra9Lb7sM+DwQAbZjemtFVytqrf8A/KESNglzG6vU3N5NYk+vGUJb21k42mJJa4RI0O/YZHZ3WuTWOQhbpThj1QJ+/Oh+Pvm/T7GgMcTl6blYjaEAoYAvp0P5eCxBU7iJpnCC7QfNWU9npqsLwwF/TojMykG1NmTTt/ndzp2wi4qbkFgi5LSGyS4Abmuc3LwUL17RdOc6le4kMV9konYUmwf1C6XUWsxKv93AuYC9Y6YBjGqt3ReGeEQp1QXcAJwPPJnethhTsC7UWu9USn0BuBZ493TvZxGNRh07Xc8nJicn5/x3UOwzdkag+2BfZv/WZ4/R0RRg37M7HY9f2hzgqX09dHfnhmge6jZFIDFwiO7J8jsEeGGxYYrGM0dHeM3JC9j7bHbCTXPIz74jx+juNh+aI+NRouMjpGLJjAClho7S3d3H2MgQE9F45jPvPWD+zedPZheNHuodKvl7cWTEtCfk9zEZizsePxk1vbqh4dGC/buPZFv26B27GGkrrG/ad8C5nHzv/v10G/kBnVxisVj6Ggfo9vV7/l2PTpqZjP37D9CdKryHJVw7dz3L+LGp1WRZtu3yeI1EymAkmiQeN7/zZ3ftYrjZfHzv6Ytm7K6H/5craUPRHJTWen/6ZbU7R7wHU4z227ZdCjymtbaeFDcC25RS73Ep0iibSCRCV1dXJS41a+nu7p7z30Gxz3iyTnDbtiOZ/f2/72fTsgWux699aITB8VjB/p/seJKWyBDPOfOUqjXt7ALe3R/miUNDfPyKs1jQlH2wtbf24I+0ZOxKcpClne0wHIW0EFx41sk0hAIs2ZkiuX8ic+xjg3uBXpZ3LITD5rFjSX/J34uG3jHgAJFQAL8/9/gD/ePEkikCwSNAgsamppz9/+/n27hly5HM+3Xr1zuOIjlo9ACFgr9s+Qo2bl5KKF1peMPvd3HqigVcvHkx5t/TEImEgTgrVqygq2u559/1yF29QIxVq1bR5ZBT9Pv3QirF2nXrMzm/cgmHjwIJNm7c4Fhwk88Hf7qVXz5+iMWtESDJtqEGvnTLDro//VKSbaPAISINDTX+f9n83qdiw5YtWxy3F8tB/Ql4pdZ6MP3aFa31xWVblHv+Nel7vsi2eRVwwPb+INAGtGJW7AnCtFnT3szQRJzB8RgLm8Ls7RvnJScvcz1+aWsE7dAT79DgZLq0ubrJ73966YmO2xc0hnLCcrFkilDAT2PYzCVFgv5MXikSzM1BxRLZHJRF32gMwzCKfh4rdBYK+Aoq5S7+4u8xDFjW1pBzrMUtW3LXbbkWQ7iE0f7lV0/wDz/8C3uvvQyAL95pDma03kM2BzHVhbpu985+IzMX4rv9CVPMre/xq/eaf7fbZ3zNRYp5UPcAMdvrmcbNa5va7GhBcGBNutTcyiv1j8VYX6RUfNmCBo6PREkkUznrhHqGJ1mafhjXggWNIfpGs0US8WSKcNBPQ9C0sa0x622FXHJQLbYZVLFkiuHJBAsa3cNP1gM8FPAXTMy1nu2WcE21bZBbHsiacRVPpgg6rEkDWxVflXrxVSIF5fUa+RWJ5h8OuWux5mJdYLEc1KecXlukc0S9lQq3ObAfOM/2fgUwoLUu3eNEEDxihVf29Y9n/mdfW0SglrQ1kDLMijm7IB0dnuQkh9L0maK1IZRTvBFPGoQCvowH1WbzjsJBPymDjMhai2jtQxIBekejRQUq60H5GXPprm0dU6pQwbXIosTTxZoW7ESmJVGFu5lnWx1N6bI51/BKINMaKS1Q6e1zvUzD67iNpUqpHyqlzlBKhZVS9wBHgWeVUidXyba7gPNtPf7ehbkIVxAqhr10fG+fVYlXxINKi5J9Om08maJ3NJop964FTaHsCI1kyiCZMggFsmG9fA8KsqXfiVQKnw+awrkPevtndMISnUjQ71r2bXWsKNW43K0xRCnPayKeLHlMuV0bsgtxK3vd6eDW/2+uFxJ6LX74OmZOaAD4O+Bs4GLgTsxRHBVHa30Ms9/fLUqpbuBU4EPVuJcwf2kMB1jSGmFf3zh7jo/h92VFywlLoOyx/2MjUTPfUkOBagxn2xdZ+SW7QNnDd+F02C/TaTxlEPT7iKS3L09/jkMDxQfj2UN8bg/zyfQaq6mWbJcceZEq1vFhag1Zy7l3rfD75ocP5bWTxCXAeVrrfUqpy4HbtNYPKKWOAE9Uyhit9VV57+8A7qjU9QXBiTUdTezrG2cykWLloqbMA9yJpW3mfCi7QFmeRq0FysoDWQIVDvgzf3nbJ/mG02Pf7Y1cg/6smJ2wsJGe4UkOlpjcmgnxBX0lQ3hTD/EVPy+RSpW+95TdjOp7UJ6vYOQeXw+LhWcCrx5UAvCnWxu9gKxoLAW8D6kRhDpkTUcz+/rH2H18tGj+CaCjJULA7+Ook0DVsEiiMRQgnjSIJ1OZnFIo4KPRVrlnkfGgMqMyDIIBH5GQP3NsZ0uEo0PleFBZL8nJW5qqgJV6ACdT7k1bp9ss1rWTRNozm444TFdY5rowWXgVqLuBbwO/xqzsu00pdQnwP0heSJjlbFjcQs9wlKcOD9O1vPiojIDfx+KWCD3D2UWtlljVUqCs/NFEPJkN8dlKy4OBbFbeEihrGm4yHeJrSHtZfp+P1oZgybHiVvl2OJ3TskTGaaZU6So+t+2lha1UscJUixlmporP20XcjprrOuVVoN4OPAqMAX+jtR4FTgZuB95fHdMEYWawT8A9bcVC1+Msli5oyAnx9QxPEgn6WdhUu0kvVrXeZCyZ8YxCAT9WBbbfVjZmFUlkQnwps5rPEi6fz8xZjUaLr+iwBMk6zwqlOU2JLeVBuU7U9eJBTTF8WArXeVAVGFiYvUeZx891RcrDUw4q3f/ufXnbrq+KRYIww5y2ckHm9Rl549qdWNYWYY+tweyRoUmWLWio+iLdYlihvPFYMvPQCwf8mTEi9o4HlseTHcduEPL7Mg/egN9HcyToWjpuYQ/xQbZowMmD8hKqc6KUh5EoGuIr3vTVDa9l5NPRinJzSNmSCCPn51zH68DCCOb8pXMwBwLm/J+otf67ilsmCDNEUzjId956Dm0NQVYsbCx5fGdLhMf2DmTe9wzVdpEuZEN847FkJpwXCvi5ePNivvv353Lhhuz4+lAw34MyCAR8jEyagtQcDhIKpOgfK2yKa8feSQKyHpRTyXmpQgU3MfAibKWuPfWBhcU7SVSmWew0Q3y2HXNxHLzXEN+3MSflLgJSmN0c7P8JwqzmBWoJZ69p93Ts4tYI/WOxTK7nyPBEpjS7Vli5pol4MuMZWcLxvM2Lc7peRPI8qHgyRcjvz8zEessFa2mNeMhB5XlQyXRxhlPIrVpVfMmUUXqwYJkulFfdmclOEm7Hz3VPymuZ+auAV2mt/6+KtgjCrKCzxSw17x+LsaTVLJioZYEEmF4gmHOgrLxTyKVc3tpuCWwyZVbxrVzUlOll96vHD2XWMLmRcslBOYnRVPNEJXNQHkavV69IYuZyUPn3KvVvM1fwKlADwKFqGiIIswWzozQcH4lm+trVcg0U2EvHk4ST5utwwFmg8nNQ8aRBwJ97bCToJ5bwWCSRV8U3lRBfuQML7efZz7U/yKdazOD1vIoE+Mr1oKZ5/mzDq0B9GrheKfU+zJ7qMftOrfX8kHNBwCZQo9HMw6zWHpRddOLBbBWf47EFnSRSmXCg/ZhYiTbgBUUSRRrD2i/l5Hm4PWhLPX8TydwQn5POTfUh7ho6tIovptOMz7pHuTJXwc83G/AqUJ/AXJTr1jWi9HxoQZgjLG7JelDWw3ZpnXhQ0USKSDI3B5VPphefrZFrIK8jeCjgy+l47oQlOqF8D8qh8Z5dlBxDgGV2M7dfy35uqgIeVObe09zv6R6yYLcoXgXqTVW1QhBmEVYOqnc0mnkY17pIImITqHiihAflUiSRe0wgp+O5E1bYLpODSjl7UD5fbojPMQToFuLz0MbIfq6z+BW9RAGlhK0SVXyZsvHpCpQUSYDW+o8ASqkVwGbgYaBNa12d2daCUMc0hgO0RIIcH4kymS5KsLyqWmEP21ldyt16CoaC5uPRXiSRL2b2dkhuApUpkrDKzF1yUKGAP0c4nATKvZOE8/bs/txOEo/s6Xc8phwyh8+AC+VVYLImzW1BysfrOqgWzJHsV2CWmW8G/jM9E+pV6c7jgjBvWNwaoXc0xlg0QWdLxPUhPlPk5KCS3jyouK0XX0OoMAdlXa8p7HxP+zwocK/iCwf8OZ5QwiG3NdUy81TKyAkDfuq2pzKvrc1TXgdVQgymk4Ka6qlzPaSXj9f/q74EdALrAKuD5Icwv+evVsEuQahrOlvCHB+Z5MjQZM3De0Cm0WssmbK1OnLJQTkWSbh4UEXyUJl1UEGrk4SzQOWPhI875KimGi7LX6hrF8LMVN8pj3wvvr8i3cwrsObKsmM+L9R9JfBBrfU+a4PWehfwbuDSahgmCPWM5UHVetS7hZMHVbLMPDNuwygYm24t5nVqW2SRyvOgEi4CFUx3O7dIOAxSmmqZecowcuYy2cOHhu2YcijZhqgCE3UrxVz3qLwKVCN5peVpIuS1PRKE+YDZ0XySA/0TrCoy4HCmCKYbw8ZsRRJuYcdMFV8imzMKOpSZQzYM6IRbkUTpEJ/3MvPSzWJzBch+beue1QrxVWShbpmXcBLbuSxSXgXqVuDzSqmF6fdGehT714DfVMMwQahnVrU3MTKZYCKeZG1H7QUKsmuX4rZ5UE4E/D4Cfl9GfKyBhfnXAoquhcovkrAenvmLcvNDfM5FElPzoPI7SdhFxbpnpTtJVKoCD6YfJqxHcdrTO8bwZLwi1/IqUO8F4kAf0AxsBZ5Jv39fRSwRhFnEhsXZ7uBrOooPOZwpwumuFrESRRLmPl9Os9gCDyrgIQeVXySR8aByzwkF/BhG1uNwKpKY6sDCVN48KJ8toGMJ6JTXQZXsJFEfZeb1plEvuO4PvOq/HqjItcoZt3GFUmo90JU+T2utn6mIFYIwy8gVqHrxoALmOihPAuXPGbeRn4PyViSRvRbYPKi8U+wCFgz4HIskXEN8Hnr4uQnQtD0ol+3ZThJTu66XexQe5+ZhTt+GarDbNo5mOngtM18PnAS0AsPAVnvBhCDMN1YsamRhU4iNi1tYXQc5KLD655kCZYXxih2bCfGlCtc6eRGo/GaxVv6n0IPKncvk5C259eor9fzNH1iYE+Kbbg6qZBXf9JluHqsSebB6pqhAKaVeCPwncAq5xRCGUmoLZmXf/VW0TxDqkoDfx4MfeSGNoUBNBxXaseeg3PJPFqGAXaDcPaiolyKJgnVQhfeCrIcVd3A9prwOynAf+V7tEF9NJuo6vK+P377q4BoDUEpdCtwJbAOeh7kOKgR0AC/CzEHdrZR6zgzYKQh1R1M4WDfiBFYOypwHVSy8B04hvvxWR+XnoCzdyS8jz6/y+8u+gYJrTbWTRH4Vn10zEhmBKn6NfKw8Vsl1UJUokphuDsqovxxUJSnmQX0C+LLW+p/ztg8AfwD+oJQ6BHwMuKw65gmC4JVwOsSXSKVc10BZhGy5IKdu5pm1TQ75IovsOqjcibr5nkV+p4nP3N7teq18SjaLNXJDfE6NYyvdLDa7TqoiQb7pnT2X1YniVXynA98tcf4PgbMqZ44gCFMlE+JLFPbWKzw2kLNQNz9fZVX1OS2qtXDrJJEvalkPq4jYTaPVkf2y9tfZHFTRSxRg+SSlq/imz3Q7Sdg3++ZgsK/Yb3ETUNh5MZdeYHHlzBEEYaqEA36icbNIwmoI636sOU7DMIx0mXnuo8Dqbu5UcWdhCY6Vv3LrZh7KaybrxFTLzJMpg2eODjteJ2GzJ5pIMl6hKbQV6WaedsPcrjA8GecXWw4WbM+/pflvMHfdqGIhPh9mY9hizPUcnSDMGsJBP2OxBLGktxxUPJnK5pHcPKgSRRIBvw+/LzfE59TN3L7fial2kkgZRk7I0O712IskXvuNh9h+cIi9p51c/ILYc1AlPKgq5qD+5RdPcPsTR1DLWjllxQL386dvQk2xQtJulCozv1IpNVJkf9uUrBIEoeKEbWXmpXJQ4XSZuSUmgYCzQMWLej0Q8GXL2S1xyA/lWdcq9kB3LTMvs5JuUXOY4clEzjVTBmw/OFT0Os73dt6eWQdVkVZHztfoGZ4EYDyWNI9zPX9256GuvOlhtuwb4BevXea4v5hA7Qfe7+Ee+6dglyAIFSYrUKVzUKGAn7FYMruo1+8c4ivmQaUMA7+frAeVqeLL86D8uWXmbtcqZ7tFvnn2cnnrD/Opjmafief+dAsxzP2zN4i1o6eY/1NEoLTWayttjCAI1SMS9Gc6SXhZBxVLZEN8rkUSRXJQyZSR9qDS713mQVn5MGt7a0OQK85ayc0P7s0c4yYiXkJ8duziaBWBTLWbuZc1WFOlVKsjK8xY6g6zPQP18lOW88cdx13313bKmiAIFSOSruLzsg4qHDSbxbo1ls10PC9WxZcy8NtyUG7zoKw1VtbD2Km1kpsQeSmSsBN3WLdVrgOVHXTo7bjp4OYhlRz5UUEbakmpVVwiUIIwR7AKH+LJlOu4d4twXpFEfhWfJSBF10GliyQCeVV8Tt3Mc/Y7VA1OdR6U/TyfzzlnVolckZ1sFd/0rzXdEF/KMLKCOkt9qWJr3UWgBGGOEA74iZeRg4rbKqjyQ3wBv4cqvnSIL7+KL+myDsraH89bGOzzuT+IveVg0vfx+x3ttV+inHxUOfcumxIeUjkhvrmMCJQgzBFCQT/xpOEtB5UOB2Y8qDyB8vl8ZreJEotr/f7CKr58D8rylgzDIJky/+K3t1by+3yuVXwpo9A2O/bzggGfo8dn96CKlbpbZHNQxfdXcx6U9xCfMacbxopACcIcIRQoIwcVyFb8QaEHBaaIlOVBpbLb7WGbkD+736oatM+fCvh8RXrxmSLoboPdXp/jgEWn7hJeKN3NvBJl5tM8n9ntRZX6/CJQgjBHsCbbTsSTHtdBGTYPqvB4t9lNFsmUKWx+hyq+gE2hLA8qle5aARSE+Fx78VHCg7IVcYQC/pLTesvJR7kLkLdmst7uMc3zXRrlziaK+fqe5kHNBpRSzcBXgEFgGfA+rXWpVk2CMGewvKaxaMJDDsr0NqwcVP5EXet6xVb5W+ugAg5VfAG/r0CMkikjk58K2AQx4Pe5CodhFPYJtGPvth4M+Bw9JPtD3IsH5bWKr5oLdTMhvhISZl+oO1sFqhhzyYN6OzCC2UPwaREnYb5hidJ4LFmyF18o4CeZMjIPeCcvJej3EU94WQdVWKVnF5VQIFtmbpWt2z0ov89XsODWIpVyDj9a2EN6Tl6gdQ2n1254bhZbzSo+q0hizpeZF2dWelBKqauAq2ybbgQU8IzW+nql1HVKqedprf9YC/sEoRZYXcXNAYSlQ3wAk3HnKj5IV/qV6Gbu9/syOaLMyPc8rydoG8dhFTHkFkkU7yRRLMQXTdhDfM7HlVskkS3bLnVc9RQq60EVJ2UYs7a83KLYTLVZKVBa65uBm+3blFKbMcfRAxwDWmbWKkGoLWHbA9rLOiiAibjZt84tB1VqHpS9SCIjUPkelD87sNCpSMJfJMSXMrKtlJyw58jy11bZr2FRiRCfV/EoRqaTRMkQXon9Lq/nCrNSoFz4BvANpdSZmP/+X6qxPYIwo9jzTl5aHUG2GelUclCWEAUcqviCDh6UNdoj3z6zis+tSKK4B2XvHOF2XE6Hcy8elMdjp9rjL+de063iM+yvZ59ElTK5bgRKKeUDvgM8qbW+Lr3tMuDzQATYDrxNaz3sdL7W+jhwxQyZKwh1h91r8rJQF2DCEii3HFSJThJ+X7aKz+5B2b2ezELdlJEpWw/4/Xz378+lIejnPT963DUHZRgULTO356DcvMacEJ8nDyqdg3LZn/V+pk+pjumlzzfmpuuUpi4ESinVBdwAnA88md62GFOwLtRa71RKfQG4Fnh3pe4bjUbp7i4cPz2fmJycnPPfwXz4jAA9R8Yyrwf7e+nuTroe23vM7CK998AhAA7s30fzRE/OMcl4lMGhhOt3NzQ8QjyWZKfWABztOUZ3d5y+gQGMVMJm12HzXvv20Rf2p7cdYtOaFkiCkUwwMDDgeJ+BwSGSibjr5xgYynbDjkcnHY8ZHhnNvN6xcydDLSHX64H5+wLQ29vraFMiYX62I0d76O52vmcpYrEYAAcOHKDbV1jPNTZq2rx//wG6U30YLsJ64OBBBiMB0+46e555sWVoaJB43P3fty4ECngPphjZR3dcCjymtd6Zfn8jsE0p9R6tdUX+ZohEInR1dVXiUrOW7u7uOf8dzIfPCHDUdwwwRWbFsmV0da13PXZH9BBwnLb2xUAfG9evp2vlgpxjWn7fT0Mk6PrdNT40Qswf5+STuoA9tHd00tW1mdbtUSJ9SRibAGDt6lVADytXraatIQQcYu3q1XR1LQUgHD5Ma9sCx/u0bp2kccSAkUTBPoBQpBEw79Pa3AzHowXHNDU3Z45Zt34DazqaXb8XgPDvjgFx2js6HG0KBg8BSZYsWVL0Oy56jzt6gDgrVq6kq6twFlLLQyPABKtWraLrxCX4/XsdF16tWLGShU0h4AgNNX+e7c5558WWtieihPrc/5CqC4HSWl8DoJR6kW3zKuCA7f1BzAGJrWSLIQRBSFNODiqcl4NyrOLz+zNFDU6YRRJmOMreTy9pOOegUoaRqQoM5pWZF+skUWwlZ7QqIb7cn6WOmw6uYcS8dVBuET/7Z5uFKShg9jaLdbPNXW4FYR5jF6VQqSq+TJl5suBci1JVfPZqPXs/vUQqtz1ROCcHlS6SsJeZ+4sUJJSo4ot5KTO3r4Mqo0hiuotoveDeLDZ3v89FpQ3bMbO93NyJehao/cBy2/sVwIDWeszleEGY14SmUiQRd/egggF/8ZHvRrYYImBbbJsqqOLLLtRNuPbic18HVaRGIsfDc/vMuR6U+7UsMkUSJTtJlL6WG9kiCG8XcfsO5kKro2LUs0DdBZyvlNqUfv8u4NYa2iMIdY29/16pXnwFZeYO66BCfl/xke92D8rmBSXyqviCtlZHVpm5XcDMThLuAuXmPUCuB+UW4rM/uD2F+PJ+5uO1y0MxSk7U9eXew62qzyzim8XKVML0uhUorfUx4K3ALUqpbuBU4EO1tUoQ6pfcHFTpibqQLTMPTCXEZ+sYEbCJTCpl5HhImYW6hmHr/WcP8ZVYqOuhF9+33/JXGdFrjeSm1sttFmsdUs2R75l7uWzP/8Ru34BdnGarTBXLQdVFkYSF1vqqvPd3AHfUxhpBmF3k5KBKFkmYpcnjMauThFuIr7gHZXlKfn9WoBJ53czt86LSEcU8D8q9R57hMcQXsLVcCgf9YCvmm/I6qBKdJCpB6ZEe1k3dz5+LoT2LuhIoQRCmTo4HVaJIIhJKdz4vslDXDPG5P/0StlyT3+fLPNhTRm6ro2wzWbAeuXZbSw0s9FIkEfT7M8flh/pyWh2VUSRRikp0knDzwryPfM++rmUnianeu9RZIlCCMEeIBL3noBqCpgc1Fi3Wi8/DwEIrxOe3VfEl8wXK/JkyjMwDOb/M3O0BZ3alcP8cUZsHFXARqJxWR2WUmbuKh3VcySuVxus13ETaMGZ1Bgpwr1CEOs5BCYJQHuXkoBrSHtRoWqCcclClRr4nUkbmPvaRGfndzK0Ef8owMq2TgnkeVrEiCS85qGDAFuILFPGgPBVJeK3iq0SZefEslLXXzYk0XF7PFUSgBGGOkFtmXjxR0hDO96CcevEVX6ib60EVDixsSRcrBGwClXQqkvC5l2wbJUJ8FgG/L+OpFYb4bDmossZtuIXfKlDFV+Ij5Yf4XA+f5QpVKjQoIT5BmCPkFkl4C/GNTKY9KMciiVI5qFRG2AK2PJIpUH5+9/7nsrNnNCMwyVR2PEbIXiRRtIqveIgvY6vf5ykH5WlgYYlOEtkiiuov1LVsdyszTxnGrOxibmfWVPEJgjB17N0ZvIx8D/h9mYF/jkUSgeIelD3X5Pf7cj0oH6xc1MTKRU0cHBgHTA8ru1A3t0iimEB56eztt82lyg/xGWV7UFaxh8v+vJ/ToVQGKVXCg6oXB6paGikhPkGYI9hzNY2hQNFjfT4fDWlPI+D3OYpA0O/LLKx1ImFb72QXmXgyVVClB+bDNrNQN6+ThGsOKuXeRSHH1kB29Hy+B2W/tpciiewhzsd6XSflBa/dKlxzUMasjOx5RgRKEOYgVhFEMRrTeSin8B6YXk4y5R5CStpGy5tVfOb2eDKVkw/LlJnbiiTs3p6vSA4qv2TdDTPEZ76OFMtBeRKo4kUSpfaXQ8mpvZkD3EN82WvNPqkqZbEIlCDMQSIlPCiASDoP5TaJ1hoh7za0MJFM2ZrFZr2TeNLICbNlPSgyRRL566TcPBtrKGIpAn5/xoO0Plf2GtnX5ayDKtK/tuC6U8W9hi+/1VGR82efLuVQ7F9XBEoQ5iBePCjrmGIeFOA69t0+2t1eKh5Ppgo6RYApYBkPqmDchvtCXZ/Px95rL+PjrzgpZ5/dU7IXSeR7UHb7va2Dyi44dt6f/lmRbubFF+qWykHZVbSmOagqXVcEShDmIKUW6kI2xNfg4m1ZIuPmQcVTRmb9lL0bhFuIL2UYxJIpQoHcnJffFh7Mp1iro3DePayHeX4Oyl6J6MWDSmUEyI0Khvg82uI+D2p2N4st9R2KQAnCHMRL5ZtVau7mbVmFDm7dJOwelL0bRCyRyhFIX6bM3GAynszc1yLgc/ckkkVCfJE8gbIuYReoizcvzul4XolefNky9KkLQ+bUEvfIelDunSQKrlkDptPqqNjvqpSZC8Ic4htvOsvV48nH8qDcKv6sSjunSj7DMHKKJBpCfibjqczx9hBethefwWQ8VZAfKzpuI5XNYeU/BO25pnDAnwnfWTZ1toRZ0hrhLzaB8lJ5lyohQJXIQWX1qfhFMgt1i+Sg6qE2YqomGIZRNAclAiUIc4iXnrK89EFpWhvM//3dQnyWB2X3QCzyWxa1NIQYGo+l9+WWmVsCOBZLEk0kCzw2c6Gus43mol/z9dlrFuXss3tKkZC/IHwXCvjx+8jzoJzvYydTpeey3+tAw2KUOxTR7SGe0yy2hqG+qX4XBhStkpAQnyDMU1a3NwNZocpnSWsEgKPDkwX7rB5+1rmtDUFGogkSyZRZxZcXfmuJBBmZjBONpwoE0V4BmE8smcp4SmeuXkT3p1/KCQsagMLmuNY1LIExBcpHLFlukYT1071ww36fqVAiwldgi/vAwvoI8U0H8aAEQShgfacpUEvbGhz3r+kw9//9dx5jdUeTuV4pZT5UhyfiAHS0mCK2sDHE7uNjXPqffwJgcVrcLBY0hvjOA3sBOHdte86+9uYIdz7Vw2VfvS+z8NR68O7pHeP89dnjG8MBNi5t5fDQJM3pXn+NoQDBgD/zfsOSFsIBPx952Yncv6s3514f+eV2vv/wPsD0OJIp816pdMjSMLLi++uth3nm6Ehmn1kmbzCU/uy3bTvMn/f1m9fKiFr2Xtly9ULl2H18DID/uncXP3xkH6mU1bbI/Lnz2CgAX7l7B9+6bzeHBicKrgFw03276Rk2h18dHJjgFV+7L2dGlP27zLETw/EYe9l6/vb8c3M+q8NnfNn192VCwfbPZn2PhmHQNxZjdXuT42cDEShBmLe84vTlHBuZ5K9PP8Fx/9qOJv7xRZt4+vBQRjj8PvOved+iRi7a2MkLT1wCwFsuWMvAeIxoPMVZaxYVXPOfXqq4bdthwMdVF6zN2XfluavoG42STBnpXIsPn8/8y3rD4hbeeN6anOM/+6pT+M32I5x8Qhvfe2gfF27sAOCqC9YyMpnglaefwJvPN89Z2BTi2PAkLZEgsfERJv2NOdcyWySlBx76zPuesWohzZEghwcn8KfHeFgDEQM+OHddOxOxJJPW9EXyc0S+gm323T4frGpv4pHdfahlrTl2WDZsXNLCb588ypmrFxIK+Ola3sbd3T1cvHkx93T38JKTl9EzPEl7c4QzVsGdT/Xw3E2dhAP+zHdo3ctnu6/Pvt1n2ZY9yEfWW/NhPyZ3O7ZzrWN+9+TRjLivam9k5aLGzGfyp7/fzHvb6ws2dkDyKE74ZuPq40rR3d1tdHV11dqMmtLd3c1c/w7mw2cUSiO/B/XLli1btpx99tl/lb9dclCCIAhCXSICJQiCINQlIlCCIAhCXSICJQiCINQlIlCCIAhCXSICJQiCINQlIlCCIAhCXSICJQiCINQl83qh7pYtW44D+2pthyAIwjxnzdlnn704f+O8FihBEAShfpEQnyAIglCXiEAJgiAIdYkIlCAIglCXiEAJgiAIdYkIlCAIglCXiEAJgiAIdYkIlCAIglCXiEAJgiAIdUmw1gYItUEpdRnweSACbAfeprUerq1VM4NSajlwPXAEGNFaf6zGJgk1Qim1AvgZ8Eat9d4amyPkIR7UPEQptRj4DnCF1loBu4Fra2vVjPJO4Cat9fuA1UqptTW2R6gBSqk24MPAYI1NEVwQgZqfXAo8prXemX5/I/BGpZSvhjbNJMuBvenXB4EVtTNFqBVa62Gt9fuB47W2RXBGBGp+sgo4YHt/EGgDWmtjzoyzD/M7AFgJHKqhLYIguCA5qPmJ2x8myRm1onb8N/A1pdQVwC7JPQhCfSICNT/ZD5xne78CGNBaj9XInmmTDk9+B3hSa31deptjIYjWuhe4smbGClWjnN8D6xyt9VU1MFXwgIT45id3AecrpTal378LuLWG9kwLpVQXcA/wOtu2+V4IMu+Q34O5hwjUPERrfQx4K3CLUqobOBX4UG2tmhbvwXwI/cy2bb4XgsxH5PdgjiEhvnmK1voO4I5a21EJtNbXACilXmTbXKwQZF6s95pvyO/B3EM8KGGuMt8LQQQT+T2YxYhACXOV/ZjrnSxmfSGIMCXk92AWIwIlzFXmVCGIMGXk92AWIwIlzEnmYCGIMAXk92B24zMMo9Y2CIIgCEIB4kEJgiAIdYkIlCAIglCXiEAJgiAIdYkIlCAIglCXiEAJgiAIdYkIlCAIglCXiEAJgiAIdYkIlCAIglCXiEAJgiAIdYkIlCAIglCXiEAJgiAIdYkIlCAIglCXyERdQZgiSqm9wBqX3ZcDXwE+o7X+1kzZlI9S6qq0DStrZYMgTBURKEGYHh8CfuSwfQB4ABidWXMEYe4gAiUI02NYa33UZd/xGbVEEOYYIlCCUCXSIcDPaK2/pZTyA58DrgZ8wH8CVwFXa63/oJSKAF8A3oiZG74HeK/WukcptRbYA7wmfcxK4F7g74B+4ADwb1rrm2z3fhr4OnkenFLqFcCngZOAKPA74O1a62Gl1L8Bl2itL3L5DD7gX4F/AFqAh9I27qzE9yUI+UiRhCDMDP8CvAVTgC4BXgGst+3/HPCc9PbnYf6/+Zu0KNiv8cb0/rOBD2utU8DPgFdbBymlTgE2Az+3G6CUWgf8AvgGcCLwWuCFmGPQvXANpii+GTgP2AXcq5Rq8ni+IJSFeFCCMD3+Syn1lbxtgw5FCe8GPqm1vhNAKfUW4Jn06ybMh//5WuvH09veDPQBF2F6SACf0lo/kt7/Q+Cc9PafAPcppRZorYcwhecPae/LbkMQeJ/W+r/T7/cqpe4GTvb4Wf8J+Eet9b1pG94LvBy4Avi+x2sIgmdEoARhenyKPE8FSNrfKKU6gROAx6xtWmutlBpIv10PhDFFxn5qA6YnZAnUs7Z9w0Aofa1HlFIHgL8GfoApUF/ON1RrvVMpFVVK/StwCqYwnQz8uNSHVEq1YIYWf6iUSjnYKAgVRwRKEKbHca31rhLHJNI/fXnbrffW/4fPA4byrw8sSr+OuZwP8FPgCqXU48BGzFBeDkqp0zErC28D7sMUsffbDjEcbA/m/Xw98HTeMYMO5wnCtJEclCBUGa31IHAYM28EgFJqPbAw/fZZTK+rU2u9Ky14xzEFxG2dVT4/Bl4M/C1wl9Z6wOGYNwMPaK2v1Fp/XWv9GLCJrNDFgFabjc3AEttnOAYst9m4BzN3drpHGwWhLMSDEoTp0aaUWuawfSzv/deAT6ar4o4BX01vN7TWI0qpmzDzWe/EFLNrgdOAncDSUkZorZ9QSu0DPoBZZedEH3CKUuo8zOq/d2Hmsfan9z8GfEYp9TrgceCT5IYrvwz8u1KqB3gS+DCmKL6/lH2CMBXEgxKE6fEl4IjDf5/MO+46zLDbzzFLxG/HDP1ZYbsPAXdhhuoeAxqBS7XWE2XY8mMgANzqsv+rmCG+/wMeBNZi5tDOTO+/J/15volZQv5M+nj7Z/gGcAOwHTOP9RKt9eEybBQEz/gMwynsLAhCJVFKvRTYorU+nn6/GNOTWqe13ltL2wShXhGBEoQZQCn1K8yqu3/CLEb4NLBGa31uTQ0ThDpGQnyCMDNcgxnSexB4GDMUd3lNLRKEOkc8KEEQBKEuEQ9KEARBqEtEoARBEIS6RARKEARBqEtEoARBEIS6RARKEARBqEv+PyIilTPGFXhAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# From PyHessian\n",
    "\n",
    "def density_generate(eigenvalues,\n",
    "                     weights,\n",
    "                     num_bins=10000,\n",
    "                     sigma_squared=1e-5,\n",
    "                     overhead=0.01):\n",
    "\n",
    "    eigenvalues = np.array(eigenvalues)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    lambda_max = np.mean(np.max(eigenvalues, axis=1), axis=0) + overhead\n",
    "    lambda_min = np.mean(np.min(eigenvalues, axis=1), axis=0) - overhead\n",
    "\n",
    "    grids = np.linspace(lambda_min, lambda_max, num=num_bins)\n",
    "    sigma = sigma_squared * max(1, (lambda_max - lambda_min))\n",
    "\n",
    "    num_runs = eigenvalues.shape[0]\n",
    "    density_output = np.zeros((num_runs, num_bins))\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        for j in range(num_bins):\n",
    "            x = grids[j]\n",
    "            tmp_result = gaussian(eigenvalues[i, :], x, sigma)\n",
    "            density_output[i, j] = np.sum(tmp_result * weights[i, :])\n",
    "    density = np.mean(density_output, axis=0)\n",
    "    normalization = np.sum(density) * (grids[1] - grids[0])\n",
    "    density = density / normalization\n",
    "    return density, grids\n",
    "\n",
    "\n",
    "def gaussian(x, x0, sigma_squared):\n",
    "    return np.exp(-(x0 - x)**2 /\n",
    "                  (2.0 * sigma_squared)) / np.sqrt(2 * np.pi * sigma_squared)\n",
    "\n",
    "def get_esd_plot(eigenvalues, weights):\n",
    "    density, grids = density_generate(eigenvalues, weights)\n",
    "    plt.semilogy(grids, density + 1.0e-7)\n",
    "    plt.ylabel('Density (Log Scale)', fontsize=14, labelpad=10)\n",
    "    plt.xlabel('Eigenvlaue', fontsize=14, labelpad=10)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.axis([np.min(eigenvalues) - 1, np.max(eigenvalues) + 1, None, None])\n",
    "    plt.xscale('symlog')\n",
    "    plt.tight_layout()\n",
    "\n",
    "get_esd_plot(density_eigen, density_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 96/96 [2:34:50<00:00, 96.78s/it]   \n"
     ]
    }
   ],
   "source": [
    "top_evals = []\n",
    "hessian_traces = []\n",
    "\n",
    "xs = xs.to('cpu')\n",
    "ys = ys.to('cpu')\n",
    "\n",
    "for model in tqdm.tqdm(models):\n",
    "    model = model.to('cpu')\n",
    "    ref_model = ModelWrapper(model)\n",
    "    hessian_comp = hessian(ref_model, F.mse_loss, data=((xs, ys), ys), cuda=False)\n",
    "\n",
    "    _top_evals, _ = hessian_comp.eigenvalues(top_n=3)\n",
    "    trace = hessian_comp.trace()\n",
    "\n",
    "    top_evals.append(_top_evals)\n",
    "    hessian_traces.append(trace)\n",
    "\n",
    "    model.to('mps')\n",
    "\n",
    "xs = xs.to('mps')\n",
    "ys = ys.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities_over_time = []\n",
    "\n",
    "xs = xs.to('cpu')\n",
    "ys = ys.to('cpu')\n",
    "\n",
    "steps_to_models = dict(zip(steps, models))\n",
    "models_subset = [steps_to_models[step] for step in highlight_steps]\n",
    "\n",
    "for step, model in tqdm.tqdm(zip(highlight_steps, models_subset)):\n",
    "    print(step)\n",
    "    model = model.to('cpu')\n",
    "    ref_model = ModelWrapper(model)\n",
    "    hessian_comp = hessian(ref_model, F.mse_loss, data=((xs, ys), ys), cuda=False)\n",
    "    density_eigen, density_weight = hessian_comp.density()\n",
    "    densities_over_time.append((density_eigen, density_weight))\n",
    "\n",
    "    get_esd_plot(density_eigen, density_weight)\n",
    "    plt.show()\n",
    "\n",
    "    model.to('mps')\n",
    "\n",
    "xs = xs.to('mps')\n",
    "ys = ys.to('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Essential Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try this for restricted subsets of weights as well\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def extract_weights_over_checkpoints(models: Iterable[nn.Module], extract_weights: Callable = lambda m: [p.flatten() for p in m.parameters()], normalize=False):\n",
    "    for model in models:\n",
    "        weights = torch.cat(extract_weights(model)).detach().cpu().numpy()\n",
    "\n",
    "        if normalize:\n",
    "            weights /= np.linalg.norm(weights)\n",
    "\n",
    "        yield weights\n",
    "\n",
    "\n",
    "\n",
    "def get_pca_weights_trace(models: Iterable[nn.Module], extract_weights: Callable = lambda m: [p.flatten() for p in m.parameters()], num_components=3, normalize=False) -> Dict[str, Tuple[PCA, np.ndarray]]:\n",
    "    weights = np.array([w for w in extract_weights_over_checkpoints(models, extract_weights, normalize=normalize)])\n",
    "\n",
    "    pca = PCA(n_components=num_components)\n",
    "    weights_reduced = pca.fit_transform(weights)\n",
    "\n",
    "    return weights_reduced, pca\n",
    "\n",
    "all_weights, weights_pca = get_pca_weights_trace(models, num_components=3, normalize=False)\n",
    "\n",
    "plot_multiple_slices(\n",
    "    steps, \n",
    "    all_weights, \n",
    "    weights_pca, \n",
    "    highlight_steps,\n",
    "    transitions_of_steps,\n",
    "    connect_dots=True, \n",
    "    save=None,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_embedding_weights = lambda m: [p.flatten() for p in m.token_sequence_transformer.token_embedding.parameters()]\n",
    "extract_unembedding_weights = lambda m: [p.flatten() for p in m.token_sequence_transformer.unembedding[1].parameters()]\n",
    "extract_embedding_unembedding_weights = lambda m: [p.flatten() for p in m.token_sequence_transformer.token_embedding.parameters()] + [p.flatten() for p in m.token_sequence_transformer.unembedding[1].parameters()]\n",
    "extract_block_0_weights = lambda m: [p.flatten() for p in m.token_sequence_transformer.blocks[0].parameters()]\n",
    "extract_block_1_weights = lambda m: [p.flatten() for p in m.token_sequence_transformer.blocks[1].parameters()]\n",
    "extract_lns = lambda m: [m.state_dict()[f\"{ln}.{part}\"] for ln in layer_norms for part in [\"weight\", \"bias\"]]\n",
    "extract_mlp_0 = lambda m: [p.flatten() for p in m.token_sequence_transformer.blocks[0].compute[0].parameters()] + [p.flatten() for p in m.token_sequence_transformer.blocks[0].compute[2].parameters()]\n",
    "extract_mlp_1 = lambda m: [p.flatten() for p in m.token_sequence_transformer.blocks[1].compute[0].parameters()] + [p.flatten() for p in m.token_sequence_transformer.blocks[1].compute[2].parameters()]\n",
    "extract_attn_0 = lambda m: [p.flatten() for p in m.token_sequence_transformer.blocks[0].attention.attention.parameters()]\n",
    "extract_attn_1 = lambda m: [p.flatten() for p in m.token_sequence_transformer.blocks[1].attention.attention.parameters()]\n",
    "\n",
    "extract_weights_fns = {\n",
    "    \"Embedding & Unembedding\": extract_embedding_unembedding_weights, \n",
    "    \"Unembedding\": extract_unembedding_weights, \n",
    "    \"Embedding\": extract_embedding_weights, \n",
    "    \"Block 0\": extract_block_0_weights, \n",
    "    \"Block 1\": extract_block_1_weights, \n",
    "    \"Layer norms\": extract_lns,\n",
    "    \"MLP 0\": extract_mlp_0,\n",
    "    \"MLP 1\": extract_mlp_1,\n",
    "    \"Attention 0\": extract_attn_0,\n",
    "    \"Attention 1\": extract_attn_1,\n",
    "}\n",
    "\n",
    "for label, extract_weights in extract_weights_fns.items():\n",
    "    print(label)\n",
    "\n",
    "    for normalize in [False, True]:\n",
    "        print(f\"Normalize: {normalize}\")\n",
    "        subset_weights, weights_pca = get_pca_weights_trace(models, num_components=3, extract_weights=extract_weights, normalize=normalize)\n",
    "\n",
    "        plot_multiple_slices(\n",
    "            steps, \n",
    "            subset_weights, \n",
    "            weights_pca, \n",
    "            highlight_steps,\n",
    "            transitions_of_steps,\n",
    "            connect_dots=True, \n",
    "            save=None,\n",
    "        )\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = run.checkpointer.file_ids\n",
    "embedding_matrices = []  # Shape: (64, 5): 64 vectors x (1 y dim + 4 x dims)\n",
    "\n",
    "for model in models:\n",
    "    embedding_matrices.append(model.state_dict()['token_sequence_transformer.token_embedding.weight'])\n",
    "\n",
    "\n",
    "embedding_vec_x_norms = [vec.norm(dim=1) for vec in embedding_matrices]  # (64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA explained Variance over time\n",
    "pcas = []\n",
    "\n",
    "for model in models:\n",
    "    embed = model.token_sequence_transformer.token_embedding.weight.detach().cpu().numpy()\n",
    "    pca = PCA(n_components=embed.shape[1])\n",
    "    proj = pca.fit_transform(embed)[:,:3]\n",
    "    pcas.append((proj, pca))\n",
    "\n",
    "explained_variances = [{\"value\": value, \"index\": idx, \"step\": step} for step, (_, pca) in zip(steps, pcas) for idx, value in enumerate(pca.explained_variance_ratio_)]\n",
    "explained_variances = pd.DataFrame(explained_variances)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(data=explained_variances, x=\"step\", y=\"value\", hue=\"index\", palette=\"viridis\", ax=ax)\n",
    "ax.legend(title=\"Component\", loc='upper left')\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"Explained Variance of Embedding Vector PCA over Time\")\n",
    "ax.set_ylabel(\"Explained Variance\")\n",
    "ax.set_xlabel(\"Step, $t$\")\n",
    "\n",
    "fig.set_facecolor('white')\n",
    "plot_transitions(ax, TRANSITIONS, limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the final PCA\n",
    "\n",
    "def compute_explained_variance(pca, embed):\n",
    "    proj = pca.transform(embed)\n",
    "\n",
    "    # Step 4 and 5: Compute variance of projected data and total variance\n",
    "    variance_projected = np.var(proj, axis=0)\n",
    "    total_variance = np.sum(variance_projected)\n",
    "\n",
    "    # Step 6: Calculate explained variance ratio\n",
    "    explained_variance_ratio = variance_projected / total_variance\n",
    "\n",
    "    return explained_variance_ratio\n",
    "\n",
    "explained_variances_rel_last_pca = [{\"value\": value, \"index\": idx, \"step\": step} for step, embed in zip(steps, embedding_matrices) for idx, value in enumerate(compute_explained_variance(pcas[-1][-1], embed.detach().cpu().numpy()))]\n",
    "explained_variances_rel_last_pca = pd.DataFrame(explained_variances_rel_last_pca)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(data=explained_variances_rel_last_pca, x=\"step\", y=\"value\", hue=\"index\", palette=\"viridis\", ax=ax)\n",
    "ax.legend(title=\"Component\", loc='upper left')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"Explained Variance of Embedding Vector PCA over Time\")\n",
    "ax.set_ylabel(\"Explained Variance\")\n",
    "ax.set_xlabel(\"Step, $t$\")\n",
    "\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "plot_transitions(ax, TRANSITIONS, limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's project embedding vectors onto these pca components and track their evolution\n",
    "last_pca = pcas[-1][-1]\n",
    "transformed = [last_pca.transform(embed.detach().cpu().numpy()) for embed in embedding_matrices]\n",
    "transition_middles = [get_nearest_step((t[0] + t[1]) * 0.5) for t in TRANSITIONS]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(transition_middles), figsize=(20, 5))\n",
    "\n",
    "min_x, max_x = 0, 0\n",
    "min_y, max_y = 0, 0\n",
    "\n",
    "for ax, middle in zip(axes, transition_middles):\n",
    "    middle_idx = steps.index(middle)\n",
    "    middle_embeddings = transformed[middle_idx]\n",
    "    sns.scatterplot(data=pd.DataFrame(middle_embeddings), x=0, y=1, ax=ax)\n",
    "    ax.set_title(f\"Step {middle}\")\n",
    "\n",
    "    min_x = min(min_x, middle_embeddings[:, 0].min())\n",
    "    max_x = max(max_x, middle_embeddings[:, 0].max())\n",
    "    min_y = min(min_y, middle_embeddings[:, 1].min())\n",
    "    max_y = max(max_y, middle_embeddings[:, 1].max())\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlim(min_x * 1.25, max_x * 1.25)\n",
    "    ax.set_ylim(min_y * 1.25, max_y * 1.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Assuming 'pcas', 'embedding_matrices', 'TRANSITIONS', and 'steps' are defined as in your context.\n",
    "\n",
    "last_pca = pcas[-1][-1]\n",
    "transformed = [last_pca.transform(embed.detach().cpu().numpy()) for embed in embedding_matrices]\n",
    "transition_middles = [get_nearest_step((t[0] + t[1]) * 0.5) for t in TRANSITIONS]\n",
    "\n",
    "min_x, max_x = min([t[:, 0].min() for t in transformed]), max([t[:, 0].max() for t in transformed])\n",
    "min_y, max_y = min([t[:, 1].min() for t in transformed]), max([t[:, 1].max() for t in transformed])\n",
    "\n",
    "# Set up the figure.\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.xlim(min_x * 1.25, max_x * 1.25)\n",
    "plt.ylim(min_y * 1.25, max_y * 1.25)\n",
    "scat = ax.scatter([], [])\n",
    "\n",
    "# Update function for the animation.\n",
    "def update(frame):\n",
    "    middle_embeddings = transformed[frame]\n",
    "    ax.clear()\n",
    "    ax.set_xlim(min_x * 1.25, max_x * 1.25)\n",
    "    ax.set_ylim(min_y * 1.25, max_y * 1.25)\n",
    "    ax.set_title(f\"Step {steps[frame]}\")\n",
    "    sns.scatterplot(data=pd.DataFrame(middle_embeddings), x=0, y=1, ax=ax)\n",
    "\n",
    "# Create the animation.\n",
    "ani = FuncAnimation(fig, update, frames=range(len(steps)), repeat=False)\n",
    "\n",
    "# To save the animation, you can use the following line:\n",
    "ani.save(FIGURES / 'M1-embed.mp4', writer='ffmpeg', fps=1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W_U, W_E gives us a 5x5 matrix\n",
    "# The rows act, respectively, on y, and the x components. \n",
    "# So we can see what the cossim of these vectors is with the task vector. \n",
    "\n",
    "task = run.pretrain_dist.task_distribution.tasks[0]\n",
    "task_np = task.detach().cpu().numpy()\n",
    "task_embed = np.zeros(5)\n",
    "# task_embed[1:] = task_np\n",
    "print(task_np)\n",
    "\n",
    "embed_unembed = [\n",
    "    model.token_sequence_transformer.unembedding[1].weight.detach().cpu().numpy() @ model.token_sequence_transformer.token_embedding.weight.detach().cpu().numpy()\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "cossims = [\n",
    "    # (v @ task_embed) / (np.linalg.norm(v) * np.linalg.norm(task_embed)) for v in embed_unembed\n",
    "    np.mean(v ** 2) for v in embed_unembed\n",
    "]\n",
    "\n",
    "norm_ratio = [\n",
    "    np.linalg.norm(v, axis=0) / np.linalg.norm(task_embed) for v in embed_unembed\n",
    "]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(run.checkpointer.file_ids, cossims, label=[\"$y$\", \"$x_1$\", \"$x_2$\", \"$x_3$\", \"$x_4$\"])\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Step, $t$\")\n",
    "ax.set_ylabel(r\"Cosine similarity between $(W_U W_E)_i$ and $\\mathbf{t}$\")\n",
    "ax.set_title(\"$W_U W_E$ over time\")\n",
    "\n",
    "ax.legend(loc='lower left')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(run.checkpointer.file_ids, norm_ratio)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Step, $t$\")\n",
    "ax.set_ylabel(r\"$\\|(W_U W_E)_i\\| / \\|\\mathbf{t}\\|$\")\n",
    "\n",
    "plot_transitions(axes, TRANSITIONS, limit=True)\n",
    "\n",
    "fig.set_facecolor('white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_stats = []\n",
    "\n",
    "for step, model in zip(steps, models):\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    bias_stats.append({\n",
    "        \"step\": step,\n",
    "        \"postn_embedding_norm\": state_dict[\"token_sequence_transformer.postn_embedding.weight\"].norm().item(),\n",
    "        \"unembedding_ln_bias_norm\": state_dict[\"token_sequence_transformer.unembedding.0.bias\"].norm().item(),    \n",
    "    })\n",
    "\n",
    "bias_stats = pd.DataFrame(bias_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postn_embeddings = []\n",
    "unembedding_biases = []\n",
    "\n",
    "for step, model in zip(steps, models):\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    _postn_embedding = state_dict[\"token_sequence_transformer.postn_embedding.weight\"]\n",
    "\n",
    "    for i, p in enumerate(_postn_embedding):\n",
    "        postn_embeddings.append({\n",
    "            \"step\": step,\n",
    "            \"postn_embedding_0\": p[0].item(),\n",
    "            \"idx\": i,\n",
    "            \"postn_embedding_x_std\": p[::2].std().item(),\n",
    "            \"postn_embedding_x_mean\": p[::2].mean().item(),\n",
    "            \"postn_embedding_y_std\": p[1::2].std().item(),\n",
    "            \"postn_embedding_y_mean\": p[1::2].mean().item(),\n",
    "        })\n",
    "\n",
    "    _unembedding_bias = state_dict[\"token_sequence_transformer.unembedding.0.bias\"]\n",
    "    for i, p in enumerate(_unembedding_bias):\n",
    "        unembedding_biases.append({\n",
    "            \"step\": step,\n",
    "            \"unembedding_bias\": p.item(),\n",
    "            \"idx\": i\n",
    "        })\n",
    "\n",
    "postn_embeddings = pd.DataFrame(postn_embeddings)\n",
    "unembedding_biases = pd.DataFrame(unembedding_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 2))\n",
    "\n",
    "axes[0].matshow(models[-1].token_sequence_transformer.postn_embedding.weight[:, ::2].T.detach().cpu().numpy(), aspect=\"auto\")\n",
    "axes[1].matshow(models[-1].token_sequence_transformer.postn_embedding.weight[:, 1::2].T.detach().cpu().numpy(), aspect=\"auto\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.matshow(models[-1].token_sequence_transformer.unembedding[0].bias.reshape((1, 64)).detach().cpu().numpy(), aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "# sns.lineplot(data=bias_stats, x=\"step\", y=\"postn_embedding_norm\", ax=ax)\n",
    "sns.lineplot(data=postn_embeddings, x=\"step\", y=\"postn_embedding_0\", hue=\"idx\", palette=\"gray\", ax=ax, alpha=0.5)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "ax = axes[1]\n",
    "# sns.lineplot(data=bias_stats, x=\"step\", y=\"unembedding_ln_bias_norm\", ax=ax)\n",
    "sns.lineplot(data=unembedding_biases, x=\"step\", y=\"unembedding_bias\", hue=\"idx\", palette=\"gray\", ax=ax, alpha=0.5)\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "plot_transitions(axes, TRANSITIONS, limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches\n",
    "from icl.model import to_token_sequence, from_predicted_token_sequence\n",
    "\n",
    "class EmbedUnembedOnlyV2(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.to_token_sequence = to_token_sequence\n",
    "        self.token_embedding = model.token_sequence_transformer.token_embedding\n",
    "        self.positional_embedding = model.token_sequence_transformer.postn_embedding\n",
    "        self.unembedding = model.token_sequence_transformer.unembedding\n",
    "        self.from_predicted_token_sequence = from_predicted_token_sequence\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        tokens = self.to_token_sequence(xs, ys)\n",
    "\n",
    "        T = tokens.shape[1]\n",
    "        x = self.token_embedding(tokens) # + self.positional_embedding.weight.T[:T, :]\n",
    "        # Set everything to zero except for dimensions 46 and 51\n",
    "        # embedded[:, :, :46] = 0\n",
    "        # embedded[:, :, 47:51] = 0\n",
    "        # embedded[:, :, 52:] = 0\n",
    "\n",
    "        x = self.unembedding[0](x)\n",
    "        unembedded = self.unembedding[1](x)\n",
    "\n",
    "        # raise ValueError(\"Done\")\n",
    "    \n",
    "        return self.from_predicted_token_sequence(unembedded)\n",
    "\n",
    "\n",
    "# embed_unembed_only_model = EmbedUnembedOnly(run.model)\n",
    "\n",
    "def get_embed_unembed_with_bias(model, multiplier=1.):\n",
    "    eu_model = EmbedUnembedOnlyV2(model).to('cpu')\n",
    "\n",
    "    w = np.zeros(4)\n",
    "    basis = torch.eye(4, device=\"cpu\") * multiplier\n",
    "    ys = torch.zeros(1, 1, 1, device=\"cpu\") \n",
    "\n",
    "    for i in range(4):\n",
    "        w[i] = eu_model(basis[i].unsqueeze(0).unsqueeze(0), ys)[0].item()\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "embed_unembed_with_bias = [\n",
    "    get_embed_unembed_with_bias(model, 10)\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "print(embed_unembed_with_bias[0].shape, task_embed.shape)\n",
    "\n",
    "cossims = [\n",
    "    (v @ task_np) / (np.linalg.norm(v) * np.linalg.norm(task_np)) for v in embed_unembed_with_bias\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(run.checkpointer.file_ids, cossims)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"step\")\n",
    "ax.set_ylabel(r\"Cosine similarity with $w_1$\")\n",
    "ax.set_title(\"Effective weight using only embedding (token only), unembedding (linear only), with biases\")\n",
    "\n",
    "# plt.legend(loc='lower left')\n",
    "plot_transitions(ax, TRANSITIONS)\n",
    "fig.set_facecolor('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0].state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.analysis.slt import prepend_keys\n",
    "\n",
    "layer_norms = [\n",
    "    \"token_sequence_transformer.unembedding.0\",\n",
    "    \"token_sequence_transformer.blocks.0.layer_norms.0\",\n",
    "    \"token_sequence_transformer.blocks.0.layer_norms.1\",\n",
    "    \"token_sequence_transformer.blocks.1.layer_norms.0\",\n",
    "    \"token_sequence_transformer.blocks.1.layer_norms.1\",\n",
    "]\n",
    "\n",
    "list(model.state_dict().keys())\n",
    "\n",
    "def get_ln(model, key):\n",
    "    return (model.state_dict()[f'{key}.weight'], model.state_dict()[f'{key}.bias'])\n",
    "\n",
    "unembedding_lns = [get_ln(model, 'token_sequence_transformer.unembedding.0') for model in models]\n",
    "block_1_attn_lns =  [get_ln(model, 'token_sequence_transformer.blocks.0.layer_norms.0') for model in models]\n",
    "block_1_mlp_lns =  [get_ln(model, 'token_sequence_transformer.blocks.0.layer_norms.1') for model in models]\n",
    "block_2_attn_lns =  [get_ln(model, 'token_sequence_transformer.blocks.1.layer_norms.0') for model in models]\n",
    "block_2_mlp_lns =  [get_ln(model, 'token_sequence_transformer.blocks.1.layer_norms.1') for model in models]\n",
    "\n",
    "def ln_norm(weight, bias):\n",
    "    return torch.norm(weight).detach().cpu().numpy()\n",
    "\n",
    "def ln_norm_std(weight, bias):\n",
    "    return torch.std(weight.abs()).detach().cpu().numpy()\n",
    "\n",
    "unembedding_ln_norms = [ln_norm(weight, bias) for weight, bias in unembedding_lns]\n",
    "block_1_attn_ln_norms = [ln_norm(weight, bias) for weight, bias in block_1_attn_lns]\n",
    "block_1_mlp_ln_norms = [ln_norm(weight, bias) for weight, bias in block_1_mlp_lns]\n",
    "block_2_attn_ln_norms = [ln_norm(weight, bias) for weight, bias in block_2_attn_lns]\n",
    "block_2_mlp_ln_norms = [ln_norm(weight, bias) for weight, bias in block_2_mlp_lns]\n",
    "\n",
    "unembedding_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in unembedding_lns])\n",
    "block_1_attn_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in block_1_attn_lns])\n",
    "block_1_mlp_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in block_1_mlp_lns])\n",
    "block_2_attn_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in block_2_attn_lns])\n",
    "block_2_mlp_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in block_2_mlp_lns])\n",
    "\n",
    "def frac_nonzero(weight, eps=1e-1):\n",
    "    return (weight.abs() > eps).float().mean().detach().cpu().numpy()\n",
    "\n",
    "unembedding_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in unembedding_lns]\n",
    "block_1_attn_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in block_1_attn_lns]\n",
    "block_1_mlp_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in block_1_mlp_lns]\n",
    "block_2_attn_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in block_2_attn_lns]\n",
    "block_2_mlp_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in block_2_mlp_lns]\n",
    "\n",
    "ln_stats = []\n",
    "\n",
    "def get_stats(weight):\n",
    "    return {\n",
    "        \"norm\": weight.norm().item(),\n",
    "        \"norm_std\": weight.abs().std().item(),\n",
    "        \"std\": weight.std().item(),\n",
    "        \"mean\": weight.mean().item(),\n",
    "        \"max\": weight.max().item(),\n",
    "        \"min\": weight.min().item(),\n",
    "    }\n",
    "    \n",
    "\n",
    "for step, model in zip(steps, models):\n",
    "    for layer in [\"unembedding.0\", \"blocks.0.layer_norms.0\", \"blocks.0.layer_norms.1\", \"blocks.1.layer_norms.0\", \"blocks.1.layer_norms.1\"]:\n",
    "        weight, bias = get_ln(model, f\"token_sequence_transformer.{layer}\")\n",
    "\n",
    "        ln_stats.append({\n",
    "            \"step\": step,\n",
    "            \"layer\": layer,\n",
    "            \"layer_pretty\": layer.replace(\"_\", \" \").title(),\n",
    "            **prepend_keys(get_stats(weight), \"weight/\"),\n",
    "            **prepend_keys(get_stats(bias), \"bias/\"),\n",
    "        })\n",
    "\n",
    "ln_stats = pd.DataFrame(ln_stats)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.lineplot(data=ln_stats, x=\"step\", y=\"norm\", hue=\"layer\", palette=\"deep\", ax=ax)\n",
    "\n",
    "# Fill between using the std\n",
    "for layer in [\"unembedding\", \"block_1_attn\", \"block_1_mlp\", \"block_2_attn\", \"block_2_mlp\"]:\n",
    "    ax.fill_between(steps, eval(f\"{layer}_ln_norms\") - eval(f\"{layer}_ln_norms_std\"), eval(f\"{layer}_ln_norms\") + eval(f\"{layer}_ln_norms_std\"), alpha=0.2)\n",
    "\n",
    "ax.legend(title=\"Layer\", loc='lower left')\n",
    "plot_transitions(ax, TRANSITIONS)\n",
    "ax.set_title(\"Layer Norm Weight Norms over Time\")\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(100, 500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Unembed\", \"Block 1 Attn\", \"Block 1 MLP\", \"Block 2 Attn\", \"Block 2 MLP\"]\n",
    "\n",
    "for i, lns in enumerate([unembed_lns, block_1_attn_lns, block_1_mlp_lns, block_2_attn_lns, block_2_mlp_lns]):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "    label = labels[i]\n",
    "    lns_df = pd.DataFrame([{\"step\": step, \"idx\": i, \"weight\": weight.item(), \"bias\": bias.item()} for step, (weights, biases) in zip(steps, lns) for i, (weight, bias) in enumerate(zip(weights, biases))])\n",
    "\n",
    "    ax = axes[0]\n",
    "    # inset_ax_1 = ax.inset_axes([0.1, 0.1, 0.4, 0.4])\n",
    "\n",
    "    sns.lineplot(data=lns_df, x=\"step\", y=\"weight\", color=PRIMARY, ax=ax)\n",
    "    # sns.lineplot(data=lns_df, x=\"step\", y=\"weight\", hue=\"idx\", palette=\"gray\", ax=inset_ax_1, alpha=0.1)\n",
    "    ax.set_title(f\"{label} Layer Norm Weights over Time\")\n",
    "    ax.set_ylabel(\"$u_i$\")\n",
    "    ax.set_xlim(100, 500_000)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    # inset_ax_2 = ax.inset_axes([0.1, 0.1, 0.4, 0.4])\n",
    "\n",
    "    sns.lineplot(data=lns_df, x=\"step\", y=\"bias\", color=PRIMARY, ax=ax)\n",
    "    # sns.lineplot(data=lns_df, x=\"step\", y=\"bias\", hue=\"idx\", palette=\"gray\", ax=inset_ax_2, alpha=0.1)\n",
    "    ax.set_title(f\"{label} Layer Norm Biases over Time\")\n",
    "    ax.set_ylabel(\"$\\mathrm{unembed bias}_i$\")\n",
    "    ax.set_xlim(100, 500_000)\n",
    "    \n",
    "    plot_transitions(axes, TRANSITIONS, limit=True)\n",
    "    # plot_transitions(np.array([inset_ax_1, inset_ax_2]), TRANSITIONS, limit=True)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(\"Step, $t$\")\n",
    "        ax.set_xscale('log')\n",
    "        \n",
    "    # for ax in [inset_ax_1, inset_ax_2]:\n",
    "    #     ax.legend().remove()\n",
    "\n",
    "    # for ax in [*axes, inset_ax_1, inset_ax_2]:\n",
    "    #     ax.set_xscale('log')\n",
    "\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# act_stats_over_time = act_stats_over_time[act_stats_over_time[\"step\"] > 0]\n",
    "\n",
    "resid_stream_layers = [\n",
    "    \"token_sequence_transformer.token_embedding\",\n",
    "    \"token_sequence_transformer.blocks.0.resid_after_attn\",\n",
    "    \"token_sequence_transformer.blocks.0\",\n",
    "    \"token_sequence_transformer.blocks.1.resid_after_attn\",\n",
    "    \"token_sequence_transformer.blocks.1\"\n",
    "]\n",
    "\n",
    "try: \n",
    "    del act_stats_over_time\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "act_stats_over_time = []\n",
    "\n",
    "for step, model in zip(steps, models):\n",
    "    hooked_model = hook(model)\n",
    "    output, act = hooked_model.run_with_cache(xs, ys)\n",
    "\n",
    "    for layer in resid_stream_layers:\n",
    "        #print(act[layer].shape)\n",
    "        act_stats_over_time.append({\n",
    "            'mean': act[layer].mean().item(),  # mean over batch, over tokens, over activations\n",
    "            'abs_mean': act[layer].mean(dim=-1).abs().mean().item(),  # mean over batch and tokens of abs mean over activations\n",
    "            'var': act[layer].var().item(),  # var over batch, over tokens, over activations\n",
    "            'batch_var_of_mean': act[layer].var(dim=-1).mean().item(),  # mean over batch and tokens of var over activations\n",
    "            'batch_var_of_var': act[layer].var(dim=-1).var().item(),  # var over batch and tokens of var over activations\n",
    "            'max': act[layer].max().item(),\n",
    "            'min': act[layer].min().item(),\n",
    "            \"step\": step, \n",
    "            \"layer\": layer,\n",
    "            \"layer_idx\": resid_stream_layers.index(layer),\n",
    "            \"dim46\": act[layer][:, :, 46].abs().mean().item(),\n",
    "            \"dim51\": act[layer][:, :, 51].abs().mean().item(),\n",
    "        })\n",
    "\n",
    "act_stats_over_time = pd.DataFrame(act_stats_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_metrics_to_plot = [\n",
    "    (\"$\\overline{|\\mathrm{mean}[z^{(l)}_t]|}$\", \"abs_mean\", {}),\n",
    "#    (\"Mean over batch, token index, and activation index of residual stream activations\", \"std\", {}),\n",
    "    (\"$\\overline{\\mathrm{var}[z^{(l)}_t]}$\", \"batch_var_of_mean\", {}),\n",
    "#    (\"Std over batch and token index of std within residual stream activations\", \"batch_std_of_std\", {}),\n",
    "#    (\"Dim 46\", \"dim46\", {}),\n",
    "#    (\"Dim 51\", \"dim51\", {}),\n",
    "]\n",
    "\n",
    "layers = act_stats_over_time[\"layer\"].unique()\n",
    "slopes = np.zeros((len(layers), len(run.checkpointer.file_ids[1:])))\n",
    "\n",
    "slopes_list = []\n",
    "\n",
    "for (_, key, _) in more_metrics_to_plot:\n",
    "    for j, layer in enumerate(layers):\n",
    "        values = act_stats_over_time.loc[act_stats_over_time[\"layer\"] == layer][key].values\n",
    "        slopes[j, :] = dlog_dlogt(run.checkpointer.file_ids[1:], values)\n",
    "\n",
    "    slopes_list.extend([{\"layer\": layer, \"step\": step, key: slopes[j, step_idx]} for j, layer in enumerate(layers) for step_idx, step in enumerate(run.checkpointer.file_ids[1:]) for slope in slopes])\n",
    "\n",
    "slopes_df = pd.DataFrame(slopes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, len(more_metrics_to_plot), figsize=(20, 6))\n",
    "\n",
    "# act_stats_over_time = act_stats_over_time[act_stats_over_time[\"step\"] > 0]\n",
    "\n",
    "for i, (ax, (metric_name, key, kwargs)) in enumerate(zip(axes, more_metrics_to_plot)):\n",
    "    sns.lineplot(ax=ax, data=act_stats_over_time, x=\"step\", y=key, hue='layer', palette='viridis')\n",
    "\n",
    "    ax.set_title(metric_name + \" over Time\")\n",
    "    ax.set_xlabel('Time Steps')\n",
    "    ax.set_ylabel(metric_name)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend().remove()\n",
    "\n",
    "    # sns.lineplot(ax=axes[1, i], data=slopes_df, x=\"step\", y=key, hue='layer', palette='viridis')\n",
    "    # axes[1, i].set_title(metric_name + \" Slope over Time\")\n",
    "    # axes[1, i].set_xlabel('Time Steps')\n",
    "    # axes[1, i].set_ylabel(metric_name + \" Slope\")\n",
    "\n",
    "plot_transitions(axes, TRANSITIONS, limit=True)\n",
    "\n",
    "for ax in axes[:2].flatten():\n",
    "    pass\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor(\"white\")\n",
    "\n",
    "# axes[1, 0].set_ylim(-10, 10)\n",
    "# axes[1, 1].set_ylim(-5, 5)\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Union, Iterable, Optional\n",
    "from torchtyping import TensorType\n",
    "from devinfra.utils.iterables import map_nested\n",
    "\n",
    "from icl.experiments.utils import iter_models\n",
    "from devinfra.utils.iterables import flatten_dict\n",
    "\n",
    "from icl.train import Run\n",
    "\n",
    "def compute_attention_entropies(attn: TensorType[\"B\", \"H\", \"2K\", \"2K\"]):\n",
    "    \"\"\"\n",
    "    Computes the entropy of each token in each head, averaged across the batch, \n",
    "    then averages this over heads. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Threshold attention weights to avoid log(0)\n",
    "    log_attention = torch.where(attn > 0, torch.log(attn), torch.tensor(0.0).to(attn.device))\n",
    "    entropy_per_token = - torch.sum(attn * log_attention, dim=-1).mean(dim=0).squeeze(-1) # TensorType[\"H\", \"2K\"]\n",
    "\n",
    "    num_heads, num_tokens = entropy_per_token.shape\n",
    "\n",
    "    entropy_per_head = entropy_per_token.mean(dim=-1) # TensorType[\"H\"]\n",
    "    entropy = entropy_per_head.mean() # TensorType[]    \n",
    "    \n",
    "    # Each token computes entropy over a variable context length, so we normalize by the maximum possible entropy\n",
    "    # for a token with a fixed context length.\n",
    "\n",
    "    max_entropy_per_token = torch.log2(torch.arange(1, num_tokens + 1).to(attn.device)) # TensorType[\"H\", \"2K\"]\n",
    "    max_entropy_per_token[0] = 1. # Special case for the first token to avoid dividing by 0\n",
    "\n",
    "    entropy_per_token_normalized = entropy_per_token / max_entropy_per_token\n",
    "    entropy_per_head_normalized = entropy_per_token_normalized.mean(dim=-1) # TensorType[\"H\"]\n",
    "    entropy_normalized = entropy_per_head_normalized.mean() # TensorType[]    \n",
    "\n",
    "    results: Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]] = {\"mean\": entropy, \"mean_normalized\": entropy_normalized}\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        head_results = {\"mean\": entropy_per_head[i], \"mean_normalized\": entropy_per_head_normalized[i]}\n",
    "\n",
    "        for j in range(num_tokens):\n",
    "            head_results[f\"token_{j}\"] = entropy_per_token[i, j]\n",
    "            head_results[f\"token_{j}_normalized\"] = entropy_per_token_normalized[i, j]\n",
    "\n",
    "        results[f\"head_{i}\"] = head_results\n",
    "\n",
    "    return map_nested(lambda x: convert_tensor(x, \"np\"), results)\n",
    "\n",
    "\n",
    "def get_attention_entropies_trace(\n",
    "    steps: List[int],\n",
    "    models: Iterable[nn.Module],\n",
    "    xs: torch.Tensor,\n",
    "    ys: torch.Tensor,\n",
    "    **paths,\n",
    "):\n",
    "    results = defaultdict(list)\n",
    "    reverse_paths = {v: k for k, v in paths.items()}\n",
    "\n",
    "    for activations in extract_activations_over_checkpoints(models, xs, ys, *paths.values(), return_type=\"pt\"):\n",
    "        for k, v in activations.items():\n",
    "            if k == \"\":\n",
    "                continue\n",
    "            path = reverse_paths[k]\n",
    "            results[path].append(compute_attention_entropies(v))\n",
    "\n",
    "    values = []\n",
    "\n",
    "    for i in range(len(steps)):\n",
    "        value = {}\n",
    "\n",
    "        for block in results.keys():\n",
    "            value[block] = results[block][i]\n",
    "        \n",
    "        value[\"step\"] = steps[i]\n",
    "        values.append(flatten_dict(value, flatten_lists=True))\n",
    "\n",
    "    return pd.DataFrame(values)\n",
    "\n",
    "\n",
    "num_blocks = run.config.task_config.num_layers\n",
    "num_heads = run.config.task_config.num_heads\n",
    "num_tokens = run.config.task_config.max_examples * 2\n",
    "\n",
    "\n",
    "attn_entropies = get_attention_entropies_trace(\n",
    "    run.checkpointer.file_ids,\n",
    "    models, \n",
    "    xs, \n",
    "    ys, \n",
    "    **{f\"block_{b}\": f\"token_sequence_transformer.blocks.{b}.attention.attention_softmax\" for b in range(num_blocks)}\n",
    ")\n",
    "\n",
    "# run_attn_entropy_slug = \"attn-S-\" + run.config.to_slug(delimiter=\"-\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_patterns(df: pd.DataFrame, num_blocks: int, num_heads: int, num_tokens: int, title=\"\", save: Optional[str] = None, normalized=False, figsize=(20, 25), logx=False, logy=False):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    num_cols = num_blocks * 2\n",
    "    num_rows = 1 + 1 + num_heads\n",
    "\n",
    "    suffix = \"\" if not normalized else \"_normalized\"\n",
    "    suffix_title = \"\" if not normalized else \" (Normalized)\"\n",
    "\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    # Create subplot for mean entropy of first two blocks\n",
    "    ax0 = plt.subplot2grid((num_rows, num_cols), (0, 0), colspan=num_cols)\n",
    "    block_cmap = sns.color_palette(\"viridis\", num_blocks)\n",
    "\n",
    "    for b in range(num_blocks):\n",
    "        ax0.plot(df.step, df[f\"block_{b}/mean{suffix}\"], label=f\"block_{b}\", color=block_cmap[b])\n",
    "\n",
    "    ax0.set_title(\"Blocks\")\n",
    "    ax0.set_xlabel(\"Step\")\n",
    "    ax0.set_ylabel(f\"Entropy{suffix_title}\")\n",
    "    ax0.legend()\n",
    "\n",
    "    plot_transitions(ax0, TRANSITIONS, limit=True)\n",
    "\n",
    "    # Create subplots for each block, showing entropy in different heads\n",
    "    ax1 = [plt.subplot2grid((num_rows, num_cols), (1, i*2), colspan=2) for i in range(num_blocks)]\n",
    "    head_cmap = sns.color_palette(\"viridis\", num_heads)\n",
    "    \n",
    "    for b in range(num_blocks):\n",
    "        ax1[b].set_title(f\"Block {b}\")\n",
    "        ax1[b].set_xlabel(\"Step\")\n",
    "        ax1[b].set_ylabel(f\"Entropy{suffix_title}\")\n",
    "        for h in range(num_heads):\n",
    "            series = df[f\"block_{b}/head_{h}/mean{suffix}\"]\n",
    "            ax1[b].plot(df.step, series, label=f\"Head {h}\", color=head_cmap[h])\n",
    "\n",
    "    ax1[0].legend()\n",
    "\n",
    "    plot_transitions(ax1, TRANSITIONS, limit=True)\n",
    "\n",
    "    # Create subplots for each head in each block, detailing entropy for each token\n",
    "    ax2 = [plt.subplot2grid((num_rows, num_cols), (i//(num_cols) + 2, i%(num_cols))) for i in range(num_heads * num_blocks * 2)]\n",
    "    ax_idx = 0\n",
    "    token_cmap = sns.color_palette(\"viridis\", num_tokens)\n",
    "\n",
    "\n",
    "    for h in range(num_heads):\n",
    "        for b in range(num_blocks):\n",
    "            for x_or_y in (1, 0):\n",
    "                ax2[ax_idx].set_title(f\"Block {b} Head {h}\")\n",
    "                ax2[ax_idx].set_xlabel(\"Step\")\n",
    "                ax2[ax_idx].set_ylabel(f\"Entropy{suffix_title}\")\n",
    "\n",
    "                for t in range(1-int(x_or_y), num_tokens, 2):\n",
    "                    series = df[f\"block_{b}/head_{h}/token_{t}{suffix}\"]\n",
    "                    ax2[ax_idx].plot(df.step, series, label=f\"Token {t}\", color=token_cmap[t])\n",
    "                    \n",
    "                ax_idx += 1\n",
    "\n",
    "    ax2[0].legend()\n",
    "    ax2[1].legend()\n",
    "\n",
    "    plot_transitions(ax2, TRANSITIONS, limit=True)\n",
    "\n",
    "    for ax in [ax0, *ax1, *ax2]:\n",
    "        if logx:\n",
    "            ax.set_xscale(\"log\")\n",
    "        if logy:\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    if save:\n",
    "        parent_dir = os.path.dirname(save)\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "        plt.savefig(save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for normalized in (True, False):\n",
    "    plot_attention_patterns(\n",
    "        attn_entropies, \n",
    "        num_blocks=num_blocks, \n",
    "        num_heads=num_heads, \n",
    "        num_tokens=num_tokens, \n",
    "        title=run.config.to_latex(), \n",
    "        save=FIGURES / (f\"{MODEL_ID}-attn-entropy-normalized-{normalized}\" + \".png\"),\n",
    "        figsize=(25, 25),\n",
    "        normalized=normalized,\n",
    "        logx=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activations\n",
    "\n",
    "from devinfra.utils.seed import set_seed\n",
    "\n",
    "DEVICE = 'mps'\n",
    "\n",
    "# Gonna override activations sorry.\n",
    "losses_over_time = []\n",
    "outputs_over_time = []\n",
    "activation_stats_over_time = []\n",
    "\n",
    "train_xs_noise, train_ys_noise = run.evaluator.pretrain_xs, run.evaluator.pretrain_ys\n",
    "\n",
    "pretrain_dist_noiseless = run.config.task_config.pretrain_dist_factory().to(\n",
    "    DEVICE\n",
    ")\n",
    "pretrain_dist_noiseless.var = 0.\n",
    "\n",
    "set_seed(run.config.task_config.true_seed)\n",
    "\n",
    "train_xs, train_ys = pretrain_dist_noiseless.get_batch(8, 8196)\n",
    "\n",
    "# assert torch.allclose(train_xs, train_xs_noise)\n",
    "\n",
    "resid_stream_layers = [\n",
    "    \"token_sequence_transformer.token_embedding\",\n",
    "    \"token_sequence_transformer.blocks.0.resid_after_attn\",\n",
    "    \"token_sequence_transformer.blocks.0\",\n",
    "    \"token_sequence_transformer.blocks.1.resid_after_attn\",\n",
    "    \"token_sequence_transformer.blocks.1\"\n",
    "]\n",
    "\n",
    "act_stats_over_time = []\n",
    "\n",
    "for step, model in tqdm.tqdm(zip(steps, models)):\n",
    "    hooked_model = hook(model)\n",
    "    output, act = hooked_model.run_with_cache(train_xs, train_ys)\n",
    "    outputs_over_time.append(output)\n",
    "    losses_over_time.append(nn.MSELoss()(output, train_ys).item())\n",
    "    \n",
    "    for layer in resid_stream_layers:\n",
    "        #print(act[layer].shape)\n",
    "        act_stats_over_time.append({\n",
    "            'mean': act[layer].mean().item(),  # mean over batch, over tokens, over activations\n",
    "            'abs_mean': act[layer].mean(dim=-1).abs().mean().item(),  # mean over batch and tokens of abs mean over activations\n",
    "            'var': act[layer].var().item(),  # var over batch, over tokens, over activations\n",
    "            'batch_var_of_mean': act[layer].var(dim=-1).mean().item(),  # mean over batch and tokens of var over activations\n",
    "            'batch_var_of_var': act[layer].var(dim=-1).var().item(),  # var over batch and tokens of var over activations\n",
    "            'max': act[layer].max().item(),\n",
    "            'min': act[layer].min().item(),\n",
    "            \"step\": step, \n",
    "            \"layer\": layer,\n",
    "            \"layer_idx\": resid_stream_layers.index(layer)\n",
    "        })\n",
    "\n",
    "act_stats_over_time = pd.DataFrame(act_stats_over_time)\n",
    "from devinfra.utils.iterables import flatten_dict\n",
    "\n",
    "act_stats_over_time\n",
    "# act_stats_over_time = act_stats_over_time[act_stats_over_time[\"step\"] > 0]\n",
    "\n",
    "\n",
    "more_metrics_to_plot = [\n",
    "    (\"$\\overline{|\\mathrm{mean}[z^{(l)}_t]|}$\", \"abs_mean\", {}),\n",
    "#    (\"Mean over batch, token index, and activation index of residual stream activations\", \"std\", {}),\n",
    "    (\"$\\overline{\\mathrm{var}[z^{(l)}_t]}$\", \"batch_var_of_mean\", {}),\n",
    "#    (\"Std over batch and token index of std within residual stream activations\", \"batch_std_of_std\", {}),\n",
    "]\n",
    "\n",
    "layers = act_stats_over_time[\"layer\"].unique()\n",
    "slopes = np.zeros((len(layers), len(run.checkpointer.file_ids[1:])))\n",
    "\n",
    "slopes_list = []\n",
    "\n",
    "for (_, key, _) in more_metrics_to_plot:\n",
    "    for j, layer in enumerate(layers):\n",
    "        values = act_stats_over_time.loc[act_stats_over_time[\"layer\"] == layer][key].values\n",
    "        slopes[j, :] = dlog_dlogt(run.checkpointer.file_ids[1:], values)\n",
    "\n",
    "    slopes_list.extend([{\"layer\": layer, \"step\": step, key: slopes[j, step_idx]} for j, layer in enumerate(layers) for step_idx, step in enumerate(run.checkpointer.file_ids[1:]) for slope in slopes])\n",
    "\n",
    "slopes_df = pd.DataFrame(slopes_list)\n",
    "fig, axes = plt.subplots(1, len(more_metrics_to_plot), figsize=(20, 6))\n",
    "\n",
    "# act_stats_over_time = act_stats_over_time[act_stats_over_time[\"step\"] > 0]\n",
    "\n",
    "for i, (metric_name, key, kwargs) in enumerate(more_metrics_to_plot):\n",
    "    sns.lineplot(ax=axes[i], data=act_stats_over_time, x=\"step\", y=key, hue='layer', palette='viridis')\n",
    "\n",
    "    axes[i].set_title(metric_name + \" over Time\")\n",
    "    axes[i].set_xlabel('Time Steps')\n",
    "    axes[i].set_ylabel(metric_name)\n",
    "    axes[i].set_yscale('log')\n",
    "\n",
    "    # sns.lineplot(ax=axes[1, i], data=slopes_df, x=\"step\", y=key, hue='layer', palette='viridis')\n",
    "    # axes[1, i].set_title(metric_name + \" Slope over Time\")\n",
    "    # axes[1, i].set_xlabel('Time Steps')\n",
    "    # axes[1, i].set_ylabel(metric_name + \" Slope\")\n",
    "\n",
    "plot_transitions(axes, TRANSITIONS, limit=True)\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(100, 500_000)\n",
    "\n",
    "axes[0, 0].legend(title=\"Layer\")\n",
    "axes[0, 1].legend().remove()\n",
    "fig.tight_layout()\n",
    "fig.set_facecolor(\"white\")\n",
    "\n",
    "# axes[1, 0].set_ylim(-10, 10)\n",
    "# axes[1, 1].set_ylim(-5, 5)\n",
    "fig.set_facecolor('white')\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanistic Interpretability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
