{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures for \"The Developmental Landscape of In-Context Learning\"\n",
    "\n",
    "TODO: add a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/Projects/icl/.venv/lib/python3.9/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_seed\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/Users/Jesse/Projects/icl/figures')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "from copy import deepcopy \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Optional\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from icl.analysis.utils import get_unique_run, get_unique_config\n",
    "from icl.constants import FIGURES, SWEEPS, DATA\n",
    "from icl.figures.notation import str_d_dlogt, str_d_dt, str_dlog_dlogt\n",
    "from icl.figures.colors import plot_transitions, gen_transition_colors, get_transition_type, PRIMARY, SECONDARY, TERTIARY, BRED, BBLUE, BRED, BGREEN\n",
    "from icl.constants import DEVICE\n",
    "from icl.figures.plotting import WIDTH, HEIGHT, FULL_WIDTH, FULL_HEIGHT\n",
    "from icl.monitoring import stdlogger\n",
    "\n",
    "MODELS_ID = \"L2H4Minf\"\n",
    "LLC_SWEEP_ID = \"hmy71gjb\"\n",
    "\n",
    "stdlogger.setLevel(logging.DEBUG)\n",
    "\n",
    "FIGURES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/Projects/devinfra/devinfra/utils/iterables.py:29: UserWarning: Number of steps in int_logspace is not 100, got 91.\n",
      "  warnings.warn(\n",
      "/Users/Jesse/Projects/icl/src/icl/regression/baselines.py:165: UserWarning: The operator 'aten::_linalg_solve_ex.result' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  ws_hat = torch.linalg.solve(LHS, RHS)   # BKDD^-1 @ BKD1 -> B K D 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shorthands\n",
    "BATCH_SIZE = 8192\n",
    "K = 8\n",
    "D = 4\n",
    "\n",
    "runs = [get_unique_run(\n",
    "    str(SWEEPS / \"training-runs/L2H4Minf.yaml\"), \n",
    "    task_config={\"model_seed\": model_seed, \"layer_norm\": True},\n",
    ") for model_seed in range(5)]\n",
    "\n",
    "steps = set(runs[0].checkpointer.file_ids)\n",
    "\n",
    "# for run in runs[1:]:\n",
    "#     if steps != set(run.checkpointer.file_ids):\n",
    "#         stdlogger.warning(\"Not all runs have the same checkpoints. Using intersection.\")\n",
    "\n",
    "#     steps.intersection_update(run.checkpointer.file_ids)\n",
    "\n",
    "steps = list(steps)\n",
    "num_steps = len(steps)\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint 99/190:  40%|████      | 2/5 [03:26<04:01, 80.63s/it] "
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "all_optimizer_state_dicts = []\n",
    "\n",
    "if os.path.exists(DATA / MODELS_ID / 'models.pt'):\n",
    "    stdlogger.info(f\"Loading models from {DATA / MODELS_ID}\")\n",
    "    all_models = torch.load(DATA / MODELS_ID / 'models.pt')\n",
    "    all_optimizer_state_dicts = torch.load(DATA / MODELS_ID / 'optimizer_state_dicts.pt')\n",
    "else:\n",
    "    stdlogger.info(f\"Retrieving models from bucket\")\n",
    "\n",
    "    pbar = tqdm.tqdm(runs)\n",
    "\n",
    "    for run in pbar:\n",
    "        models = []\n",
    "        optimizer_state_dicts = []\n",
    "\n",
    "        for i, step in enumerate(steps):\n",
    "            checkpoint = run.checkpointer.load_file(step)\n",
    "            m = deepcopy(run.model)\n",
    "            m.load_state_dict(checkpoint[\"model\"])\n",
    "            models.append(m)\n",
    "            optimizer_state_dicts.append(checkpoint[\"optimizer\"])\n",
    "\n",
    "            pbar.set_description(f\"Checkpoint {i}/{num_steps}\")\n",
    "            \n",
    "        all_models.append(models)\n",
    "        all_optimizer_state_dicts.append(optimizer_state_dicts)\n",
    "\n",
    "    stdlogger.info(f\"Saving models to {DATA / MODELS_ID}\")\n",
    "    \n",
    "    with open(DATA / MODELS_ID / 'models.pt', 'wb') as f:\n",
    "        torch.save(all_models, f)\n",
    "\n",
    "    with open(DATA / MODELS_ID / 'optimizer_state_dicts.pt', 'wb') as f:\n",
    "        torch.save(all_optimizer_state_dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+------------+\n",
      "|                            Modules                             | Parameters |\n",
      "+----------------------------------------------------------------+------------+\n",
      "|       token_sequence_transformer.token_embedding.weight        |    320     |\n",
      "|       token_sequence_transformer.postn_embedding.weight        |    1024    |\n",
      "| token_sequence_transformer.blocks.0.attention.attention.weight |   12288    |\n",
      "|  token_sequence_transformer.blocks.0.attention.output.weight   |    4096    |\n",
      "|      token_sequence_transformer.blocks.0.compute.0.weight      |    4096    |\n",
      "|       token_sequence_transformer.blocks.0.compute.0.bias       |     64     |\n",
      "|      token_sequence_transformer.blocks.0.compute.2.weight      |    4096    |\n",
      "|       token_sequence_transformer.blocks.0.compute.2.bias       |     64     |\n",
      "|    token_sequence_transformer.blocks.0.layer_norms.0.weight    |     64     |\n",
      "|     token_sequence_transformer.blocks.0.layer_norms.0.bias     |     64     |\n",
      "|    token_sequence_transformer.blocks.0.layer_norms.1.weight    |     64     |\n",
      "|     token_sequence_transformer.blocks.0.layer_norms.1.bias     |     64     |\n",
      "| token_sequence_transformer.blocks.1.attention.attention.weight |   12288    |\n",
      "|  token_sequence_transformer.blocks.1.attention.output.weight   |    4096    |\n",
      "|      token_sequence_transformer.blocks.1.compute.0.weight      |    4096    |\n",
      "|       token_sequence_transformer.blocks.1.compute.0.bias       |     64     |\n",
      "|      token_sequence_transformer.blocks.1.compute.2.weight      |    4096    |\n",
      "|       token_sequence_transformer.blocks.1.compute.2.bias       |     64     |\n",
      "|    token_sequence_transformer.blocks.1.layer_norms.0.weight    |     64     |\n",
      "|     token_sequence_transformer.blocks.1.layer_norms.0.bias     |     64     |\n",
      "|    token_sequence_transformer.blocks.1.layer_norms.1.weight    |     64     |\n",
      "|     token_sequence_transformer.blocks.1.layer_norms.1.bias     |     64     |\n",
      "|        token_sequence_transformer.unembedding.0.weight         |     64     |\n",
      "|         token_sequence_transformer.unembedding.0.bias          |     64     |\n",
      "|        token_sequence_transformer.unembedding.1.weight         |    320     |\n",
      "|         token_sequence_transformer.unembedding.1.bias          |     5      |\n",
      "+----------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 51717 (0.0M)\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params} ({total_params//1e6}M)\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(all_models[0][-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral (loss, delta ridge, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.figures.derivatives import d_dt, d_dlogt, dlog_dlogt\n",
    "\n",
    "def add_slopes(df, column, model_seed):\n",
    "    seed_subset = df[df.model_seed == model_seed].sort_values(\"step\")\n",
    "    _steps = seed_subset['step'].values\n",
    "\n",
    "    d_dts = d_dt(_steps, seed_subset[column].values)\n",
    "    d_dlogts = d_dlogt(_steps, seed_subset[column].values)\n",
    "    dlog_dlogts = dlog_dlogt(_steps, seed_subset[column].values)\n",
    "    \n",
    "    for step, _d_dt, _d_dlogt, _dlog_dlogt, in zip(seed_subset['step'], d_dts, d_dlogts, dlog_dlogts):\n",
    "        df.loc[((df.step == step) & (df.model_seed==model_seed)), f\"{column}/d_dt\"] = _d_dt\n",
    "        df.loc[((df.step == step) & (df.model_seed==model_seed)), f\"{column}/d_dlogt\"] = _d_dlogt\n",
    "        df.loc[((df.step == step) & (df.model_seed==model_seed)), f\"{column}/dlog_dlogt\"] = _dlog_dlogt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 189/190 [08:47<00:02,  2.79s/it]\n",
      " 99%|█████████▉| 189/190 [08:46<00:02,  2.79s/it]\n",
      " 99%|█████████▉| 189/190 [08:52<00:02,  2.82s/it]\n",
      " 99%|█████████▉| 189/190 [08:57<00:02,  2.84s/it]\n",
      " 99%|█████████▉| 189/190 [08:49<00:02,  2.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pretrain/mse_subsequence</th>\n",
       "      <th>pretrain/mse_subseq/token/0</th>\n",
       "      <th>pretrain/mse_subseq/token/1</th>\n",
       "      <th>pretrain/mse_subseq/token/2</th>\n",
       "      <th>pretrain/mse_subseq/token/3</th>\n",
       "      <th>pretrain/mse_subseq/token/4</th>\n",
       "      <th>pretrain/mse_subseq/token/5</th>\n",
       "      <th>pretrain/mse_subseq/token/6</th>\n",
       "      <th>pretrain/mse_subseq/token/7</th>\n",
       "      <th>pretrain/mse_subseq</th>\n",
       "      <th>...</th>\n",
       "      <th>true/delta_ridge</th>\n",
       "      <th>step</th>\n",
       "      <th>model_seed</th>\n",
       "      <th>weight/norm</th>\n",
       "      <th>pretrain/mse/d_dt</th>\n",
       "      <th>pretrain/mse/d_dlogt</th>\n",
       "      <th>pretrain/mse/dlog_dlogt</th>\n",
       "      <th>weight/norm/d_dt</th>\n",
       "      <th>weight/norm/d_dlogt</th>\n",
       "      <th>weight/norm/dlog_dlogt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.264059</td>\n",
       "      <td>4.223984</td>\n",
       "      <td>4.360218</td>\n",
       "      <td>4.157660</td>\n",
       "      <td>4.104471</td>\n",
       "      <td>4.213433</td>\n",
       "      <td>4.166513</td>\n",
       "      <td>4.175281</td>\n",
       "      <td>4.247562</td>\n",
       "      <td>4.206140</td>\n",
       "      <td>...</td>\n",
       "      <td>2.536717</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.909348</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.222313</td>\n",
       "      <td>4.223981</td>\n",
       "      <td>4.360214</td>\n",
       "      <td>4.157659</td>\n",
       "      <td>4.104469</td>\n",
       "      <td>4.213432</td>\n",
       "      <td>4.166512</td>\n",
       "      <td>4.175280</td>\n",
       "      <td>4.247561</td>\n",
       "      <td>4.206139</td>\n",
       "      <td>...</td>\n",
       "      <td>2.536715</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.909348</td>\n",
       "      <td>-2.622604e-06</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.219220</td>\n",
       "      <td>4.223976</td>\n",
       "      <td>4.360209</td>\n",
       "      <td>4.157657</td>\n",
       "      <td>4.104465</td>\n",
       "      <td>4.213430</td>\n",
       "      <td>4.166508</td>\n",
       "      <td>4.175277</td>\n",
       "      <td>4.247559</td>\n",
       "      <td>4.206135</td>\n",
       "      <td>...</td>\n",
       "      <td>2.536712</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24.909348</td>\n",
       "      <td>-3.814697e-06</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.283030</td>\n",
       "      <td>4.223969</td>\n",
       "      <td>4.360202</td>\n",
       "      <td>4.157654</td>\n",
       "      <td>4.104461</td>\n",
       "      <td>4.213428</td>\n",
       "      <td>4.166503</td>\n",
       "      <td>4.175275</td>\n",
       "      <td>4.247555</td>\n",
       "      <td>4.206131</td>\n",
       "      <td>...</td>\n",
       "      <td>2.536708</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24.909348</td>\n",
       "      <td>-4.768372e-06</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.233604</td>\n",
       "      <td>4.223961</td>\n",
       "      <td>4.360194</td>\n",
       "      <td>4.157651</td>\n",
       "      <td>4.104454</td>\n",
       "      <td>4.213425</td>\n",
       "      <td>4.166497</td>\n",
       "      <td>4.175271</td>\n",
       "      <td>4.247551</td>\n",
       "      <td>4.206125</td>\n",
       "      <td>...</td>\n",
       "      <td>2.536702</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24.909348</td>\n",
       "      <td>-6.198883e-06</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>2.673234</td>\n",
       "      <td>4.083452</td>\n",
       "      <td>3.260144</td>\n",
       "      <td>2.266535</td>\n",
       "      <td>1.499078</td>\n",
       "      <td>1.086742</td>\n",
       "      <td>0.774978</td>\n",
       "      <td>0.600018</td>\n",
       "      <td>0.521194</td>\n",
       "      <td>1.761518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102291</td>\n",
       "      <td>373737</td>\n",
       "      <td>4</td>\n",
       "      <td>222.775085</td>\n",
       "      <td>-2.638620e-07</td>\n",
       "      <td>-0.098285</td>\n",
       "      <td>-0.055775</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>72.314930</td>\n",
       "      <td>0.324610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>2.781868</td>\n",
       "      <td>4.085014</td>\n",
       "      <td>3.266781</td>\n",
       "      <td>2.301377</td>\n",
       "      <td>1.627052</td>\n",
       "      <td>1.218396</td>\n",
       "      <td>0.926676</td>\n",
       "      <td>0.754805</td>\n",
       "      <td>0.637363</td>\n",
       "      <td>1.852183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199315</td>\n",
       "      <td>75757</td>\n",
       "      <td>4</td>\n",
       "      <td>40.282597</td>\n",
       "      <td>-1.311492e-06</td>\n",
       "      <td>-0.096718</td>\n",
       "      <td>-0.052085</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>26.459183</td>\n",
       "      <td>0.659412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>2.779250</td>\n",
       "      <td>4.084165</td>\n",
       "      <td>3.258761</td>\n",
       "      <td>2.265608</td>\n",
       "      <td>1.477728</td>\n",
       "      <td>1.050309</td>\n",
       "      <td>0.745599</td>\n",
       "      <td>0.574186</td>\n",
       "      <td>0.493149</td>\n",
       "      <td>1.743688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084611</td>\n",
       "      <td>484848</td>\n",
       "      <td>4</td>\n",
       "      <td>232.149216</td>\n",
       "      <td>-1.004480e-07</td>\n",
       "      <td>-0.048488</td>\n",
       "      <td>-0.027801</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.391759</td>\n",
       "      <td>0.018918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>2.744716</td>\n",
       "      <td>4.083723</td>\n",
       "      <td>3.258915</td>\n",
       "      <td>2.299166</td>\n",
       "      <td>1.548486</td>\n",
       "      <td>1.126998</td>\n",
       "      <td>0.850175</td>\n",
       "      <td>0.667554</td>\n",
       "      <td>0.594283</td>\n",
       "      <td>1.803662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146541</td>\n",
       "      <td>186868</td>\n",
       "      <td>4</td>\n",
       "      <td>115.900276</td>\n",
       "      <td>8.405793e-07</td>\n",
       "      <td>0.153130</td>\n",
       "      <td>0.085275</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>144.731806</td>\n",
       "      <td>1.247879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>4.182449</td>\n",
       "      <td>4.147954</td>\n",
       "      <td>4.261123</td>\n",
       "      <td>4.136225</td>\n",
       "      <td>4.100826</td>\n",
       "      <td>4.187189</td>\n",
       "      <td>4.135476</td>\n",
       "      <td>4.184031</td>\n",
       "      <td>4.240444</td>\n",
       "      <td>4.174159</td>\n",
       "      <td>...</td>\n",
       "      <td>2.506923</td>\n",
       "      <td>507</td>\n",
       "      <td>4</td>\n",
       "      <td>24.948587</td>\n",
       "      <td>-3.156127e-04</td>\n",
       "      <td>-0.159874</td>\n",
       "      <td>-0.038297</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.014346</td>\n",
       "      <td>0.000575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>950 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pretrain/mse_subsequence  pretrain/mse_subseq/token/0  \\\n",
       "0                    4.264059                     4.223984   \n",
       "1                    4.222313                     4.223981   \n",
       "2                    4.219220                     4.223976   \n",
       "3                    4.283030                     4.223969   \n",
       "4                    4.233604                     4.223961   \n",
       "..                        ...                          ...   \n",
       "945                  2.673234                     4.083452   \n",
       "946                  2.781868                     4.085014   \n",
       "947                  2.779250                     4.084165   \n",
       "948                  2.744716                     4.083723   \n",
       "949                  4.182449                     4.147954   \n",
       "\n",
       "     pretrain/mse_subseq/token/1  pretrain/mse_subseq/token/2  \\\n",
       "0                       4.360218                     4.157660   \n",
       "1                       4.360214                     4.157659   \n",
       "2                       4.360209                     4.157657   \n",
       "3                       4.360202                     4.157654   \n",
       "4                       4.360194                     4.157651   \n",
       "..                           ...                          ...   \n",
       "945                     3.260144                     2.266535   \n",
       "946                     3.266781                     2.301377   \n",
       "947                     3.258761                     2.265608   \n",
       "948                     3.258915                     2.299166   \n",
       "949                     4.261123                     4.136225   \n",
       "\n",
       "     pretrain/mse_subseq/token/3  pretrain/mse_subseq/token/4  \\\n",
       "0                       4.104471                     4.213433   \n",
       "1                       4.104469                     4.213432   \n",
       "2                       4.104465                     4.213430   \n",
       "3                       4.104461                     4.213428   \n",
       "4                       4.104454                     4.213425   \n",
       "..                           ...                          ...   \n",
       "945                     1.499078                     1.086742   \n",
       "946                     1.627052                     1.218396   \n",
       "947                     1.477728                     1.050309   \n",
       "948                     1.548486                     1.126998   \n",
       "949                     4.100826                     4.187189   \n",
       "\n",
       "     pretrain/mse_subseq/token/5  pretrain/mse_subseq/token/6  \\\n",
       "0                       4.166513                     4.175281   \n",
       "1                       4.166512                     4.175280   \n",
       "2                       4.166508                     4.175277   \n",
       "3                       4.166503                     4.175275   \n",
       "4                       4.166497                     4.175271   \n",
       "..                           ...                          ...   \n",
       "945                     0.774978                     0.600018   \n",
       "946                     0.926676                     0.754805   \n",
       "947                     0.745599                     0.574186   \n",
       "948                     0.850175                     0.667554   \n",
       "949                     4.135476                     4.184031   \n",
       "\n",
       "     pretrain/mse_subseq/token/7  pretrain/mse_subseq  ...  true/delta_ridge  \\\n",
       "0                       4.247562             4.206140  ...          2.536717   \n",
       "1                       4.247561             4.206139  ...          2.536715   \n",
       "2                       4.247559             4.206135  ...          2.536712   \n",
       "3                       4.247555             4.206131  ...          2.536708   \n",
       "4                       4.247551             4.206125  ...          2.536702   \n",
       "..                           ...                  ...  ...               ...   \n",
       "945                     0.521194             1.761518  ...          0.102291   \n",
       "946                     0.637363             1.852183  ...          0.199315   \n",
       "947                     0.493149             1.743688  ...          0.084611   \n",
       "948                     0.594283             1.803662  ...          0.146541   \n",
       "949                     4.240444             4.174159  ...          2.506923   \n",
       "\n",
       "       step  model_seed  weight/norm  pretrain/mse/d_dt  pretrain/mse/d_dlogt  \\\n",
       "0         0           0    24.909348       0.000000e+00              0.000000   \n",
       "1         1           0    24.909348      -2.622604e-06             -0.000006   \n",
       "2         2           0    24.909348      -3.814697e-06             -0.000011   \n",
       "3         3           0    24.909348      -4.768372e-06             -0.000019   \n",
       "4         4           0    24.909348      -6.198883e-06             -0.000031   \n",
       "..      ...         ...          ...                ...                   ...   \n",
       "945  373737           4   222.775085      -2.638620e-07             -0.098285   \n",
       "946   75757           4    40.282597      -1.311492e-06             -0.096718   \n",
       "947  484848           4   232.149216      -1.004480e-07             -0.048488   \n",
       "948  186868           4   115.900276       8.405793e-07              0.153130   \n",
       "949     507           4    24.948587      -3.156127e-04             -0.159874   \n",
       "\n",
       "     pretrain/mse/dlog_dlogt  weight/norm/d_dt  weight/norm/d_dlogt  \\\n",
       "0                   0.000000          0.000000             0.000000   \n",
       "1                  -0.000001          0.000000             0.000000   \n",
       "2                  -0.000003          0.000000             0.000000   \n",
       "3                  -0.000005          0.000000             0.000000   \n",
       "4                  -0.000007          0.000000             0.000000   \n",
       "..                       ...               ...                  ...   \n",
       "945                -0.055775          0.000193            72.314930   \n",
       "946                -0.052085          0.000352            26.459183   \n",
       "947                -0.027801          0.000009             4.391759   \n",
       "948                 0.085275          0.000774           144.731806   \n",
       "949                -0.038297          0.000028             0.014346   \n",
       "\n",
       "     weight/norm/dlog_dlogt  \n",
       "0                  0.000000  \n",
       "1                  0.000000  \n",
       "2                  0.000000  \n",
       "3                  0.000000  \n",
       "4                  0.000000  \n",
       "..                      ...  \n",
       "945                0.324610  \n",
       "946                0.659412  \n",
       "947                0.018918  \n",
       "948                1.247879  \n",
       "949                0.000575  \n",
       "\n",
       "[950 rows x 74 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from icl.analysis.evals import ICLEvaluator\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "run = runs[0]\n",
    "\n",
    "evaluator = ICLEvaluator(\n",
    "    pretrain_dist=run.pretrain_dist,\n",
    "    true_dist=run.true_dist,\n",
    "    max_examples=run.config.task_config.max_examples,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    seed=run.config.task_config.true_seed,  # type: ignore \n",
    ")\n",
    "\n",
    "evals_over_time_df = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(DATA / MODELS_ID / \"evals_over_time.pt\"):\n",
    "    stdlogger.info(\"Loading evals from disk\")\n",
    "    with open(DATA / MODELS_ID / \"evals_over_time.pt\", 'rb') as f:\n",
    "        evals_over_time = torch.load(f)\n",
    "else:\n",
    "    stdlogger.info(\"Running evals\")\n",
    "    evals_over_time = [{**evaluator(model), \"step\": step, \"model_seed\": i} for i, _models in enumerate(all_models) for step, model in zip(steps, tqdm.tqdm(_models))]\n",
    "    evals_over_time_df = pd.DataFrame(evals_over_time)\n",
    "\n",
    "    stdlogger.info(\"Calculating weight norms\")\n",
    "    for i, _models in enumerate(all_models):\n",
    "        for step, model in zip(steps, _models):\n",
    "            evals_over_time_df.loc[((evals_over_time_df.step == step) & (evals_over_time_df.model_seed==i)), \"weight/norm\"] = (sum(torch.norm(p) ** 2 for p in model.parameters()) ** 0.5).item()\n",
    "\n",
    "    stdlogger.info(\"Calculating derivatives\")\n",
    "    for i in evals_over_time_df.model_seed.unique():\n",
    "        for column in ['pretrain/mse', \"weight/norm\"]:\n",
    "            add_slopes(evals_over_time_df, column, i)\n",
    "\n",
    "    stdlogger.info(\"Saving evals to disk\")\n",
    "    with open(DATA / MODELS_ID / \"evals_over_time.pt\", 'wb') as f:\n",
    "        torch.save(evals_over_time, f)\n",
    "\n",
    "evals_over_time_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLC estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.40it/s]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
      "<ipython-input-26-22406cc1972f>:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch-loss/chain-2/std/std</th>\n",
       "      <th>batch-loss/chain-7/std/4</th>\n",
       "      <th>batch-loss/chain-6/std/mean</th>\n",
       "      <th>loss/mean/2</th>\n",
       "      <th>loss/mean/7</th>\n",
       "      <th>batch-loss/chain-5/std/5</th>\n",
       "      <th>wbic/mean/6</th>\n",
       "      <th>batch-loss/chain-0/mean/5</th>\n",
       "      <th>batch-loss/chain-1/std/0</th>\n",
       "      <th>batch-loss/chain-2/mean/6</th>\n",
       "      <th>...</th>\n",
       "      <th>batch-loss/chain-8/std/0</th>\n",
       "      <th>batch-loss/chain-1/mean/2</th>\n",
       "      <th>batch-loss/chain-7/std/5</th>\n",
       "      <th>batch-loss/chain-1/mean/3</th>\n",
       "      <th>llc/std/2</th>\n",
       "      <th>batch-loss/chain-7/mean/3</th>\n",
       "      <th>wbic/std/std</th>\n",
       "      <th>batch-loss/chain-0/std/mean</th>\n",
       "      <th>model_seed</th>\n",
       "      <th>step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.264470</td>\n",
       "      <td>0.265750</td>\n",
       "      <td>4.439744</td>\n",
       "      <td>4.474955</td>\n",
       "      <td>0.285651</td>\n",
       "      <td>4688701.50</td>\n",
       "      <td>4.453844</td>\n",
       "      <td>0.273078</td>\n",
       "      <td>4.487923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.266796</td>\n",
       "      <td>4.433786</td>\n",
       "      <td>0.256581</td>\n",
       "      <td>4.451212</td>\n",
       "      <td>17.755661</td>\n",
       "      <td>4.432512</td>\n",
       "      <td>1703.146477</td>\n",
       "      <td>0.268458</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003820</td>\n",
       "      <td>0.265399</td>\n",
       "      <td>0.263671</td>\n",
       "      <td>4.425170</td>\n",
       "      <td>4.477533</td>\n",
       "      <td>0.301905</td>\n",
       "      <td>4667845.00</td>\n",
       "      <td>4.461284</td>\n",
       "      <td>0.257812</td>\n",
       "      <td>4.469196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274583</td>\n",
       "      <td>4.419053</td>\n",
       "      <td>0.268279</td>\n",
       "      <td>4.446731</td>\n",
       "      <td>17.693489</td>\n",
       "      <td>4.426115</td>\n",
       "      <td>2348.498732</td>\n",
       "      <td>0.270319</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003786</td>\n",
       "      <td>0.265424</td>\n",
       "      <td>0.263710</td>\n",
       "      <td>4.425153</td>\n",
       "      <td>4.477554</td>\n",
       "      <td>0.301775</td>\n",
       "      <td>4667819.50</td>\n",
       "      <td>4.461306</td>\n",
       "      <td>0.257505</td>\n",
       "      <td>4.469173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274791</td>\n",
       "      <td>4.419082</td>\n",
       "      <td>0.267966</td>\n",
       "      <td>4.446737</td>\n",
       "      <td>17.725080</td>\n",
       "      <td>4.426130</td>\n",
       "      <td>2216.673692</td>\n",
       "      <td>0.270326</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.265280</td>\n",
       "      <td>0.263705</td>\n",
       "      <td>4.425192</td>\n",
       "      <td>4.477541</td>\n",
       "      <td>0.301968</td>\n",
       "      <td>4667816.50</td>\n",
       "      <td>4.461197</td>\n",
       "      <td>0.257453</td>\n",
       "      <td>4.469088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274774</td>\n",
       "      <td>4.419080</td>\n",
       "      <td>0.268162</td>\n",
       "      <td>4.446727</td>\n",
       "      <td>17.679588</td>\n",
       "      <td>4.426131</td>\n",
       "      <td>2413.262776</td>\n",
       "      <td>0.270338</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.265399</td>\n",
       "      <td>0.263675</td>\n",
       "      <td>4.425185</td>\n",
       "      <td>4.477520</td>\n",
       "      <td>0.301639</td>\n",
       "      <td>4667831.00</td>\n",
       "      <td>4.461257</td>\n",
       "      <td>0.257757</td>\n",
       "      <td>4.469166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274576</td>\n",
       "      <td>4.419018</td>\n",
       "      <td>0.268173</td>\n",
       "      <td>4.446680</td>\n",
       "      <td>17.683422</td>\n",
       "      <td>4.426101</td>\n",
       "      <td>2358.911137</td>\n",
       "      <td>0.270344</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.073918</td>\n",
       "      <td>0.340378</td>\n",
       "      <td>0.313977</td>\n",
       "      <td>3.699310</td>\n",
       "      <td>3.328932</td>\n",
       "      <td>0.503810</td>\n",
       "      <td>3466857.00</td>\n",
       "      <td>3.775999</td>\n",
       "      <td>0.238505</td>\n",
       "      <td>3.090042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267582</td>\n",
       "      <td>3.617803</td>\n",
       "      <td>0.345056</td>\n",
       "      <td>3.469490</td>\n",
       "      <td>24.637434</td>\n",
       "      <td>3.420046</td>\n",
       "      <td>126904.854249</td>\n",
       "      <td>0.486331</td>\n",
       "      <td>0</td>\n",
       "      <td>479797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.073030</td>\n",
       "      <td>0.340323</td>\n",
       "      <td>0.314419</td>\n",
       "      <td>3.698307</td>\n",
       "      <td>3.326499</td>\n",
       "      <td>0.492948</td>\n",
       "      <td>3463597.25</td>\n",
       "      <td>3.777043</td>\n",
       "      <td>0.238893</td>\n",
       "      <td>3.087273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267225</td>\n",
       "      <td>3.611918</td>\n",
       "      <td>0.344430</td>\n",
       "      <td>3.462779</td>\n",
       "      <td>24.557392</td>\n",
       "      <td>3.418888</td>\n",
       "      <td>125944.830586</td>\n",
       "      <td>0.485179</td>\n",
       "      <td>0</td>\n",
       "      <td>484848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.073581</td>\n",
       "      <td>0.338793</td>\n",
       "      <td>0.315070</td>\n",
       "      <td>3.696387</td>\n",
       "      <td>3.323102</td>\n",
       "      <td>0.482055</td>\n",
       "      <td>3459223.00</td>\n",
       "      <td>3.774743</td>\n",
       "      <td>0.238713</td>\n",
       "      <td>3.089475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267813</td>\n",
       "      <td>3.607190</td>\n",
       "      <td>0.343320</td>\n",
       "      <td>3.456326</td>\n",
       "      <td>24.520861</td>\n",
       "      <td>3.418793</td>\n",
       "      <td>125107.340861</td>\n",
       "      <td>0.485441</td>\n",
       "      <td>0</td>\n",
       "      <td>489898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.074062</td>\n",
       "      <td>0.337162</td>\n",
       "      <td>0.315380</td>\n",
       "      <td>3.696808</td>\n",
       "      <td>3.322949</td>\n",
       "      <td>0.488047</td>\n",
       "      <td>3460043.50</td>\n",
       "      <td>3.777103</td>\n",
       "      <td>0.238817</td>\n",
       "      <td>3.091058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267835</td>\n",
       "      <td>3.606944</td>\n",
       "      <td>0.342204</td>\n",
       "      <td>3.456615</td>\n",
       "      <td>24.499849</td>\n",
       "      <td>3.415974</td>\n",
       "      <td>125458.254138</td>\n",
       "      <td>0.485523</td>\n",
       "      <td>0</td>\n",
       "      <td>494949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.074319</td>\n",
       "      <td>0.338225</td>\n",
       "      <td>0.315355</td>\n",
       "      <td>3.698817</td>\n",
       "      <td>3.327330</td>\n",
       "      <td>0.496265</td>\n",
       "      <td>3463926.25</td>\n",
       "      <td>3.778592</td>\n",
       "      <td>0.238841</td>\n",
       "      <td>3.092528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268080</td>\n",
       "      <td>3.609663</td>\n",
       "      <td>0.342910</td>\n",
       "      <td>3.459869</td>\n",
       "      <td>24.534773</td>\n",
       "      <td>3.418252</td>\n",
       "      <td>125992.322016</td>\n",
       "      <td>0.484969</td>\n",
       "      <td>0</td>\n",
       "      <td>499999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>950 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     batch-loss/chain-2/std/std  batch-loss/chain-7/std/4  \\\n",
       "0                      0.002800                  0.264470   \n",
       "1                      0.003820                  0.265399   \n",
       "2                      0.003786                  0.265424   \n",
       "3                      0.003757                  0.265280   \n",
       "4                      0.003815                  0.265399   \n",
       "..                          ...                       ...   \n",
       "185                    0.073918                  0.340378   \n",
       "186                    0.073030                  0.340323   \n",
       "187                    0.073581                  0.338793   \n",
       "188                    0.074062                  0.337162   \n",
       "189                    0.074319                  0.338225   \n",
       "\n",
       "     batch-loss/chain-6/std/mean  loss/mean/2  loss/mean/7  \\\n",
       "0                       0.265750     4.439744     4.474955   \n",
       "1                       0.263671     4.425170     4.477533   \n",
       "2                       0.263710     4.425153     4.477554   \n",
       "3                       0.263705     4.425192     4.477541   \n",
       "4                       0.263675     4.425185     4.477520   \n",
       "..                           ...          ...          ...   \n",
       "185                     0.313977     3.699310     3.328932   \n",
       "186                     0.314419     3.698307     3.326499   \n",
       "187                     0.315070     3.696387     3.323102   \n",
       "188                     0.315380     3.696808     3.322949   \n",
       "189                     0.315355     3.698817     3.327330   \n",
       "\n",
       "     batch-loss/chain-5/std/5  wbic/mean/6  batch-loss/chain-0/mean/5  \\\n",
       "0                    0.285651   4688701.50                   4.453844   \n",
       "1                    0.301905   4667845.00                   4.461284   \n",
       "2                    0.301775   4667819.50                   4.461306   \n",
       "3                    0.301968   4667816.50                   4.461197   \n",
       "4                    0.301639   4667831.00                   4.461257   \n",
       "..                        ...          ...                        ...   \n",
       "185                  0.503810   3466857.00                   3.775999   \n",
       "186                  0.492948   3463597.25                   3.777043   \n",
       "187                  0.482055   3459223.00                   3.774743   \n",
       "188                  0.488047   3460043.50                   3.777103   \n",
       "189                  0.496265   3463926.25                   3.778592   \n",
       "\n",
       "     batch-loss/chain-1/std/0  batch-loss/chain-2/mean/6  ...  \\\n",
       "0                    0.273078                   4.487923  ...   \n",
       "1                    0.257812                   4.469196  ...   \n",
       "2                    0.257505                   4.469173  ...   \n",
       "3                    0.257453                   4.469088  ...   \n",
       "4                    0.257757                   4.469166  ...   \n",
       "..                        ...                        ...  ...   \n",
       "185                  0.238505                   3.090042  ...   \n",
       "186                  0.238893                   3.087273  ...   \n",
       "187                  0.238713                   3.089475  ...   \n",
       "188                  0.238817                   3.091058  ...   \n",
       "189                  0.238841                   3.092528  ...   \n",
       "\n",
       "     batch-loss/chain-8/std/0  batch-loss/chain-1/mean/2  \\\n",
       "0                    0.266796                   4.433786   \n",
       "1                    0.274583                   4.419053   \n",
       "2                    0.274791                   4.419082   \n",
       "3                    0.274774                   4.419080   \n",
       "4                    0.274576                   4.419018   \n",
       "..                        ...                        ...   \n",
       "185                  0.267582                   3.617803   \n",
       "186                  0.267225                   3.611918   \n",
       "187                  0.267813                   3.607190   \n",
       "188                  0.267835                   3.606944   \n",
       "189                  0.268080                   3.609663   \n",
       "\n",
       "     batch-loss/chain-7/std/5  batch-loss/chain-1/mean/3  llc/std/2  \\\n",
       "0                    0.256581                   4.451212  17.755661   \n",
       "1                    0.268279                   4.446731  17.693489   \n",
       "2                    0.267966                   4.446737  17.725080   \n",
       "3                    0.268162                   4.446727  17.679588   \n",
       "4                    0.268173                   4.446680  17.683422   \n",
       "..                        ...                        ...        ...   \n",
       "185                  0.345056                   3.469490  24.637434   \n",
       "186                  0.344430                   3.462779  24.557392   \n",
       "187                  0.343320                   3.456326  24.520861   \n",
       "188                  0.342204                   3.456615  24.499849   \n",
       "189                  0.342910                   3.459869  24.534773   \n",
       "\n",
       "     batch-loss/chain-7/mean/3   wbic/std/std  batch-loss/chain-0/std/mean  \\\n",
       "0                     4.432512    1703.146477                     0.268458   \n",
       "1                     4.426115    2348.498732                     0.270319   \n",
       "2                     4.426130    2216.673692                     0.270326   \n",
       "3                     4.426131    2413.262776                     0.270338   \n",
       "4                     4.426101    2358.911137                     0.270344   \n",
       "..                         ...            ...                          ...   \n",
       "185                   3.420046  126904.854249                     0.486331   \n",
       "186                   3.418888  125944.830586                     0.485179   \n",
       "187                   3.418793  125107.340861                     0.485441   \n",
       "188                   3.415974  125458.254138                     0.485523   \n",
       "189                   3.418252  125992.322016                     0.484969   \n",
       "\n",
       "     model_seed    step  \n",
       "0             1       0  \n",
       "1             1       1  \n",
       "2             1       2  \n",
       "3             1       3  \n",
       "4             1       4  \n",
       "..          ...     ...  \n",
       "185           0  479797  \n",
       "186           0  484848  \n",
       "187           0  489898  \n",
       "188           0  494949  \n",
       "189           0  499999  \n",
       "\n",
       "[950 rows x 285 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "def merge_dfs(df1, df2, inplace=True):\n",
    "    if not inplace:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    seeds = df1[\"model_seed\"].unique()\n",
    "    steps = df1[\"step\"].unique()\n",
    "\n",
    "    stdlogger.info(f\"Merging {len(seeds)} seeds and {len(steps)} steps\")\n",
    "    for seed in seeds:\n",
    "        for step in steps:\n",
    "            for k in df2.columns:\n",
    "                if k not in df1.columns:\n",
    "                    df1.loc[(df1[\"model_seed\"] == seed) & (df1[\"step\"] == step), k] = df2.loc[(df2[\"model_seed\"] == seed) & (df2[\"step\"] == step), k]\n",
    "\n",
    "    return df1\n",
    "\n",
    "if os.path.exists(DATA / MODELS_ID / \"llcs.pt\"):\n",
    "    stdlogger.info(\"Retrieving LLC estimates from disk\")\n",
    "    with open(DATA / MODELS_ID / \"llcs.pt\", 'rb') as f:\n",
    "        llc_df = torch.load(f)\n",
    "\n",
    "else:\n",
    "    stdlogger.info(\"Retrieving LLC estimates from wandb sweep\")\n",
    "\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"devinterp/icl/{LLC_SWEEP_ID}\")\n",
    "    wandb_runs = sweep.runs\n",
    "\n",
    "    llc_df = None\n",
    "\n",
    "    for llc_run in tqdm.tqdm(wandb_runs):\n",
    "        history_df = llc_run.history()\n",
    "\n",
    "        llc_mean_columns = [f'llc/mean/{i}' for i in range(8)]\n",
    "        history_df[llc_mean_columns] = history_df[llc_mean_columns].replace(\"NaN\", np.nan)\n",
    "\n",
    "        llc_std_columns = [f'llc/std/{i}' for i in range(8)]\n",
    "        history_df[llc_std_columns] = history_df[llc_std_columns].replace(\"NaN\", np.nan)\n",
    "        history_df['model_seed'] = llc_run.config['task_config']['model_seed']\n",
    "        history_df['step'] = history_df['_step']\n",
    "\n",
    "        if llc_df is None:\n",
    "            llc_df = history_df\n",
    "        else:\n",
    "            llc_df = pd.concat([llc_df, history_df])\n",
    "\n",
    "    stdlogger.info(\"Saving LLCs to disk\")\n",
    "    with open(DATA / MODELS_ID / \"llcs.pt\", 'wb') as f:\n",
    "        torch.save(llc_df, f)\n",
    "\n",
    "stdlogger.info(\"Merging LLCs into evals\")\n",
    "evals_over_time_df = merge_dfs(evals_over_time_df, llc_df)\n",
    "\n",
    "stdlogger.info(\"Calculating derivatives\")\n",
    "for i in evals_over_time_df.model_seed.unique():\n",
    "    for column in ['llc/mean/mean']:\n",
    "        add_slopes(evals_over_time_df, column, i)\n",
    "\n",
    "llc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'icl.regression'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdevinfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_seed\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01micl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalysis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbaselines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fit_ridge\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01micl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_transformations\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdevinfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miterables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flatten_dict\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'icl.regression'"
     ]
    }
   ],
   "source": [
    "# Wait if my layer norm theory is right. Then we should see a sudden improvement in the ability of the model to make predictions for out-of-distribution xs/ys (not ws). \n",
    "from devinfra.utils.seed import set_seed\n",
    "from icl.analysis.baselines import fit_ridge\n",
    "from icl.regression.tasks import apply_transformations\n",
    "from devinfra.utils.iterables import flatten_dict\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def eval_loss(yhats, ys):\n",
    "    losses = ((yhats - ys) ** 2).mean(dim=0)[:, 0]\n",
    "    return [loss.item() for loss in losses]\n",
    "\n",
    "OOD_MULTIPLIER = 5\n",
    "ws = torch.normal(\n",
    "    mean=0.,\n",
    "    std=1.,\n",
    "    size=(BATCH_SIZE, D,),\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# sample i.i.d. inputs and outputs for each task according to the\n",
    "# regression model\n",
    "xs = torch.normal(\n",
    "    mean=0.,\n",
    "    std=1.,\n",
    "    size=(BATCH_SIZE, K, D),\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "errors = torch.normal(\n",
    "    mean=0.,\n",
    "    std=0.125,\n",
    "    size=(BATCH_SIZE, K, 1,),\n",
    "    device=DEVICE,\n",
    ")\n",
    "ys = apply_transformations(ws, xs, 0.125, DEVICE) # xs @ ws.view(BATCH_SIZE, D, 1) + errors\n",
    "\n",
    "ood_a_xs = 3 * xs\n",
    "ys_ood_a_inputs = apply_transformations(ws, ood_a_xs, 0.125, DEVICE) # ood_xs @ ws.view(BATCH_SIZE, D, 1) + errors\n",
    "ood_b_xs = 5 * xs\n",
    "ys_ood_b_inputs = apply_transformations(ws, ood_b_xs, 0.125, DEVICE) # ood_xs @ ws.view(BATCH_SIZE, D, 1) + errors\n",
    "ood_c_xs = 10 * xs\n",
    "ys_ood_c_inputs = apply_transformations(ws, ood_c_xs, 0.125, DEVICE) # ood_xs @ ws.view(BATCH_SIZE, D, 1) + errors\n",
    "ood_d_xs = 100 * xs\n",
    "ys_ood_d_inputs = apply_transformations(ws, ood_d_xs, 0.125, DEVICE) # ood_xs @ ws.view(BATCH_SIZE, D, 1) + errors\n",
    "\n",
    "first_xs = deepcopy(xs[:, 0:1, :])\n",
    "first_x_ws = fit_ridge(first_xs, ys[:, 0:1, :], 0.125)\n",
    "first_x_ys = apply_transformations(first_x_ws, xs, 0., DEVICE)\n",
    "\n",
    "first_xs /= torch.norm(first_xs, dim=-1, keepdim=True) ** 2\n",
    "for b in range(BATCH_SIZE):\n",
    "    first_xs[b] *= ys[b, 0, 0]\n",
    "\n",
    "ys_using_first_x = xs @ first_xs.view(BATCH_SIZE, D, 1) \n",
    "\n",
    "def eval_all(model):\n",
    "    ypreds = model(xs, ys)\n",
    "    results = {\n",
    "        \"loss\": eval_loss(ypreds, ys),\n",
    "        \"loss_0\": eval_loss(ypreds, torch.zeros_like(ys)),\n",
    "        \"loss_first_x\": eval_loss(ypreds, first_x_ys),\n",
    "        \"ood_a_inputs_loss\": eval_loss(model(ood_a_xs, ys_ood_a_inputs), ys_ood_a_inputs),\n",
    "        \"ood_a_tasks_loss\": eval_loss(model(xs, ys_ood_a_inputs), ys_ood_a_inputs),\n",
    "        \"ood_b_inputs_loss\": eval_loss(model(ood_b_xs, ys_ood_b_inputs), ys_ood_b_inputs),\n",
    "        \"ood_b_tasks_loss\": eval_loss(model(xs, ys_ood_b_inputs), ys_ood_b_inputs),\n",
    "        \"ood_c_inputs_loss\": eval_loss(model(ood_c_xs, ys_ood_c_inputs), ys_ood_c_inputs),\n",
    "        \"ood_c_tasks_loss\": eval_loss(model(xs, ys_ood_c_inputs), ys_ood_c_inputs),\n",
    "        \"ood_d_inputs_loss\": eval_loss(model(ood_d_xs, ys_ood_d_inputs), ys_ood_d_inputs),\n",
    "        \"ood_d_tasks_loss\": eval_loss(model(xs, ys_ood_d_inputs), ys_ood_d_inputs),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "for seed, (run, _models) in enumerate(zip(runs, all_models)):\n",
    "    # sample a batch of random tasks\n",
    "    for step, model in tqdm.tqdm(zip(steps, _models), total=len(steps), desc=f\"Seed {seed}\"):\n",
    "        metrics = eval_all(model)\n",
    "        \n",
    "        for k, v in metrics.items():\n",
    "            for i in range(8):\n",
    "                    evals_over_time_df.loc[((evals_over_time_df.step == step) & (evals_over_time_df.model_seed==seed)), f\"{k}/{i}\"] = v[i]\n",
    "                    \n",
    "            evals_over_time_df.loc[((evals_over_time_df.step == step) & (evals_over_time_df.model_seed==seed)), f\"{k}/mean\"] = np.mean(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhessian\n",
    "from pyhessian import hessian # Hessian computation\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs[0], inputs[1])\n",
    "\n",
    "hessian_stats_df = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(DATA / MODELS_ID / \"hessian_stats.pt\"):\n",
    "    stdlogger.info(\"Loading hessian stats from disk\")\n",
    "    with open(DATA / MODELS_ID / \"hessian_stats.pt\", 'rb') as f:\n",
    "        hessian_stats_df = torch.load(f)\n",
    "\n",
    "else:\n",
    "    stdlogger.info(\"Running hessian stats\")\n",
    "    hessian_stats = []\n",
    "\n",
    "    xs = xs.to('cpu')\n",
    "    ys = ys.to('cpu')\n",
    "\n",
    "    for i, _models in enumerate(all_models):\n",
    "        for step, model in zip(steps, tqdm.tqdm(_models)):\n",
    "            model = model.to('cpu')\n",
    "            ref_model = ModelWrapper(model)\n",
    "            hessian_comp = hessian(ref_model, F.mse_loss, data=((xs[:1024], ys[:1024]), ys[:1024]), cuda=False)\n",
    "\n",
    "            _top_evals, _ = hessian_comp.eigenvalues(top_n=20)\n",
    "            trace = hessian_comp.trace()\n",
    "\n",
    "            hessian_stats.append({\n",
    "                \"model_seed\": i,\n",
    "                \"step\": step,\n",
    "                \"hessian/trace\": trace,\n",
    "            } | {f\"hessian/evals/{i}\": _top_evals[i] for i in range(20)})\n",
    "            model.to('mps')\n",
    "                \n",
    "    xs = xs.to('mps')\n",
    "    ys = ys.to('mps')\n",
    "\n",
    "    hessian_stats_df = pd.DataFrame(hessian_stats)\n",
    "\n",
    "    stdlogger.info(\"Saving hessian stats to disk\")\n",
    "    with open(DATA / MODELS_ID / \"hessian_stats.pt\", 'wb') as f:\n",
    "        torch.save(hessian_stats_df, f)\n",
    "\n",
    "hessian_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model):\n",
    "    return model.state_dict()['token_sequence_transformer.token_embedding.weight']\n",
    "\n",
    "def get_unembedding(model):\n",
    "    return model.state_dict()['token_sequence_transformer.unembedding.1.weight']\n",
    "\n",
    "import itertools\n",
    "\n",
    "models1 = all_models[MODEL_SEED]\n",
    "\n",
    "embed_sing_vals = []\n",
    "postn_sing_vals = []\n",
    "entangling = []\n",
    "\n",
    "for step, model in zip(steps, models1):\n",
    "    embed = model.token_sequence_transformer.token_embedding.weight.detach().cpu().numpy()\n",
    "    U_embed, S_embed, Vt_embed = np.linalg.svd(embed)\n",
    "    embed_trace = np.sum(S_embed ** 2)\n",
    "    S_embed_normed = S_embed ** 2 / embed_trace \n",
    "\n",
    "    # pca = PCA(n_components=5).fit(embed)\n",
    "\n",
    "    for i in range(len(S_embed)):\n",
    "        embed_sing_vals.append({\n",
    "            \"step\": step, \"index\": i,\n",
    "            \"embed/S\": S_embed[i],\n",
    "            # \"embed/S_normed\": pca.explained_variance_ratio_[i],\n",
    "            \"embed/S_normed\": S_embed_normed[i], \n",
    "        })\n",
    "    \n",
    "    postn = model.token_sequence_transformer.postn_embedding.weight.detach().cpu().numpy()\n",
    "    U_postn, S_postn, Vt_postn = np.linalg.svd(postn)\n",
    "    postn_trace = np.sum(S_postn ** 2)\n",
    "    S_postn_normed = S_postn ** 2 / postn_trace\n",
    "\n",
    "    # pca = PCA(n_components=16).fit(postn)\n",
    "\n",
    "    for i in range(len(S_postn)):\n",
    "        postn_sing_vals.append({\n",
    "            \"step\": step, \"index\": i,\n",
    "            \"postn/S\": S_postn[i], \n",
    "            # \"postn/S_normed\": pca.explained_variance_ratio_[i], # \n",
    "            \"postn/S_normed\": S_postn_normed[i], \n",
    "        })\n",
    "\n",
    "    # Compute the cossim between the singular vectors of the embedding and the positional encoding space\n",
    "    # entangling = np.zeros((len(S_embed), len(S_postn)))\n",
    "\n",
    "    # for i, j in itertools.product(range(len(S_embed)), range(len(S_postn))):\n",
    "    #     v = V_embed[i]\n",
    "    #     u = V_postn[j]\n",
    "    #     cossim = np.dot(v, u) / (np.linalg.norm(v) * np.linalg.norm(u))\n",
    "    #     entangling[i, j] = cossim\n",
    "\n",
    "\n",
    "    cossims = []\n",
    "    for i in range(len(S_embed)):           \n",
    "        u = embed[:, i] # U_embed[:,]\n",
    "        # v = Vt_postn[i]\n",
    "        u_proj = np.dot(U_postn, u)\n",
    "        cossim = np.abs(np.dot(u_proj, u) / (np.linalg.norm(u_proj) * np.linalg.norm(u)))\n",
    "        cossims.append(cossim)\n",
    "\n",
    "    # cossims = sorted(cossims, reverse=True) \n",
    "    \n",
    "    for i in range(len(S_embed)):\n",
    "        entangling.append({\n",
    "            \"step\": step,\n",
    "            \"embed/trace\": embed_trace,\n",
    "            \"postn/trace\": postn_trace,\n",
    "            \"index\": i,\n",
    "            \"cossim\": cossims[i],   \n",
    "        })\n",
    "\n",
    "print(embed.shape, postn.shape)   \n",
    "print(U_embed.shape, S_embed.shape, Vt_embed.shape)\n",
    "print(U_postn.shape, S_postn.shape, Vt_postn.shape)\n",
    "\n",
    "embed_sing_vals = pd.DataFrame(embed_sing_vals)\n",
    "postn_sing_vals = pd.DataFrame(postn_sing_vals)\n",
    "entangling = pd.DataFrame(entangling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unembeddings = []\n",
    "\n",
    "for step, model in zip(steps, models1):\n",
    "    for subset in [\"weight\", \"bias\"]:\n",
    "        layer = f\"ln.{subset}\"\n",
    "        for i, param in enumerate(getattr(model.token_sequence_transformer.unembedding[0], subset)):\n",
    "            unembeddings.append({\"p\": param.item(), \"step\": step, \"layer\": layer, \"i\": i})\n",
    "\n",
    "        layer = f\"linear.{subset}\"\n",
    "        layer_param = getattr(model.token_sequence_transformer.unembedding[1], subset)\n",
    "        if subset == \"weight\":\n",
    "            layer_param = layer_param[0, :]\n",
    "            for i, param in enumerate(layer_param):\n",
    "                unembeddings.append({\"p\": param.item(), \"step\": step, \"layer\": layer, \"i\": i})\n",
    "        else:\n",
    "            layer_param = layer_param[0]\n",
    "            unembeddings.append({\"p\": layer_param.item(), \"step\": step, \"layer\": layer, \"i\": 0})\n",
    "\n",
    "\n",
    "unembeddings = pd.DataFrame(unembeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_unembeddings = []\n",
    "\n",
    "for step, model in zip(steps, models1):\n",
    "    reduced_weight = model.token_sequence_transformer.unembedding[1].weight[0, :] * model.token_sequence_transformer.unembedding[0].weight\n",
    "    reduced_bias = model.token_sequence_transformer.unembedding[1].weight[0, :] @ model.token_sequence_transformer.unembedding[0].bias + model.token_sequence_transformer.unembedding[1].bias[0]\n",
    "\n",
    "    for i, param in enumerate(reduced_weight):\n",
    "        reduced_unembeddings.append({\"p\": param.item(), \"subset\": \"weight\", \"step\": step,  \"i\": i})\n",
    "\n",
    "    reduced_unembeddings.append({\"p\": reduced_bias.item(), \"subset\": \"bias\", \"step\": step,  \"i\": 0})\n",
    "\n",
    "reduced_unembeddings = pd.DataFrame(reduced_unembeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.analysis.slt import prepend_keys\n",
    "\n",
    "layer_norms = [\n",
    "    \"token_sequence_transformer.blocks.0.layer_norms.0\",\n",
    "    \"token_sequence_transformer.blocks.0.layer_norms.1\",\n",
    "    \"token_sequence_transformer.blocks.1.layer_norms.0\",\n",
    "    \"token_sequence_transformer.blocks.1.layer_norms.1\",\n",
    "    \"token_sequence_transformer.unembedding.0\",\n",
    "]\n",
    "\n",
    "list(model.state_dict().keys())\n",
    "\n",
    "def get_ln(model, key):\n",
    "    return (model.state_dict()[f'{key}.weight'], model.state_dict()[f'{key}.bias'])\n",
    "\n",
    "unembedding_lns = [get_ln(model, 'token_sequence_transformer.unembedding.0') for model in models]\n",
    "block_1_attn_lns =  [get_ln(model, 'token_sequence_transformer.blocks.0.layer_norms.0') for model in models]\n",
    "block_1_mlp_lns =  [get_ln(model, 'token_sequence_transformer.blocks.0.layer_norms.1') for model in models]\n",
    "block_2_attn_lns =  [get_ln(model, 'token_sequence_transformer.blocks.1.layer_norms.0') for model in models]\n",
    "block_2_mlp_lns =  [get_ln(model, 'token_sequence_transformer.blocks.1.layer_norms.1') for model in models]\n",
    "\n",
    "def ln_norm(weight, bias):\n",
    "    return torch.norm(weight).detach().cpu().numpy()\n",
    "\n",
    "def ln_norm_std(weight, bias):\n",
    "    return torch.std(weight.abs()).detach().cpu().numpy()\n",
    "\n",
    "unembedding_ln_norms = [ln_norm(weight, bias) for weight, bias in unembedding_lns]\n",
    "block_1_attn_ln_norms = [ln_norm(weight, bias) for weight, bias in block_1_attn_lns]\n",
    "block_1_mlp_ln_norms = [ln_norm(weight, bias) for weight, bias in block_1_mlp_lns]\n",
    "block_2_attn_ln_norms = [ln_norm(weight, bias) for weight, bias in block_2_attn_lns]\n",
    "block_2_mlp_ln_norms = [ln_norm(weight, bias) for weight, bias in block_2_mlp_lns]\n",
    "\n",
    "unembedding_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in unembedding_lns])\n",
    "block_1_attn_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in block_1_attn_lns])\n",
    "block_1_mlp_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in block_1_mlp_lns])\n",
    "block_2_attn_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in block_2_attn_lns])\n",
    "block_2_mlp_ln_norms_std = np.array([ln_norm_std(weight, bias) for weight, bias in block_2_mlp_lns])\n",
    "\n",
    "def frac_nonzero(weight, eps=1e-1):\n",
    "    return (weight.abs() > eps).float().mean().detach().cpu().numpy()\n",
    "\n",
    "unembedding_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in unembedding_lns]\n",
    "block_1_attn_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in block_1_attn_lns]\n",
    "block_1_mlp_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in block_1_mlp_lns]\n",
    "block_2_attn_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in block_2_attn_lns]\n",
    "block_2_mlp_ln_norm_nonzero = [frac_nonzero(weight) for weight, bias in block_2_mlp_lns]\n",
    "\n",
    "ln_stats = []\n",
    "\n",
    "def get_stats(weight):\n",
    "    return {\n",
    "        \"norm\": weight.norm().item(),\n",
    "        \"norm_std\": weight.abs().std().item(),\n",
    "        \"std\": weight.std().item(),\n",
    "        \"mean\": weight.mean().item(),\n",
    "        \"max\": weight.max().item(),\n",
    "        \"min\": weight.min().item(),\n",
    "    }\n",
    "    \n",
    "\n",
    "for step, model in zip(steps, models1):\n",
    "    for layer in [ \"blocks.0.layer_norms.0\", \"blocks.0.layer_norms.1\", \"blocks.1.layer_norms.0\", \"blocks.1.layer_norms.1\", \"unembedding.0\"]:\n",
    "        weight, bias = get_ln(model, f\"token_sequence_transformer.{layer}\")\n",
    "\n",
    "        ln_stats.append({\n",
    "            \"step\": step,\n",
    "            \"layer\": layer,\n",
    "            \"layer_pretty\": layer.replace(\"_\", \" \").title(),\n",
    "            **prepend_keys(get_stats(weight), \"weight\"),\n",
    "            **prepend_keys(get_stats(bias), \"bias\"),\n",
    "        })\n",
    "\n",
    "ln_stats = pd.DataFrame(ln_stats)\n",
    "ln_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import List, Union, Iterable, Optional\n",
    "from torchtyping import TensorType\n",
    "from devinfra.utils.iterables import map_nested\n",
    "\n",
    "from icl.experiments.utils import iter_models\n",
    "from devinfra.utils.iterables import flatten_dict\n",
    "\n",
    "from icl.train import Run\n",
    "\n",
    "def compute_attention_entropies(attn: TensorType[\"B\", \"H\", \"2K\", \"2K\"]):\n",
    "    \"\"\"\n",
    "    Computes the entropy of each token in each head, averaged across the batch, \n",
    "    then averages this over heads. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Threshold attention weights to avoid log(0)\n",
    "    log_attention = torch.where(attn > 0, torch.log(attn), torch.tensor(0.0).to(attn.device))\n",
    "    entropy_per_token = - torch.sum(attn * log_attention, dim=-1).mean(dim=0).squeeze(-1) # TensorType[\"H\", \"2K\"]\n",
    "\n",
    "    num_heads, num_tokens = entropy_per_token.shape\n",
    "\n",
    "    entropy_per_head = entropy_per_token.mean(dim=-1) # TensorType[\"H\"]\n",
    "    entropy = entropy_per_head.mean() # TensorType[]    \n",
    "    \n",
    "    # Each token computes entropy over a variable context length, so we normalize by the maximum possible entropy\n",
    "    # for a token with a fixed context length.\n",
    "\n",
    "    max_entropy_per_token = torch.log2(torch.arange(1, num_tokens + 1).to(attn.device)) # TensorType[\"H\", \"2K\"]\n",
    "    max_entropy_per_token[0] = 1. # Special case for the first token to avoid dividing by 0\n",
    "\n",
    "    entropy_per_token_normalized = entropy_per_token / max_entropy_per_token\n",
    "    entropy_per_head_normalized = entropy_per_token_normalized.mean(dim=-1) # TensorType[\"H\"]\n",
    "    entropy_normalized = entropy_per_head_normalized.mean() # TensorType[]    \n",
    "\n",
    "    results: Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]] = {\"mean\": entropy, \"mean_normalized\": entropy_normalized}\n",
    "\n",
    "    for i in range(num_heads):\n",
    "        head_results = {\"mean\": entropy_per_head[i], \"mean_normalized\": entropy_per_head_normalized[i]}\n",
    "\n",
    "        for j in range(num_tokens):\n",
    "            head_results[f\"token_{j}\"] = entropy_per_token[i, j]\n",
    "            head_results[f\"token_{j}_normalized\"] = entropy_per_token_normalized[i, j]\n",
    "\n",
    "        results[f\"head_{i}\"] = head_results\n",
    "\n",
    "    return map_nested(lambda x: convert_tensor(x, \"np\"), results)\n",
    "\n",
    "\n",
    "def get_attention_entropies_trace(\n",
    "    steps: List[int],\n",
    "    models: Iterable[nn.Module],\n",
    "    xs: torch.Tensor,\n",
    "    ys: torch.Tensor,\n",
    "    **paths,\n",
    "):\n",
    "    results = defaultdict(list)\n",
    "    reverse_paths = {v: k for k, v in paths.items()}\n",
    "\n",
    "    for activations in extract_activations_over_checkpoints(models, xs, ys, *paths.values(), return_type=\"pt\"):\n",
    "        for k, v in activations.items():\n",
    "            if k == \"\":\n",
    "                continue\n",
    "            path = reverse_paths[k]\n",
    "            results[path].append(compute_attention_entropies(v))\n",
    "\n",
    "    values = []\n",
    "\n",
    "    for i in range(len(steps)):\n",
    "        value = {}\n",
    "\n",
    "        for block in results.keys():\n",
    "            value[block] = results[block][i]\n",
    "        \n",
    "        value[\"step\"] = steps[i]\n",
    "        values.append(flatten_dict(value, flatten_lists=True))\n",
    "\n",
    "    return pd.DataFrame(values)\n",
    "\n",
    "def compute_attention_variability(attn: TensorType[\"B\", \"H\", \"2K\", \"2K\"]):\n",
    "    \"\"\"\n",
    "    Computes the variability of the attention pattern of each head across the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    num_batches, num_heads, num_tokens, _ = attn.shape\n",
    "    num_tokens = num_tokens - 1\n",
    "\n",
    "    results: Dict[str, Union[float, Dict[str, float]]] = {}\n",
    "\n",
    "    attn = attn[:, :, :-1, :-1] # Remove the last y token which gets to training signal\n",
    "    mean_attn_pattern = attn.mean(dim=0, keepdim=True)\n",
    "\n",
    "    log_attention = torch.where(attn > 0, torch.log2(attn), torch.tensor(0.0).to(attn.device))\n",
    "    entropy_per_token = - torch.sum(attn * log_attention, dim=-1).mean(dim=0).squeeze(-1)\n",
    "\n",
    "    variability = (((attn - mean_attn_pattern).abs().sum(dim=-1)) / (2 * mean_attn_pattern.sum(dim=-1))).mean(dim=0) \n",
    "    prev_token_attn = torch.zeros(num_heads, num_tokens, device=attn.device)\n",
    "\n",
    "    self_attn = attn.diagonal(dim1=-2, dim2=-1).mean(dim=0)\n",
    "    prev_token_attn[:, 1:] = attn.diagonal(dim1=-2, dim2=-1, offset=-1).mean(dim=0)\n",
    "    x_tokens_attn = attn[:, :, :, 0::2].sum(dim=-1).mean(dim=0)\n",
    "    y_tokens_attn = attn[:, :, :, 1::2].sum(dim=-1).mean(dim=0)\n",
    "    hardness_per_token = (attn <= 0.001).float().mean(dim=-1).mean(dim=0)\n",
    "    distances = (torch.arange(0, num_tokens, device=attn.device).view(1, 1, 1, -1) * attn).sum(dim=-1).mean(dim=0)\n",
    "    # first_x_attn = attn[:, :, :, 0].mean(dim=0)\n",
    "\n",
    "    # print(hardness_per_token.shape)\n",
    "    for i in range(15):\n",
    "        hardness_per_token[:, i] = hardness_per_token[:, i] / (i + 1)\n",
    "        distances[:, i] = distances[:, i] / (i + 1)\n",
    "\n",
    "    results[\"entropy\"] = entropy_per_token.mean() \n",
    "    results[\"variability\"] = variability.mean().item()\n",
    "    results[\"self_attn\"] = self_attn.mean().item()\n",
    "    results[\"prev_token_attn\"] = prev_token_attn.mean().item()\n",
    "    results[\"x_tokens_attn\"] = x_tokens_attn.mean().item()\n",
    "    results[\"y_tokens_attn\"] = y_tokens_attn.mean().item()\n",
    "    results[\"hardness\"] = hardness_per_token.mean().item()\n",
    "    results[\"distance\"] = distances.mean().item()\n",
    "\n",
    "    results[\"x\"] = {\n",
    "        \"entropy\": entropy_per_token[::2].mean().item(),\n",
    "        \"variability\": variability[::2].mean().item(),\n",
    "        \"self_attn\": self_attn[::2].mean().item(),\n",
    "        \"prev_token_attn\": prev_token_attn[::2].mean().item(),\n",
    "        \"x_tokens_attn\": x_tokens_attn[::2].mean().item(),\n",
    "        \"y_tokens_attn\": y_tokens_attn[::2].mean().item(),\n",
    "        \"hardness\": hardness_per_token[::2].mean().item(),\n",
    "        \"distance\": distances[::2].mean().item()\n",
    "    }\n",
    "\n",
    "    results[\"y\"] = {\n",
    "        \"entropy\": entropy_per_token[1::2].mean().item(),\n",
    "        \"variability\": variability[1::2].mean().item(),\n",
    "        \"self_attn\": self_attn[1::2].mean().item(),\n",
    "        \"prev_token_attn\": prev_token_attn[1::2].mean().item(),\n",
    "        \"x_tokens_attn\": x_tokens_attn[1::2].mean().item(),\n",
    "        \"y_tokens_attn\": y_tokens_attn[1::2].mean().item(),\n",
    "        \"hardness\": hardness_per_token[1::2].mean().item(),\n",
    "        \"distance\": distances[1::2].mean().item()\n",
    "    }\n",
    "\n",
    "    for i in range(num_heads):  \n",
    "        head_entropy = entropy_per_token[i]\n",
    "        head_variability = variability[i]\n",
    "        head_self_attn = self_attn[i]\n",
    "        head_prev_token_attn = prev_token_attn[i]\n",
    "        head_x_tokens_attn = x_tokens_attn[i]\n",
    "        head_y_tokens_attn = y_tokens_attn[i]\n",
    "        head_hardness = hardness_per_token[i]\n",
    "        head_distance = distances[i]\n",
    "\n",
    "        head_results = {\n",
    "            \"entropy\": head_entropy.mean().item(),\n",
    "            \"variability\": head_variability.mean().item(),\n",
    "            \"self_attn\": head_self_attn.mean().item(),\n",
    "            \"prev_token_attn\": head_prev_token_attn.mean().item(),\n",
    "            \"x_tokens_attn\": head_x_tokens_attn.mean().item(),\n",
    "            \"y_tokens_attn\": head_y_tokens_attn.mean().item(),\n",
    "            \"hardness\": head_hardness.mean().item(),\n",
    "            \"distance\": head_distance.mean().item()\n",
    "        }\n",
    "\n",
    "        for x_or_y in (1, 0):\n",
    "            head_half_results = dict(\n",
    "                entropy = head_entropy[x_or_y::2].mean().item(),\n",
    "                variability = head_variability[x_or_y::2].mean().item(),\n",
    "                self_attn = head_self_attn[x_or_y::2].mean().item(),\n",
    "                prev_token_attn = head_prev_token_attn[x_or_y::2].mean().item(),\n",
    "                x_tokens_attn = head_x_tokens_attn[x_or_y::2].mean().item(),\n",
    "                y_tokens_attn = head_y_tokens_attn[x_or_y::2].mean().item(),\n",
    "                hardness = head_hardness[x_or_y::2].mean().item(),\n",
    "                distance = head_distance[x_or_y::2].mean().item()\n",
    "            )\n",
    "\n",
    "            head_results[\"x\" if not x_or_y else \"y\"] = head_half_results\n",
    "\n",
    "            for j in range(x_or_y, num_tokens, 2):\n",
    "                head_results[f\"token_{j}/entropy\"] = head_entropy[j].item()\n",
    "                head_results[f\"token_{j}/entropy_normalized\"] = head_entropy[j].item() / np.log2(j + 1)\n",
    "                head_results[f\"token_{j}/variability\"] = head_variability[j].item()\n",
    "                head_results[f\"token_{j}/self_attn\"] = self_attn[i, j].item()\n",
    "                head_results[f\"token_{j}/prev_token_attn\"] = prev_token_attn[i, j].item()\n",
    "                head_results[f\"token_{j}/x_tokens_attn\"] = x_tokens_attn[i, j].item()\n",
    "                head_results[f\"token_{j}/y_tokens_attn\"] = y_tokens_attn[i, j].item()\n",
    "                head_results[f\"token_{j}/hardness\"] = hardness_per_token[i, j].item()\n",
    "                head_results[f\"token_{j}/distance\"] = distances[i, j].item()\n",
    "\n",
    "        results[f\"head_{i}\"] = head_results\n",
    "\n",
    "    return map_nested(lambda x: convert_tensor(x, \"np\"), results)\n",
    "\n",
    "\n",
    "def get_attention_variabilities(\n",
    "    steps: List[int],\n",
    "    models: Iterable[nn.Module],\n",
    "    xs: torch.Tensor,\n",
    "    ys: torch.Tensor,\n",
    "    **paths,\n",
    "):\n",
    "    results = defaultdict(list)\n",
    "    reverse_paths = {v: k for k, v in paths.items()}\n",
    "\n",
    "    for activations in tqdm.tqdm(extract_activations_over_checkpoints(models, xs, ys, *paths.values(), return_type=\"pt\"), total=len(models)):\n",
    "        for k, v in activations.items():\n",
    "            if k == \"\":\n",
    "                continue\n",
    "            path = reverse_paths[k]\n",
    "            results[path].append(compute_attention_variability(v))\n",
    "\n",
    "    values = []\n",
    "\n",
    "    for i in range(len(steps)):\n",
    "        value = {}\n",
    "\n",
    "        for block in results.keys():\n",
    "            value[block] = results[block][i]\n",
    "        \n",
    "        value[\"step\"] = steps[i]\n",
    "        values.append(flatten_dict(value, flatten_lists=True))\n",
    "\n",
    "    return pd.DataFrame(values)\n",
    "\n",
    "attn_variabilities = get_attention_variabilities(\n",
    "    run.checkpointer.file_ids,\n",
    "    models, \n",
    "    xs, \n",
    "    ys, \n",
    "    **{f\"block_{b}\": f\"token_sequence_transformer.blocks.{b}.attention.attention_softmax\" for b in range(num_blocks)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "from functools import partial\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import sys\n",
    "\n",
    "del sys.modules['icl.figures.colors']\n",
    "\n",
    "from icl.figures.colors import plot_transitions, gen_transition_colors, get_transition_type, PRIMARY, SECONDARY, TERTIARY, BRED, BBLUE, BRED, BGREEN, decrease_brightness, increase_saturation, increase_contrast, rainbow, LR_TRANSITION_COLORS\n",
    "\n",
    "def get_transition_indices(steps, transitions):\n",
    "    transition_indices = []\n",
    "    for step in steps:\n",
    "        # Find the index of the transition that the current step falls into\n",
    "        index = next((i for i, transition in enumerate(transitions) if transition[0] <= step < transition[1]), None)\n",
    "        transition_indices.append(index if index is not None else -1)\n",
    "\n",
    "    return transition_indices\n",
    "\n",
    "def get_nearest_step(step):\n",
    "    idx = np.argmin(np.abs(np.array(steps) - step))\n",
    "    return steps[idx]\n",
    "\n",
    "TRANSITIONS = [\n",
    "    (0, 1500, 'R1'),\n",
    "    (1500, 40_000, 'R2'),\n",
    "    (40000, 320000, 'R3'),\n",
    "    (320000, 500000, 'R4'),\n",
    "]\n",
    "\n",
    "TRANSITIONS = [(get_nearest_step(start), get_nearest_step(end), label) for start, end, label in TRANSITIONS]\n",
    "\n",
    "transition_rainbow = list(reversed(rainbow(len(TRANSITIONS))))\n",
    "transitions_cmap = LinearSegmentedColormap.from_list(\"transitions\", LR_TRANSITION_COLORS)\n",
    "gradient = np.linspace(0, 1, 256)\n",
    "gradient = np.vstack((gradient, gradient))\n",
    "_plot_transitions = partial(plot_transitions, transitions=TRANSITIONS, colors=LR_TRANSITION_COLORS)\n",
    "\n",
    "transitions_of_steps = get_transition_indices(steps, TRANSITIONS)\n",
    "highlight_steps = list(map(get_nearest_step, [t[0] for t in TRANSITIONS][1:]))\n",
    "\n",
    "# Show the transition colors\n",
    "fig, ax = plt.subplots(figsize=(10, 1))\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(100, 500_000)\n",
    "_plot_transitions(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple-Seed Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_palette('deep')\n",
    "\n",
    "# Data setup (use your actual data here)\n",
    "# For demonstration, replace evals_over_time_df with your DataFrame\n",
    "# evals_over_time_df = ...\n",
    "\n",
    "# Create figure\n",
    "fig, axs = plt.subplots(2, 1, figsize=(WIDTH * 1.5, HEIGHT * 1.5))\n",
    "\n",
    "axs[0].set_ylabel(r'Test loss  $\\hat\\ell(w_t)$' '\\n')\n",
    "axs[1].set_ylabel(r'Local learning coeff.  $\\hat\\lambda(w_t)$')\n",
    "\n",
    "# First line plot\n",
    "sns.lineplot(evals_over_time_df, x='step', y='pretrain/mse', ax=axs[0])\n",
    "# axs[0].set_title(r'(b) Loss over Time')\n",
    "axs[0].set_xscale('log')\n",
    "# axs[0].set_yscale('log')\n",
    "\n",
    "# Second line plot\n",
    "sns.lineplot(evals_over_time_df, x='step', y='llc/mean/mean', ax=axs[1])\n",
    "# axs[1].set_title(r'(d) Local Learning Coefficient over Time')\n",
    "axs[1].set_xscale('log')\n",
    "\n",
    "# Set x-label for both plots\n",
    "# for ax in axs:\n",
    "\n",
    "handles = _plot_transitions(axs, xlim=(100, 500_000)) \n",
    "axs[0].set_xlabel('')\n",
    "axs[1].set_xlabel('Training step $t$')\n",
    "\n",
    "\n",
    "# fig.legend(handles=handles, loc='upper center', bbox_to_anchor=(1.05, .65), ncol=1)\n",
    "fig.legend(handles=handles, loc='upper center', bbox_to_anchor=(0.55, 0.01), ncol=len(TRANSITIONS))\n",
    "\n",
    "# axs[1].legend(handles=handles, loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=len(TRANSITIONS))\n",
    "# Layout adjustments\n",
    "plt.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "fig.savefig(FIGURES / f\"lr-fig1-top.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Specific Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SEED = 0  # 0, 1, 2, 3, 4\n",
    "MODEL_ID = f\"LR{MODEL_SEED}\"\n",
    "\n",
    "if not os.path.exists(FIGURES / MODEL_ID):\n",
    "    os.makedirs(FIGURES / MODEL_ID)\n",
    "\n",
    "models1 = all_models[MODEL_SEED]\n",
    "final_model = deepcopy(models1[-1]).to('cpu')\n",
    "run = runs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t1, t2, _ in TRANSITIONS: # + [(499_999, None, None)]:\n",
    "    print(f\"Step {t1}->{t2} Delta Lambdahat:\", evals_over_time_df.loc[(evals_over_time_df.step == get_nearest_step(t2)) & (evals_over_time_df.model_seed == MODEL_SEED)]['llc/mean/mean'].values[0] - evals_over_time_df.loc[(evals_over_time_df.step == get_nearest_step(t1)) & (evals_over_time_df.model_seed == MODEL_SEED)]['llc/mean/mean'].values[0])\n",
    "    print(f\"Step {t1}->{t2} Delta Loss:\", evals_over_time_df.loc[(evals_over_time_df.step == get_nearest_step(t2)) & (evals_over_time_df.model_seed == MODEL_SEED)]['pretrain/mse'].values[0] - evals_over_time_df.loc[(evals_over_time_df.step == get_nearest_step(t1)) & (evals_over_time_df.model_seed == MODEL_SEED)]['pretrain/mse'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import lines as mlines\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "\n",
    "_df = evals_over_time_df.loc[evals_over_time_df.model_seed == MODEL_SEED]\n",
    "\n",
    "metrics_to_plot = [\n",
    "    (r\"\\hat\\ell(w_t)\", \"pretrain/mse\", {\"logy\": True, \"derivative\": \"d_dlogt\", \"spline\": True, \"s\": 0.1}, ),\n",
    "    # (r\"L_\\mathcal{G}(t)\", _df[\"true/mse\"], {\"logy\": False}),\n",
    "    (r\"\\hat \\lambda(w_t)\", 'llc/mean/mean', {\"derivative\": \"d_dlogt\", \"spline\": True}),\n",
    "    (r\"|w_t|\", \"weight/norm\", {\"derivative\": \"d_dt\", \"logy\": True, \"spline\": True, \"s\": 0.1}),\n",
    "] \n",
    "fig, axes = plt.subplots(2, len(metrics_to_plot), figsize=(FULL_WIDTH * 1.25, FULL_HEIGHT))\n",
    "\n",
    "# axes = np.array(axes)\n",
    "axes = axes.reshape(2, len(metrics_to_plot))\n",
    "\n",
    "def str_dlog_dlogt(s):\n",
    "    return r\"$\\delta \\log \" + s + r\"/\\delta\\log t$\"\n",
    "\n",
    "for i, (metric_name, metric_key, kwargs) in enumerate(metrics_to_plot):\n",
    "    use_spline = kwargs.get(\"spline\", False)\n",
    "\n",
    "    sns.lineplot(data=_df, x=\"step\", y=_df[metric_key], ax=axes[0, i],label=metric_name, alpha=1 - use_spline * 0.75)\n",
    "    # axes[0, i].plot(_df['step'], metric_values, label=metric_name, marker='.')\n",
    "    axes[0, i].set_title(f\"\")\n",
    "    axes[0, i].set_xlabel('Step $t$')\n",
    "    axes[0, i].set_ylabel(f\"${metric_name}$\")\n",
    "\n",
    "    if kwargs.get(\"logy\", False):\n",
    "        axes[0, i].set_yscale('log')\n",
    "\n",
    "    axes[0, i].legend().remove()\n",
    "\n",
    "    slope_type = kwargs.get(\"derivative\", \"d_dlogt\")\n",
    "\n",
    "    if slope_type == \"d_dlogt\":\n",
    "        slope_name = str_d_dlogt(metric_name)\n",
    "    elif slope_type == \"d_dt\":\n",
    "        slope_name = str_d_dt(metric_name)\n",
    "    elif slope_type == \"dlog_dlogt\":\n",
    "        slope_name = str_dlog_dlogt(metric_name)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown slope type {slope_type}\")\n",
    "\n",
    "    sns.lineplot(data=_df, x=\"step\", y=f\"{metric_key}/{slope_type}\", ax=axes[1, i], label=metric_name + \" Slope\", alpha=1 - use_spline * 0.75)\n",
    "    axes[1, i].axhline(0, linestyle='--', color='gray')\n",
    "    axes[1, i].set_title(\"\")\n",
    "    axes[1, i].set_xlabel('Step, $t$')\n",
    "    axes[1, i].set_ylabel(slope_name)\n",
    "    axes[1, i].legend().remove()\n",
    "    \n",
    "    if use_spline:     \n",
    "        _steps = np.log(np.array(steps) + 1 ).reshape((-1, 1))\n",
    "        _y = _df.groupby('step').mean()[metric_key].values\n",
    "\n",
    "        kernel = C(1.0, (1e-3, 1e3)) * RBF(3, (5e-1, 1e3))\n",
    "\n",
    "        # Create a Gaussian Process Regressor\n",
    "        gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "\n",
    "        # Fit the Gaussian Process\n",
    "        gp.fit(_steps, _y)\n",
    "        _ypred = gp.predict(_steps)\n",
    "\n",
    "        if slope_type == \"d_dlogt\":\n",
    "            _derivy = d_dt(_steps, _ypred)\n",
    "        elif slope_type == \"d_dt\":\n",
    "            _derivy = d_dt(np.exp(_steps), _ypred)\n",
    "        elif slope_type == \"dlog_dlogt\":            \n",
    "            _derivy = d_dt(_steps, np.log(_ypred))\n",
    "        \n",
    "        axes[0, i].plot(steps, _ypred, label=\"Spline\", linestyle='--', color=BRED)\n",
    "        axes[1, i].plot(steps, _derivy, label=\"Spline\", linestyle='--', color=BRED)\n",
    "\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(100, 500_000)\n",
    "    # ax.set_ylabel(\"\")\n",
    "\n",
    "# axes[0, 1].set_ylim(0, 100)\n",
    "axes[1, 0].set_ylim(-2.25, 2.25)\n",
    "axes[1, 1].set_ylim(-150, 160)\n",
    "\n",
    "patch_list = plot_transitions(axes, TRANSITIONS, xlim=True, colors=LR_TRANSITION_COLORS)\n",
    "\n",
    "# axes[1, 1].set_yscale('symlog')\n",
    "# axes[1, 0].set_yscale('symlog')\n",
    "# axes[0,0].set_ylim(0, 70)\n",
    "\n",
    "milestone_labels = [label for _, _, label in TRANSITIONS]\n",
    "gp_fit_patch = mlines.Line2D([], [], color=BRED, linestyle='--', label=\"GP Fit\")\n",
    "fig.legend(patch_list + [gp_fit_patch], milestone_labels + [\"Fit\"], loc='upper center', bbox_to_anchor=(0.5, -0.025), ncol=len(TRANSITIONS) + 1)\n",
    "\n",
    "fig.set_facecolor(\"white\")\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(FIGURES / MODEL_ID /\"loss-llc-with-slopes.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k0 = 0\n",
    "\n",
    "titles_and_labels = {\n",
    "    \"loss\": (\"Test Loss $L_\\mathrm{val}$\", \"MSE\"),\n",
    "    \"loss_0\": (\"$\\mathbb{E}[|\\hat{y}_k|^2]$\", r\"$\\|\\hat y_k\\|^2$\"),\n",
    "    # \"loss_first_x\": (\"(MSE from Ridge\", \"MSE\"),\n",
    "    \"pretrain/delta_ridge/token\": (\"MSE from Ridge\", \"MSE\"),  \n",
    "    \"ood_inputs_loss\": (r\"MSE on $x_i \\sim \\mathcal{N}(0, 5I_D)$\", \"MSE\"),\n",
    "    \"ood_targets_loss\": (r\"MSE on $\\mathbf{t} \\sim \\mathcal{N}(0, 5I_D)$\", \"MSE\"),\n",
    "    \"llc/mean\": (r\"Per-Token $\\lambda_t$\", \"$\\lambda_t^{(i)}$\"),\n",
    "    \"icl_score\": (f\"$ICL_{{{k0+1}:8}}(w_t)/g$\", \"ICL\"),\n",
    "    \"ood_inputs_rel_loss\": (r\"$\\frac{\\mathrm{MSE}(5 x_i)}{\\mathrm{MSE}(x_i)}$\", \"MSE\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Prediction Norm, OOD Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(WIDTH, HEIGHT * 2))  # Adjust the figsize as needed\n",
    "\n",
    "df = evals_over_time_df.loc[evals_over_time_df.model_seed == MODEL_SEED]\n",
    "\n",
    "metrics = ['loss_0', 'icl_score'] #, \"ood_inputs_rel_loss\"]# \"loss\", \"ood_loss\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(WIDTH * 1.5, HEIGHT * 1.5))  # Adjust the figsize as needed\n",
    "token_cmap = ScalarMappable(norm=Normalize(vmin=0, vmax=8), cmap=\"viridis\")\n",
    "\n",
    "ood_colors = sns.color_palette(\"viridis\", 4)\n",
    "\n",
    "\n",
    "for m, metric in enumerate(metrics):\n",
    "    if metric == 'icl_score':\n",
    "        data = df[f'loss/7'].values - df[f'loss/{k0}'].values\n",
    "        sns.lineplot(x=df.step, y=data, ax=axes[m], color=ood_colors[0], label=f\"$g=1$\")\n",
    "\n",
    "        for i, (l, g) in enumerate(zip(['a', 'b', 'c'], [3, 5, 10, 100])):\n",
    "            data = (df[f'ood_{l}_inputs_loss/7'].values - df[f'ood_{l}_inputs_loss/{k0}'].values ) / (g)\n",
    "            sns.lineplot(x=df.step, y=data, ax=axes[m], color=ood_colors[i+1], label=f\"$g={g}$\")\n",
    "\n",
    "    elif metric == 'ood_inputs_rel_loss':\n",
    "        for l, g in zip(['a', 'b', 'c', 'd'], [3, 5, 10, 100]):\n",
    "            data = df[f'ood_{l}_inputs_loss/mean'].values / g ** 2 # df[f'loss/mean'].values\n",
    "            sns.lineplot(x=df.step, y=data, ax=axes[m])\n",
    "    else:\n",
    "        for i in range(8):\n",
    "            color = token_cmap.to_rgba(i)\n",
    "            # if i == 0 and metric == \"loss_first_x\":\n",
    "            #     continue\n",
    "\n",
    "            sns.lineplot(data=df, x=\"step\", y=f\"{metric}/{i}\", ax=axes[m], alpha=0.5, color=color, label=f\"$k={i+1}$\" if i % 2 == 0 else \"_\")\n",
    "\n",
    "        if metric.endswith('/token'):\n",
    "            mean_metric = metric[:-6] \n",
    "        else:\n",
    "            mean_metric = f\"{metric}/mean\"\n",
    "            \n",
    "        sns.lineplot(data=df, x=\"step\", y=mean_metric, label=\"Mean\", ax=axes[m], color=BRED)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"\")\n",
    "\n",
    "    # legend = ax.legend()\n",
    "    # legend.remove()\n",
    "    ax.set_xlim(100, 500_000)\n",
    "\n",
    "ax.set_xlabel(\"Step $t$\")\n",
    "# legend = axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=6)\n",
    "# legend.set_title(\"Per-Token Losses\")\n",
    "\n",
    "# Move legend to be likee fig.legend(patch_list, milestone_titles_and_labels, loc='upper center', bbox_to_anchor=(0.5, -0.025), ncol=len(TRANSITIONS))\n",
    "for i, (ax, metric) in enumerate(zip(axes, metrics)):\n",
    "    title, label = titles_and_labels[metric]\n",
    "    # ax.set_ylabel(label)\n",
    "    ax.set_ylabel(\"\")\n",
    "    l = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'][i]\n",
    "    # ax.set_title(f\"({l}) {title}\")\n",
    "    ax.set_ylabel(title)\n",
    "\n",
    "    # if settings.get(metric, {}).get(\"logy\", False):\n",
    "    # ax.set_yscale(\"log\")\n",
    "    ax.set_xscale('log')\n",
    "\n",
    "for ax in [axes[0]]: #, axes[2]]:\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "for ax in axes.flatten()[:-1]:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    # ax.set_ylabel(\"\")\n",
    "\n",
    "# axes[3].set_yscale('linear')\n",
    "# axes[5].set_yscale('linear')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Change right space\n",
    "fig.subplots_adjust(right=0.75) \n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(1.2, .95), ncol=1)\n",
    "axes[1].legend(loc='upper center', bbox_to_anchor=(1.20, 1.), ncol=1, title=\"$x_k\\sim \\mathcal{N}(0, gI_D)$\")\n",
    "\n",
    "_plot_transitions(axes)\n",
    "\n",
    "# Add color bar on the far right\n",
    "\n",
    "fig.set_facecolor('white')\n",
    "fig.savefig(FIGURES / 'lr-behavioral-indicators.pdf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_and_labels = {\n",
    "    \"loss_0\": (\"Average Prediction Norm\", r\"$\\|\\hat y_k\\|^2$\"),\n",
    "    \"loss_first_x\": (\"MSE from Ridge-Optimal 1-sample Prediction\", \"MSE\"),\n",
    "    \"pretrain/delta_ridge/token\": (\"MSE from Ridge Regression\", \"MSE\"),  \n",
    "    \"loss\": (\"Loss\", \"MSE\"),\n",
    "    \"ood_loss\": (\"MSE on Large Inputs\", \"MSE\"),\n",
    "    \"llc/mean\": (\"Per-Token $\\lambda_t$\", \"$\\lambda_t^{(i)}$\"),\n",
    "    \"hessian\": (\"Hessian Statistics\", \"\")\n",
    "}\n",
    "\n",
    "df = evals_over_time_df.loc[evals_over_time_df.model_seed == MODEL_SEED]\n",
    "df['hessian/trace'] = hessian_traces_np\n",
    "\n",
    "metrics = [\"llc/mean\"]# \"loss\", \"ood_loss\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(WIDTH * 1.5, HEIGHT * 1.5))  # Adjust the figsize as needed\n",
    "\n",
    "token_cmap = ScalarMappable(norm=Normalize(vmin=0, vmax=8), cmap=\"viridis\")\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    color = token_cmap.to_rgba(i)\n",
    "\n",
    "    for m, metric in enumerate(metrics):\n",
    "        # if i == 0 and metric == \"loss_first_x\":\n",
    "        #     continue\n",
    "\n",
    "        sns.lineplot(data=df, x=\"step\", y=f\"{metric}/{i}\", ax=axes[m], alpha=1, color=color, label=f\"$k={i+1}$\" if i % 2 == 0 else \"_\")\n",
    "\n",
    "    # if i > 0:\n",
    "    #     sns.lineplot(data=df, x=\"step\", y=f\"loss_first_x/{i}\", ax=axes[2], alpha=0.5, color=color)\n",
    "\n",
    "for m, metric in enumerate(metrics):\n",
    "    if metric.endswith('/token'):\n",
    "        mean_metric = metric[:-6] \n",
    "    else:\n",
    "        mean_metric = f\"{metric}/mean\"\n",
    "        \n",
    "    sns.lineplot(data=df, x=\"step\", y=mean_metric, label=\"Mean\", ax=axes[m], color=BRED)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_ylabel(r\"$\\hat\\lambda_k(w_t)$\")\n",
    "ax = axes[1]\n",
    "\n",
    "sns.lineplot(x=steps, y=top_evals_np[:, 0], color=cmap3[0], label=\"Eigenvalue 1\", ax=ax)\n",
    "sns.lineplot(x=steps, y=top_evals_np[:, 1], color=cmap3[1], label=\"Eigenvalue 2\", ax=ax)\n",
    "sns.lineplot(x=steps, y=top_evals_np[:, 2], color=cmap3[2], label=\"Eigenvalue 3\", ax=ax)\n",
    "sns.lineplot(x=steps, y=hessian_traces_np, ax=ax, color=BRED, label=\"Trace\")\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Step $t$\")\n",
    "\n",
    "    ax.set_xlim(100, 500_000)\n",
    "\n",
    "# legend = axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=6)\n",
    "# legend.set_title(\"Per-Token Losses\")\n",
    "\n",
    "# Move legend to be likee fig.legend(patch_list, milestone_titles_and_labels, loc='upper center', bbox_to_anchor=(0.5, -0.025), ncol=len(TRANSITIONS))\n",
    "for i, (ax, metric) in enumerate(zip(axes, [*metrics, 'hessian'])):\n",
    "    title, label = titles_and_labels[metric]\n",
    "    l = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'][i]\n",
    "    # ax.set_title(f\"({l}) {title}\")\n",
    "    ax.set_ylabel(f\"{title}\")\n",
    "\n",
    "    if settings.get(metric, {}).get(\"logy\", False):\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('')\n",
    "ax.set_xlabel('Step $t$')\n",
    "\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_xticklabels([])\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.75) \n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(1.2, .95), ncol=1)\n",
    "axes[1].legend(loc='upper center', bbox_to_anchor=(1.20, 1.), ncol=1, title=\"Hessian Statistics\")\n",
    "\n",
    "_plot_transitions(axes)\n",
    "\n",
    "\n",
    "# Add color bar on the far right\n",
    "\n",
    "fig.set_facecolor('white')\n",
    "fig.savefig(FIGURES / 'lr-geometric-indicators.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICL Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(FULL_WIDTH, FULL_HEIGHT))\n",
    "\n",
    "icl_score_1 =  -df['loss/0'] + df['loss/3']\n",
    "icl_score_2 =  - df['loss/3'] + df['loss/7']\n",
    "\n",
    "print(icl_score_1)\n",
    "\n",
    "df['icl_score_1'] = icl_score_1\n",
    "df['icl_score_2'] = icl_score_2\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "    sns.lineplot(data=df, x=\"step\", y=\"icl_score_1\", ax=ax)\n",
    "    sns.lineplot(data=df, x=\"step\", y=\"icl_score_2\", ax=ax)\n",
    "    ax.set_ylabel(\"$I_{k:k'}$\")\n",
    "\n",
    "axes[0].set_title(\"(a) ICL Scores\")\n",
    "axes[1].set_title(\"(b) ICL Scores (Zoomed)\")\n",
    "axes[0].set_xlim(100, 500_000)\n",
    "axes[0].set_ylim(-2.8, 0.5)\n",
    "axes[1].set_xlim(1000, 10_000)\n",
    "axes[1].set_ylim(-2, 0.4)\n",
    "\n",
    "# Draw box\n",
    "ylim = axes[0].get_ylim()\n",
    "height = ylim[1] - ylim[0]\n",
    "\n",
    "bottom = (-2 - ylim[0]) / height\n",
    "# print(bottom, bottom * height, ylim)\n",
    "height = (0.4 - ylim[0]) / height\n",
    "\n",
    "xlim = axes[0].get_xlim()\n",
    "width = np.log10(xlim[1]) - np.log10(xlim[0])\n",
    "\n",
    "left = (np.log10(1000) - np.log10(100)) / width\n",
    "right = (np.log10(10000) - np.log10(100)) / width\n",
    "\n",
    "axes[0].axvline(1000, bottom, height, alpha=0.2, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[0].axvline(10000, bottom, height, alpha=0.2, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[0].axhline(-2, left, right, alpha=0.2, color='black', linestyle='--', linewidth=0.5)\n",
    "axes[0].axhline(0.4, left, right, alpha=0.2, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# ylims1 = axes[0].get_ylim()\n",
    "# ylims2 = axes[1].get_ylim()\n",
    "\n",
    "# min_ylim = min(ylims1[0], ylims2[0])\n",
    "# max_ylim = max(ylims1[1], ylims2[1])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Step $t$\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    # ax.set_yscale(\"log\")\n",
    "    ax.legend().remove()\n",
    "    # ax.set_ylim(min_ylim, max_ylim)\n",
    "\n",
    "# handles = axes[0].get_legend_handles_labels()[0]\n",
    "# ax.legend(handles=handles, labels=[f\"$M = 2^{{{m}}}$\" for m in [4, 8, 12, 16, 20]], bbox_to_anchor=(1.05, .9), loc='upper left', borderaxespad=0.)\n",
    "ax.legend(labels=[\"_\", \"$I_{1:4}$\", \"_\", \"$I_{4:8}$\"], bbox_to_anchor=(1.05, .7), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "_plot_transitions(axes)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIGURES / \"icl-scores.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOD Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_losses = []\n",
    "final_model_1 = models1[-1]\n",
    "\n",
    "ws = torch.normal(0, 1, size=(len(xs), D,), device=DEVICE)\n",
    "\n",
    "def _eval_loss(yhats, ys):\n",
    "    return F.mse_loss(yhats, ys).item()\n",
    "\n",
    "\n",
    "for step, model in zip(steps, tqdm.tqdm(models1)):\n",
    "    y_preds = model(xs, ys)\n",
    "    y_norm = y_preds.norm(dim=-1).mean().item()\n",
    "\n",
    "    for multiplier in tqdm.tqdm(np.logspace(-2.5, 4.5, 100, base=10), disable=True):\n",
    "        ood_xs = multiplier * xs\n",
    "\n",
    "        ood_ys = apply_transformations(ws, ood_xs, 0.125, DEVICE)\n",
    "        ood_input_preds = model(ood_xs, ood_ys)\n",
    "        ood_task_preds = model(xs, ood_ys)\n",
    "\n",
    "        # print(ood_input_preds.shape)\n",
    "        ood_input_norm = ood_input_preds.norm(dim=-1).mean().item() / y_norm\n",
    "        ood_task_norm = ood_task_preds.norm(dim=-1).mean().item() / y_norm\n",
    "\n",
    "        ood_input_loss = _eval_loss(ood_input_preds, ood_ys)\n",
    "        ood_task_loss = _eval_loss(ood_task_preds, ood_ys)\n",
    "\n",
    "        ood_losses.append({\n",
    "            \"step\": step,\n",
    "            \"multiplier\": multiplier,\n",
    "            \"ood_input_loss\": ood_input_loss,\n",
    "            \"ood_task_loss\": ood_task_loss,\n",
    "            \"ood_input_loss_div_gain\": ood_input_loss / multiplier, \n",
    "            \"ood_task_loss_div_gain\": ood_task_loss / multiplier,\n",
    "            \"ood_input_loss_to_og\": _eval_loss(ood_input_preds, ys),\n",
    "            \"ood_task_loss_to_og\": _eval_loss(ood_task_preds, ys),\n",
    "            \"ood_input_norm\": ood_input_norm,\n",
    "            \"ood_task_norm\": ood_task_norm,\n",
    "        })\n",
    "\n",
    "ood_losses_df = pd.DataFrame(ood_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_losses_df['ood_input_loss_div_gain'] = ood_losses_df['ood_input_loss'] / ood_losses_df['multiplier'] ** 2\n",
    "ood_losses_df['ood_task_loss_div_gain'] = ood_losses_df['ood_task_loss'] / ood_losses_df['multiplier'] ** 2\n",
    "\n",
    "for m in ['ood_input_loss', 'ood_task_loss', 'ood_input_loss_div_gain', 'ood_task_loss_div_gain', 'ood_input_loss_to_og', 'ood_task_loss_to_og', 'ood_input_norm', 'ood_task_norm']:\n",
    "    ood_losses_df[f\"{m}_log\"] = np.log10(ood_losses_df[m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(FULL_WIDTH, FULL_HEIGHT * 2))\n",
    "\n",
    "sns.lineplot(data=ood_losses_df, x=\"multiplier\", y=\"ood_input_loss\", hue=\"step\", palette='viridis', ax=axes[0, 0], alpha=0.5)\n",
    "sns.lineplot(data=ood_losses_df, x=\"multiplier\", y=\"ood_task_loss\", hue=\"step\", palette='viridis', ax=axes[0, 1], alpha=0.5)\n",
    "sns.lineplot(data=ood_losses_df, x=\"multiplier\", y=\"ood_input_norm\", hue=\"step\", palette='viridis', ax=axes[1, 0], alpha=0.5)\n",
    "sns.lineplot(data=ood_losses_df, x=\"multiplier\", y=\"ood_task_norm\", hue=\"step\", palette='viridis', ax=axes[1, 1], alpha=0.5)\n",
    "\n",
    "for ax in axes[0]:\n",
    "    ax.set_ylabel(\"MSE\")\n",
    "\n",
    "for ax in axes[1]:\n",
    "    ax.set_ylabel(\"Norm\")\n",
    "\n",
    "axes[0, 0].set_title(\"(a) OOD Input Loss\")\n",
    "axes[0, 1].set_title(\"(b) OOD Task Loss\")\n",
    "axes[1, 0].set_title(\"(c) OOD Input Norm Relative to Baseline\")\n",
    "axes[1, 1].set_title(\"(d) OOD Task Norm Relative to Baseline\")\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"OOD Multiplier\")\n",
    "    ax.legend().remove()\n",
    "    ax.axvline(1, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add a colorbar to the right showing steps\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.subplots_adjust(right=0.9)\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.025, 0.7])\n",
    "cbar = fig.colorbar(ScalarMappable(norm=Normalize(vmin=0, vmax=500_000), cmap='viridis'), cax=cbar_ax)\n",
    "cbar.set_label(\"Step $t$\")\n",
    "cbar.set_ticks([0, 100_000, 200_000, 300_000, 400_000, 500_000])\n",
    "\n",
    "fig.savefig(FIGURES / \"lr/ood-performance.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cossims?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.regression.tasks import apply_transformations\n",
    "\n",
    "def compute_eff_weight(model, xs, ws, pos=-1, norm='auto', errors=0.125):\n",
    "    B, K, D = xs.shape\n",
    "\n",
    "    xs_copy = xs.clone().detach()\n",
    "    avg_norm = xs.norm(dim=-1).mean()\n",
    "\n",
    "    if norm == 'auto':\n",
    "        norm = avg_norm\n",
    "\n",
    "    if isinstance(errors, int):\n",
    "        errors = torch.normal(\n",
    "            mean=0.,\n",
    "            std=errors,\n",
    "            size=(B, K, 1,),\n",
    "            device=DEVICE,\n",
    "        )\n",
    "\n",
    "    eff_weights = torch.zeros(B, D, device=DEVICE)\n",
    "\n",
    "    for i in range(D):\n",
    "        xs_copy[:, pos, :] = 0.0\n",
    "        xs_copy[:, pos, i] = norm\n",
    "\n",
    "        ys = xs_copy @ ws.view(B, D, 1) + errors\n",
    "\n",
    "        y_preds = model(xs_copy, ys)\n",
    "        eff_weights[:, i] = y_preds[:, pos, 0]\n",
    "    \n",
    "    return eff_weights\n",
    "\n",
    "num_samples = 256\n",
    "_xs = torch.normal(mean=0, std=1., size=(num_samples, K, D), device=DEVICE)\n",
    "ws = torch.normal(mean=0, std=1., size=(num_samples, 4), device=DEVICE)\n",
    "errors = torch.normal(\n",
    "    mean=0.,\n",
    "    std=0,\n",
    "    size=(num_samples, K, 1,),\n",
    "    device=DEVICE,\n",
    ")\n",
    "_ys = _xs @ ws.view(num_samples, D, 1) + errors\n",
    "first_xs = deepcopy(_xs[:, 0:1, :])\n",
    "first_xs /= torch.norm(first_xs, dim=-1, keepdim=True) ** 2\n",
    "first_xs *= _ys[:, 0:1, :].repeat(1, 1, 4)\n",
    "\n",
    "def prev_x_proj(xs, ys):\n",
    "    prev_xs = deepcopy(xs[:, :, :])\n",
    "    prev_xs /= torch.norm(prev_xs, dim=-1, keepdim=True) ** 2\n",
    "\n",
    "    for i in range(K):   \n",
    "        for j in range(D):\n",
    "            prev_xs[:, i, j] *= ys[:, i, 0]\n",
    "\n",
    "    prev_xs = prev_xs.roll(1, dims=1)\n",
    "    prev_xs[:, 0, :] = 0.\n",
    "\n",
    "    return prev_xs\n",
    "\n",
    "prev_xs = prev_x_proj(_xs, _ys)\n",
    "\n",
    "cumulative_xs = prev_xs.cumsum(dim=1)\n",
    "for i in range(K):\n",
    "    cumulative_xs[:, i, :] /= (i + 1)\n",
    "\n",
    "def compute_eff_weights(_models, steps, xs, ws, errors, ref_ws = None):\n",
    "    if ref_ws is None:\n",
    "        ref_ws = ws\n",
    "\n",
    "    eff_weights_df = []\n",
    "    for step, model in zip(steps, tqdm.tqdm(_models)):\n",
    "        if ref_ws == 'final_w':\n",
    "            ref_ws = compute_eff_weight(model, xs, ws, errors=errors, pos=-1)\n",
    "        for p in range(0, 8):\n",
    "            eff_weights = compute_eff_weight(model, xs, ws, errors=errors, pos=p)\n",
    "\n",
    "            if len(ref_ws.shape) > 2:\n",
    "                _ws = ref_ws[:,p, :]\n",
    "            else:\n",
    "                _ws = ref_ws\n",
    "\n",
    "            # print(_ws.shape, eff_weights.shape)\n",
    "            dot_prods = (_ws * eff_weights).sum(dim=-1)\n",
    "            cossim = dot_prods / (_ws.norm(dim=-1) * eff_weights.norm(dim=-1))\n",
    "            cossim_mean = cossim.mean(dim=0)\n",
    "\n",
    "            relnorm = (eff_weights.norm(dim=-1) / _ws.norm(dim=-1))\n",
    "\n",
    "            eff_weights_df.append({\n",
    "                \"step\": step,\n",
    "                \"cossim\": cossim_mean.item(),\n",
    "                \"cossim_std\": cossim.std(dim=0).item(),\n",
    "                \"relnorm\": relnorm.mean().item(),\n",
    "                \"relnorm_std\": relnorm.std().item(),\n",
    "                \"mse\": (((eff_weights - _ws) ** 2).sum(dim=-1) ** 0.5).mean().item(),\n",
    "                \"position\": p\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(eff_weights_df)\n",
    "\n",
    "eff_weights_df = compute_eff_weights(models1, steps, _xs, ws, errors, ref_ws=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(WIDTH * 1.5, HEIGHT))\n",
    "\n",
    "ax = axes[0]\n",
    "sns.lineplot(data=eff_weights_df, x=\"step\", y=\"cossim\", hue=\"position\", ax=ax, palette='viridis', alpha=0.8)\n",
    "ax.set_ylabel(r\"$\\mathbb{E}[S_C(\\hat{\\mathbf{t}}, \\mathbf{t})]$\")\n",
    "ax.set_title(\"(a) Avg. Cosine Similarity\")\n",
    "\n",
    "ax = axes[1]\n",
    "sns.lineplot(data=eff_weights_df, x=\"step\", y=\"relnorm\", hue=\"position\", ax=ax, palette='viridis', alpha=0.8)\n",
    "ax.set_ylabel(r\"$\\mathbb{E}[\\hat{\\mathbf{t}}/\\mathbf{t}]$\")\n",
    "ax.set_title(\"(b) Avg. Relative Norm\")\n",
    "\n",
    "# axes[1].set_yscale('log')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel(\"Step $t$\")\n",
    "    ax.set_xlim(100, 500_000)\n",
    "    ax.legend().remove()\n",
    "    ax.axvline(4100)\n",
    "\n",
    "_plot_transitions(axes)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES / \"lr/eff-s-gain-w-10.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.figures.plotting import plot_explained_variance\n",
    "\n",
    "def plot_multiple_slices(steps, samples, pca, transitions, highlighted_steps=None, connect_dots=False, palette='tab10', alpha=0.8, save=False, line_color=\"auto\", figsize=(20, 4)):\n",
    "    transition_idxs = get_transition_indices(steps, transitions)\n",
    "    # transition_idxs = [(0 if i != 4 else 1) for i in transition_idxs]\n",
    "\n",
    "    # for i in range(1, 5):\n",
    "    #     transition_idxs[-i] = 10  \n",
    "\n",
    "    if highlighted_steps is None:\n",
    "        highlighted_steps = list(map(get_nearest_step, [t[0] for t in transitions][1:]))\n",
    "\n",
    "    num_pca_components = samples.shape[-1]\n",
    "    \n",
    "    # Create a single row of subplots\n",
    "    num_pca_combos = (num_pca_components * (num_pca_components-1)) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_pca_combos + 1, figsize=figsize)\n",
    "    # fig.suptitle(title)\n",
    "\n",
    "    # Ensure ax is iterable by converting to a list if there's only one subplot\n",
    "    if num_pca_components == 2:\n",
    "        axes = [axes]\n",
    "\n",
    "    I = 0\n",
    "    for i in range(1, num_pca_components):\n",
    "        for j in range(i):\n",
    "\n",
    "            if connect_dots:\n",
    "                axes[I].plot(samples[:, i], samples[:, j], c='black', alpha=0.2)\n",
    "\n",
    "            # sc = axes[I].scatter(samples[:, i], samples[:, j], c=transition_idxs, cmap=cmap, s=50, alpha=alpha)\n",
    "            sns.scatterplot(x=samples[:, i], y=samples[:, j], hue=transition_idxs, palette=palette, s=50, alpha=alpha, ax=axes[I], legend=False)\n",
    "            axes[I].set_xlabel(f'PC {i}')\n",
    "            axes[I].set_ylabel(f'PC {j}')\n",
    "            axes[I].set_title(f'PC {i} vs PC {j}')\n",
    "\n",
    "            # Label some points\n",
    "            total_samples = len(samples)\n",
    "            for step in highlighted_steps:\n",
    "                k = steps.index(step)  # Find the index of the highlighted step\n",
    "                axes[I].text(samples[k, i], samples[k, j], str(step), fontsize=8, ha='right', va='bottom', alpha=0.8)\n",
    "\n",
    "            I += 1\n",
    "\n",
    "    plot_explained_variance(pca, ax=axes[-1], num_pca_components=num_pca_components)\n",
    "    # for I in range( num_pca_combos):\n",
    "    #     axes[I].axis('off')\n",
    "            \n",
    "    # Colorbar for the last plot\n",
    "    # cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Adjust as necessary\n",
    "        # plt.colorbar(sc, cax=cbar_ax, label='Milestones')\n",
    "\n",
    "    cmap = sns.palettes.color_palette(palette, n_colors=len(transitions) + 1)\n",
    "\n",
    "    # Plot the legend on the first subplot on the left\n",
    "    legend_ax = axes[0]\n",
    "    scatter_proxy = [plt.Line2D([0], [0], linestyle='none', marker='o', alpha=alpha, color=cmap[i]) for i in range(len(transitions))]\n",
    "    legend_labels = [label for _, _, label in transitions]\n",
    "    legend_ax.legend(scatter_proxy, legend_labels, loc='center', ncol=1, frameon=False, bbox_to_anchor=(-0.5, 0.5), title='Developmental Stages')\n",
    "    # legend_ax.set_title()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust the right side to make room for the colorbar\n",
    "\n",
    "    if save:\n",
    "        parent_dir = os.path.dirname(save)\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "        plt.savefig(save)\n",
    "\n",
    "\n",
    "    fig.set_facecolor('white')\n",
    "    \n",
    "# Usage of the function\n",
    "# Call the function with your data and the list of highlighted steps\n",
    "# plot_multiple_slices(steps, samples, pca, highlighted_steps=[100, 1000, 10000], title=\"Your Title\", num_points_to_label=10, save=\"path/to/save.png\", connect_dots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "all_weights = []\n",
    "all_gradients = []\n",
    "\n",
    "all_running_grads = []\n",
    "all_running_grads_squared = []\n",
    "\n",
    "def get_weights_vector(model):\n",
    "    return np.concatenate([param.detach().cpu().numpy().flatten() for name, param in model.named_parameters() if param is not None])\n",
    "\n",
    "def get_gradients_vector(model):\n",
    "    return np.concatenate([param.grad.detach().cpu().numpy().flatten() for name, param in model.named_parameters() if param.grad is not None])\n",
    "\n",
    "\n",
    "def get_exp_avg_sq_grads(optimizer_state_dict):\n",
    "    return np.concatenate([g[\"exp_avg_sq\"].cpu().numpy().flatten() for g in optimizer_state_dict[\"state\"].values()])\n",
    "\n",
    "def get_exp_avg_grads(optimizer_state_dict):\n",
    "    return np.concatenate([g[\"exp_avg\"].cpu().numpy().flatten() for g in optimizer_state_dict[\"state\"].values()])\n",
    "\n",
    "\n",
    "for i, (_models, opt_state_dicts) in enumerate(zip(tqdm.tqdm(all_models, desc=\"Getting outputs...\"), all_optimizer_state_dicts)):\n",
    "    # Outputs of token sequence transformer\n",
    "    outputs = []\n",
    "    for path, activations in get_vectorized_activations_trace(_models, xs, ys, 'token_sequence_transformer', normalize=False).items():\n",
    "        outputs.append(activations)\n",
    "\n",
    "    all_outputs.append(np.concatenate(outputs, axis=1))\n",
    "\n",
    "    # Weights & Gradients\n",
    "    gradients = []\n",
    "    weights = []\n",
    "\n",
    "    for model in _models:\n",
    "        model.to(DEVICE)\n",
    "        xs.to(DEVICE)\n",
    "        ys.to(DEVICE)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        ys_pred = model(xs, ys)\n",
    "        loss = F.mse_loss(ys_pred, ys)\n",
    "        loss.backward()\n",
    "\n",
    "        gradients.append(get_gradients_vector(model))\n",
    "        weights.append(get_weights_vector(model))\n",
    "\n",
    "    all_gradients.append(np.array(gradients))\n",
    "    all_weights.append(np.array(weights))\n",
    "\n",
    "    # Optimizer states\n",
    "    running_grads = []\n",
    "    running_grads_squared = []\n",
    "    for opt_state_dict in opt_state_dicts:\n",
    "        running_grads.append(get_exp_avg_grads(opt_state_dict))\n",
    "        running_grads_squared.append(get_exp_avg_sq_grads(opt_state_dict))\n",
    "\n",
    "    all_running_grads.append(np.array(running_grads))\n",
    "    all_running_grads_squared.append(np.array(running_grads_squared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def plot_essential_dynamics_grid(steps, all_samples, transitions, palette='tab10', save=False, figsize=(20, 4), num_pca_components=3, max_step=None, normalize=False, labels=None, max_plot=-1):\n",
    "    num_samples = len(all_samples)  \n",
    "\n",
    "    # Create a single row of subplots\n",
    "    num_pca_combos = (num_pca_components * (num_pca_components-1)) // 2\n",
    "    fig, all_axes = plt.subplots(num_samples, num_pca_combos + 1, figsize=figsize)\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        all_axes = [all_axes]\n",
    "\n",
    "    labels = labels or [f\"Model {i+1}\" for i in range(num_samples)]\n",
    "\n",
    "    for samples_idx, _samples in enumerate(tqdm.tqdm(all_samples, desc=\"Plotting...\")):\n",
    "        if max_step is not None:\n",
    "            max_step_idx = steps.index(max_step)\n",
    "            _samples = _samples[:max_step_idx, :]\n",
    "        if normalize:\n",
    "            _samples = _samples / np.linalg.norm(_samples, axis=1, keepdims=True)\n",
    "\n",
    "        pca = PCA(n_components=num_pca_components)\n",
    "        samples = pca.fit_transform(_samples)\n",
    "\n",
    "        with open(DATA / f\"pca-{samples_idx}.pkl\", \"wb\") as f:\n",
    "            pickle.dump((pca, samples), f)\n",
    "\n",
    "        axes = all_axes[samples_idx]\n",
    "\n",
    "        # Ensure ax is iterable by converting to a list if there's only one subplot\n",
    "        if num_pca_components == 2:\n",
    "            axes = [axes]\n",
    "\n",
    "        # colors = list(reversed(rainbow(len(transitions) + 1)))\n",
    "        if isinstance(palette, str):\n",
    "            colors = sns.palettes.color_palette(palette, n_colors=len(transitions) + 1)\n",
    "        else:\n",
    "            colors = palette\n",
    "\n",
    "        I = 0\n",
    "        for i in range(1, num_pca_components):\n",
    "            for j in range(i):\n",
    "                sns.scatterplot(x=samples[:max_plot, i], y=samples[:max_plot, j], ax=axes[I], alpha=0.5, color=\"gray\", s=10, legend=False)\n",
    "                for k, (start, end, stage) in enumerate(transitions):\n",
    "                    start_idx = steps.index(start)\n",
    "                    end_idx = steps.index(end) + 1\n",
    "\n",
    "                    if max_plot > 0:\n",
    "                        if start > max_plot:\n",
    "                            continue\n",
    "\n",
    "                        end_idx = min(end_idx, max_plot)\n",
    "\n",
    "                    # sc = axes[I].scatter(samples[:, i], samples[:, j], c=transition_idxs, cmap=cmap, s=50, alpha=alpha)\n",
    "                    axes[I].plot(samples[start_idx:end_idx, i], samples[start_idx:end_idx, j], color=colors[k])\n",
    "\n",
    "                if not transitions:\n",
    "                    axes[I].plot(samples[:max_plot, i], samples[:maxplot, j])\n",
    "\n",
    "                axes[I].set_xlabel(f'PC {i+1}')\n",
    "                axes[I].set_ylabel(f'PC {j+1}')\n",
    "                axes[I].set_title(f'PC {j+1} vs PC {i+1}')\n",
    "\n",
    "                I += 1\n",
    "\n",
    "        axes[0].set_ylabel(f\"{labels[samples_idx]}\\n\\nPC 1\")\n",
    "\n",
    "        plot_explained_variance(pca, ax=axes[-1], num_pca_components=num_pca_components)\n",
    "\n",
    "    # cmap = sns.palettes.color_palette(palette, n_colors=len(transitions) + 1)\n",
    "    # Plot the legend on the first subplot on the left\n",
    "    # legend_ax = axes[0]\n",
    "    # scatter_proxy = [plt.Line2D([0], [0], linestyle='none', marker='o', alpha=alpha, color=cmap[i]) for i in range(len(transitions))]\n",
    "    # legend_labels = [label for _, _, label in transitions]\n",
    "    # legend_ax.legend(scatter_proxy, legend_labels, loc='center', ncol=1, frameon=False, bbox_to_anchor=(-0.5, 0.5), title='Developmental Stages')\n",
    "    # legend_ax.set_title()\n",
    "\n",
    "    # plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust the right side to make room for the colorbar\n",
    "    # plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "    if transitions:\n",
    "        # Create an axis for the legend\n",
    "\n",
    "        # Create a list of handles for the legend\n",
    "        handles = [plt.Line2D([0], [0], color=colors[i], linestyle='-') for i in range(len(transitions))]\n",
    "        labels = [label for _, _, label in transitions]\n",
    "\n",
    "        # Add legend to the new axis\n",
    "        fig.legend(handles, labels, loc='center', ncol=len(labels), frameon=False, bbox_to_anchor=(0.5, 0.02))\n",
    "        # Add some space at the bottom for the legend\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout first\n",
    "    plt.subplots_adjust(bottom=0.1, top=0.9)  # Fine-tune spacing, adjust these values as needed\n",
    "\n",
    "    if save:\n",
    "        parent_dir = os.path.dirname(save)\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "        plt.savefig(save)\n",
    "\n",
    "    fig.set_facecolor('white')\n",
    "    return fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model 2\n",
    "FULL_WIDTH = 6.75\n",
    "FULL_HEIGHT = FULL_WIDTH / golden_ratio\n",
    "\n",
    "red =sns.color_palette('tab10', 4)[3]\n",
    "deep_orange = sns.color_palette('tab20c', 5)[4]\n",
    "light_orange = sns.color_palette('tab20', 4)[2]\n",
    "light_orange = decrease_brightness(light_orange, 1.5)\n",
    "light_blue = sns.color_palette('tab20', 4)[1]\n",
    "gray = sns.color_palette('tab20c', 18)[17]\n",
    "gray = decrease_brightness(gray, 0.5)\n",
    "colors = [light_orange, red, light_blue, gray]\n",
    "\n",
    "print(label)\n",
    "fig = plot_essential_dynamics_grid(steps, [all_outputs[MODEL_SEED], all_weights[MODEL_SEED], all_gradients[MODEL_SEED], all_running_grads[MODEL_SEED], all_running_grads_squared[MODEL_SEED]], TRANSITIONS, num_pca_components=4, figsize=(FULL_WIDTH * 2, FULL_HEIGHT * .8), labels=['(a) Behavioral ED', '(b) Weight ED', '(c) Gradient ED', '(d) 1st Moment Gradient ED', '(e) 2nd Moment Gradient ED'],\n",
    "                                   palette=colors)        \n",
    "fig.savefig(FIGURES / 'lr-essential-dynamics.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_essential_dynamics_grid(steps, samples, transitions, save=False, figsize=(20, 4), labels=None):\n",
    "    fig, axes = plt.subplots(1, len(transitions) - 1, figsize=figsize)\n",
    "\n",
    "    colors = sns.color_palette(\"tab10\", n_colors=len(transitions) + 1)\n",
    "\n",
    "    for i, (ax, t1, t2) in enumerate(zip(axes, transitions, transitions[1:])):\n",
    "        t1_start_idx = steps.index(t1[0])\n",
    "        boundary_idx = steps.index(t1[1])\n",
    "        t2_start_idx = steps.index(t2[1])\n",
    "\n",
    "        min_ivl = min(boundary_idx - t1_start_idx, t2_start_idx - boundary_idx)\n",
    "\n",
    "        t1_start_idx = boundary_idx - min_ivl\n",
    "        t2_start_idx = boundary_idx + min_ivl\n",
    "\n",
    "        print(t1_start_idx, boundary_idx, t2_start_idx)\n",
    "        print(steps)\n",
    "\n",
    "        _steps = steps[t1_start_idx:t2_start_idx]\n",
    "        _samples = samples[t1_start_idx:t2_start_idx, :]\n",
    "\n",
    "        pca = PCA(n_components=3)\n",
    "        projected = pca.fit_transform(_samples)\n",
    "\n",
    "        sc = ax.scatter(projected[:, 0], projected[:, 1], c='gray', s=10, alpha=0.2)\n",
    "\n",
    "        ax.plot(projected[:min_ivl+1, 0], projected[:min_ivl+1, 1], color=colors[i])\n",
    "        ax.plot(projected[min_ivl:, 0], projected[min_ivl:, 1], color=colors[i+1])\n",
    "\n",
    "        ax.set_xlabel('PC 1')\n",
    "        ax.set_ylabel('PC 2')\n",
    "\n",
    "        ax.set_title(f\"{t1[2]}-{t2[2]}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_essential_dynamics_grid(steps, all_outputs[1], TRANSITIONS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_essential_dynamics_grid(steps, samples, transitions, save=False, figsize=(20, 4), labels=None):\n",
    "    fig, axes = plt.subplots(1, len(transitions) - 1, figsize=figsize)\n",
    "\n",
    "    colors = sns.color_palette(\"tab10\", n_colors=len(transitions) + 1)\n",
    "    pca = PCA(n_components=3)\n",
    "    projected = pca.fit_transform(samples)\n",
    "\n",
    "    for i, (ax, t1, t2) in enumerate(zip(axes, transitions, transitions[1:])):\n",
    "        t1_start_idx = steps.index(t1[0])\n",
    "        boundary_idx = steps.index(t1[1])\n",
    "        t2_start_idx = steps.index(t2[1])\n",
    "\n",
    "        min_ivl = min(boundary_idx - t1_start_idx, t2_start_idx - boundary_idx)\n",
    "\n",
    "        t1_start_idx = boundary_idx - min_ivl\n",
    "        t2_start_idx = boundary_idx + min_ivl\n",
    "\n",
    "        print(t1_start_idx, boundary_idx, t2_start_idx)\n",
    "        print(steps)\n",
    "\n",
    "        _steps = steps[t1_start_idx:t2_start_idx]\n",
    "        _samples = projected[t1_start_idx:t2_start_idx, :]\n",
    "        sc = ax.scatter(_samples[:, 0], _samples[:, 1], c='gray', s=10, alpha=0.2)\n",
    "\n",
    "        ax.plot(_samples[:min_ivl+1, 0], _samples[:min_ivl+1, 1], color=colors[i])\n",
    "        ax.plot(_samples[min_ivl:, 0], _samples[min_ivl:, 1], color=colors[i+1])\n",
    "\n",
    "        ax.set_xlabel('PC 1')\n",
    "        ax.set_ylabel('PC 2')\n",
    "\n",
    "        ax.set_title(f\"{t1[2]}-{t2[2]}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "plot_essential_dynamics_grid(steps, all_outputs[1], TRANSITIONS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"tab10\")\n",
    "for label, all_samples in zip([\"Outputs\", \"Weights\", \"Gradients\", \"Running Grads\", \"Running Grads Squared\"], [all_outputs, all_weights, all_gradients, all_running_grads, all_running_grads_squared]):\n",
    "    print(label)\n",
    "    plot_essential_dynamics_grid(steps, all_samples, TRANSITIONS, num_pca_components=4, figsize=(16, 4))        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show only first N steps (but fit on all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"tab10\")\n",
    "for label, all_samples in zip([\"Outputs\", \"Weights\", \"Gradients\", \"Running Grads\", \"Running Grads Squared\"], [all_outputs, all_weights, all_gradients, all_running_grads, all_running_grads_squared]):\n",
    "    print(label)\n",
    "    plot_essential_dynamics_grid(steps, all_samples, TRANSITIONS, num_pca_components=4, figsize=(14, 12), max_plot=70)        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First 60k steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 60k steps\n",
    "MAX_STEP = get_nearest_step(TRANSITIONS[4][1])\n",
    "\n",
    "for label, all_samples in zip([\"Outputs\", \"Weights\", \"Gradients\", \"Running Grads\", \"Running Grads Squared\"], [all_outputs, all_weights, all_gradients, all_running_grads, all_running_grads_squared]):\n",
    "    print(label)\n",
    "    plot_essential_dynamics_grid(steps, [all_samples[0]], TRANSITIONS[:5], num_pca_components=4, figsize=(15, 3), max_step=MAX_STEP)        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized\n",
    "\n",
    "for label, all_samples in zip([\"Outputs\", \"Weights\", \"Gradients\", \"Running Grads\", \"Running Grads Squared\"], [all_outputs, all_weights, all_gradients, all_running_grads, all_running_grads_squared]):\n",
    "    print(label)\n",
    "    plot_essential_dynamics_grid(steps, [all_samples[0]], TRANSITIONS, num_pca_components=4, figsize=(15, 3), normalize=True)        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role of checkpoint interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devinfra.utils.iterables import int_linspace\n",
    "\n",
    "linear_steps = int_linspace(0, 500_000, 100)[:-1]\n",
    "linear_step_idxs = [steps.index(get_nearest_step(step)) for step in linear_steps]\n",
    "\n",
    "logarithmic_steps = int_logspace(1, 500_000, 100)\n",
    "logarithmic_step_idxs = [steps.index(get_nearest_step(step)) for step in logarithmic_steps]\n",
    "\n",
    "assert (all([step in steps for step in logarithmic_steps]))\n",
    "\n",
    "num_pca_components = 4\n",
    "transitions = TRANSITIONS\n",
    "    \n",
    "\n",
    "for label, all_samples in zip([\"Outputs\", \"Weights\", \"Gradients\", \"Running Grads\", \"Running Grads Squared\"], [all_outputs, all_weights, all_gradients, all_running_grads, all_running_grads_squared]):\n",
    "    linear_samples0s = [all_samples[0][idx] for idx in linear_step_idxs]\n",
    "    logarithmic_samples0s = [all_samples[0][idx] for idx in logarithmic_step_idxs]\n",
    "    print(label)\n",
    "    all_samples = [all_samples[0], linear_samples0s, logarithmic_samples0s]  \n",
    "    num_samples = len(all_samples)  \n",
    "\n",
    "    # Create a single row of subplots\n",
    "    num_pca_combos = (num_pca_components * (num_pca_components-1)) // 2\n",
    "    fig, all_axes = plt.subplots(num_samples, num_pca_combos + 1, figsize=(15, 9))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        all_axes = [all_axes]\n",
    "\n",
    "    labels = [\"Linear + Logarithmic Interval\", \"Linear Interval\", \"Logarithmic Interval\"]\n",
    "    \n",
    "    pca = PCA(n_components=num_pca_components)\n",
    "    reduced_all_samples0 = pca.fit_transform(all_samples[0])\n",
    "\n",
    "    pca = PCA(n_components=num_pca_components)\n",
    "    pca.fit(linear_samples0s)\n",
    "    reduced_linear_samples0 = pca.transform(all_samples[0])\n",
    "\n",
    "    pca = PCA(n_components=num_pca_components)\n",
    "    pca.fit(logarithmic_samples0s)\n",
    "    reduced_logarithmic_samples0 = pca.transform(all_samples[0])\n",
    "\n",
    "    reduced_all_samples = [reduced_all_samples0, reduced_linear_samples0, reduced_logarithmic_samples0]\n",
    "\n",
    "    for samples_idx, samples in enumerate(tqdm.tqdm(reduced_all_samples, desc=\"Plotting...\")):\n",
    "\n",
    "        axes = all_axes[samples_idx]\n",
    "\n",
    "        # Ensure ax is iterable by converting to a list if there's only one subplot\n",
    "        if num_pca_components == 2:\n",
    "            axes = [axes]\n",
    "\n",
    "        I = 0\n",
    "        for i in range(1, num_pca_components):\n",
    "            for j in range(i):\n",
    "                sns.scatterplot(x=samples[:, i], y=samples[:, j], ax=axes[I], alpha=0.5, color=\"gray\", s=10, legend=False)\n",
    "                for k, (start, end, stage) in enumerate(transitions):\n",
    "                    start_idx = steps.index(start)\n",
    "                    end_idx = steps.index(end) + 1\n",
    "                        \n",
    "                    # sc = axes[I].scatter(samples[:, i], samples[:, j], c=transition_idxs, cmap=cmap, s=50, alpha=alpha)\n",
    "                    axes[I].plot(samples[start_idx:end_idx, i], samples[start_idx:end_idx, j])\n",
    "\n",
    "                axes[I].set_xlabel(f'PC {i+1}')\n",
    "                axes[I].set_ylabel(f'PC {j+1}')\n",
    "                axes[I].set_title(f'PC {j+1} vs PC {i+1}')\n",
    "\n",
    "                I += 1\n",
    "\n",
    "        axes[0].set_ylabel(f\"{labels[samples_idx]}\\n\\nPC 1\")\n",
    "\n",
    "        plot_explained_variance(pca, ax=axes[-1], num_pca_components=num_pca_components)\n",
    "\n",
    "    # cmap = sns.palettes.color_palette(palette, n_colors=len(transitions) + 1)\n",
    "    # Plot the legend on the first subplot on the left\n",
    "    # legend_ax = axes[0]\n",
    "    # scatter_proxy = [plt.Line2D([0], [0], linestyle='none', marker='o', alpha=alpha, color=cmap[i]) for i in range(len(transitions))]\n",
    "    # legend_labels = [label for _, _, label in transitions]\n",
    "    # legend_ax.legend(scatter_proxy, legend_labels, loc='center', ncol=1, frameon=False, bbox_to_anchor=(-0.5, 0.5), title='Developmental Stages')\n",
    "    # legend_ax.set_title()\n",
    "\n",
    "    # plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust the right side to make room for the colorbar\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "    if transitions:\n",
    "        # Create an axis for the legend\n",
    "        legend_ax = fig.add_axes([0.1, -0.03, 0.95, 0.05])  # Adjust these values as needed\n",
    "\n",
    "        # Create a list of handles for the legend\n",
    "        handles = [plt.Line2D([0], [0], color=sns.color_palette('tab10')[i], linestyle='-') for i in range(len(transitions))]\n",
    "        labels = [label for _, _, label in transitions]\n",
    "\n",
    "        # Add legend to the new axis\n",
    "        legend_ax.legend(handles, labels, loc='center', ncol=len(labels), frameon=False)\n",
    "        legend_ax.axis('off')  # Turn off axis lines and labels\n",
    "\n",
    "    fig.set_facecolor('white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(FULL_WIDTH * 4 / 3, FULL_HEIGHT * .5 ))\n",
    "\n",
    "# PCA explained Variance over time\n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "sns.lineplot(data=embed_sing_vals, x=\"step\", y=\"embed/S_normed\", hue=\"index\", palette=\"viridis\", ax=ax)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"(a) Singular Values of Token Embedding\")\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "sns.lineplot(data=postn_sing_vals, x=\"step\", y=\"postn/S_normed\", hue=\"index\", palette=\"viridis\", ax=ax)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(\"(b) Singular Values of Positional Encoding\")\n",
    "\n",
    "for ax in axes[:2]:\n",
    "    ax.set_ylabel(r\"$\\sigma_i^2/\\,\\mathrm{Tr}\\,\\Sigma$\")\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "ax = axes[2]\n",
    "\n",
    "sns.lineplot(data=entangling, x=\"step\", y=\"cossim\", hue=\"index\", palette=\"viridis\", ax=ax)\n",
    "ax.set_xscale('log')\n",
    "ax.set_title(r\"(c) Cossim between $W_{\\mathrm{embed}}$ and $W_{\\mathrm{postn}}$\")\n",
    "ax.set_ylabel(\"$S_C$\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Step $t$\")\n",
    "    ax.legend().remove()\n",
    "    ax.set_xlim(100, 500_000)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.set_facecolor('white')\n",
    "_plot_transitions(axes)\n",
    "\n",
    "fig.savefig(FIGURES / \"embedding.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_HEIGHT = FULL_WIDTH * golden_ratio\n",
    "fig, axes = plt.subplots(2, 3, figsize=(FULL_WIDTH, FULL_HEIGHT))\n",
    "\n",
    "fancy = {\n",
    "    'ln.weight': 'Layer Norm Weights',\n",
    "    'ln.bias': 'Layer Norm Biases',\n",
    "    'linear.weight': 'Linear Weights',\n",
    "    'linear.bias': 'Linear Bias',\n",
    "}\n",
    "\n",
    "for i, layer in enumerate([\"ln\", \"linear\"]):\n",
    "    for j, layer_subset in enumerate([\"weight\", \"bias\"]):\n",
    "        data_subset = unembeddings.loc[unembeddings.layer == f\"{layer}.{layer_subset}\"]\n",
    "        means = data_subset.groupby('step').mean()\n",
    "        stds = data_subset.groupby('step').std()\n",
    "\n",
    "        if i + j != 2:\n",
    "            sns.lineplot(data=means, x=\"step\", y='p', color=BRED, ax=axes[j, i])\n",
    "            sns.lineplot(data=data_subset, x=\"step\", y=\"p\", hue=\"i\", palette=\"viridis\", ax=axes[j, i], alpha=0.8, linewidth=0.5)\n",
    "            sns.lineplot(data=means, x=\"step\", y='p', color=BRED, ax=axes[j, i])\n",
    "            axes[j, i].fill_between(means.index, means.p - stds.p, means.p + stds.p, alpha=0.25, color=BRED)\n",
    "            axes[j, i].legend().remove()\n",
    "            # handles = [axes[j, i].get_legend_handles_labels()[0][0]]\n",
    "            # axes[j, i].legend(labels=['Mean'], handles=handles, loc='upper left', frameon=False)\n",
    "\n",
    "        else:\n",
    "            sns.lineplot(data=data_subset, x=\"step\", y=\"p\", hue=\"i\", ax=axes[j, i], color=BRED)\n",
    "            axes[j, i].legend().remove()   \n",
    "\n",
    "        axes[j, i].set_xscale('log')\n",
    "        axes[j, i].set_ylabel(fancy[f\"{layer}.{layer_subset}\"])\n",
    "        axes[j, i].set_xlim(200, 500000)\n",
    "        axes[j, i].set_xlabel(\"Training step $t$\")\n",
    "        # axes[j, i].set_ylabel(\"Weight\")\n",
    "\n",
    "\n",
    "axes[0,0].hlines(0, 0, 500_000, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# fig.suptitle(\"Unembedding Weights over Time\", fontsize=14)\n",
    "\n",
    "axes[1,1].legend().remove()\n",
    "plt.tight_layout()\n",
    "\n",
    "ax = axes[0, 2]\n",
    "\n",
    "means = reduced_unembeddings.loc[reduced_unembeddings.subset == \"weight\"].groupby('step').mean()[\"p\"]\n",
    "stds = reduced_unembeddings.loc[reduced_unembeddings.subset == \"weight\"].groupby('step').std()[\"p\"]\n",
    "\n",
    "sns.lineplot(data=reduced_unembeddings.loc[reduced_unembeddings.subset == \"weight\"], x=\"step\", y=means, ax=ax, color=BRED, label=\"Mean\")\n",
    "sns.lineplot(data=reduced_unembeddings.loc[reduced_unembeddings.subset == \"weight\"], x=\"step\", y=\"p\", hue=\"i\", palette=\"viridis\", ax=ax, alpha=0.8, linewidth=0.5)\n",
    "sns.lineplot(data=reduced_unembeddings.loc[reduced_unembeddings.subset == \"weight\"], x=\"step\", y=means, ax=ax, color=BRED)\n",
    "ax.fill_between(means.index, means - stds, means + stds, alpha=0.25, color=BRED)\n",
    "ax.set_title(f\"{layer}.{layer_subset}\")\n",
    "\n",
    "ax = axes[1, 2]\n",
    "sns.lineplot(data=reduced_unembeddings.loc[reduced_unembeddings.subset == \"bias\"], x=\"step\", y=\"p\", hue=\"i\", ax=ax, color=BRED)\n",
    "\n",
    "for ax, subset in zip([axes[0, 2], axes[1, 2]], [\"Effective Unembedding Weights\", \"Effective Unembedding Bias\"]):\n",
    "    ax.legend().remove()\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_ylabel(f\"{subset}\")\n",
    "    ax.set_xlabel(\"Training step $t$\")\n",
    "    # ax.set_ylabel(\"Weight\")\n",
    "    ax.set_title('')\n",
    "    ax.set_xlim(200, 500000)\n",
    "\n",
    "_plot_transitions(axes)\n",
    "\n",
    "fig.savefig(FIGURES / \"lr-unembed.pdf\")\n",
    "# handles = [ax.get_legend_handles_labels()[0][0] for ax in axes]\n",
    "# axes[0, 2].legend(labels=['Mean'], handles=handles, loc='upper left', frameon=False)\n",
    "# ax.get_legend_handles_labels();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = WIDTH * golden_ratio\n",
    "fig, axes = plt.subplots(2, 1, figsize=(WIDTH, HEIGHT * 1.5))\n",
    "\n",
    "# Fill between using the stdp\n",
    "colors = sns.color_palette(\"viridis\", 5)\n",
    "\n",
    "for i, type_ in enumerate([\"weight\", \"bias\"]):\n",
    "    sns.lineplot(data=ln_stats, x=\"step\", y=f\"{type_}/mean\", hue=\"layer\", palette=\"viridis\", ax=axes[i], alpha=0.8)\n",
    "\n",
    "    for l, layer in enumerate(ln_stats.layer.unique()):\n",
    "        layer_ln_stats = ln_stats.loc[ln_stats.layer == layer]\n",
    "\n",
    "        axes[i].fill_between(steps, layer_ln_stats[f\"{type_}/mean\"] - layer_ln_stats[f\"{type_}/std\"], layer_ln_stats[f\"{type_}/mean\"] + layer_ln_stats[f\"{type_}/std\"], alpha=0.1, color=colors[l])\n",
    "\n",
    "for ax in axes.flatten():\n",
    "    plot_transitions(ax, TRANSITIONS, colors=LR_TRANSITION_COLORS)\n",
    "    ax.set_xscale('log')\n",
    "    # ax.set_yscale('log')\n",
    "    ax.set_xlim(100, 500_000)\n",
    "    ax.legend().remove()\n",
    "\n",
    "layers = [\"Attn. 1\", '_', \"MLP 1\",  '_', \"Attn. 2\", '_', \"MLP 2\", '_', \"Unembed\"]\n",
    "axes[0].set_title(r\"(a) LN Weights $\\gamma$\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_xticklabels([])\n",
    "axes[1].set_title(r\"(b) LN Biases $\\beta$\")\n",
    "\n",
    "axes[0].set_ylabel(r'$\\mathbb{E}[\\gamma^{(l)}_i]$')\n",
    "axes[1].set_ylabel(r'$\\mathbb{E}[\\beta^{(l)}_i]$')\n",
    "# axes[1].legend(layers, title=\"\", loc='lower left', bbox_to_anchor=(-0.25, -.5), frameon=False, ncols=5)\n",
    "axes[1].legend(layers, title=\"Layer\", loc='center', bbox_to_anchor=(1.3, 1.05), frameon=False, ncols=1)\n",
    "ax.set_xlabel('Step $t$')\n",
    "\n",
    "# plt.tight_layout(pad=0.1)\n",
    "fig.set_facecolor('white')\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "fig.savefig(FIGURES / \"lr-ln.pdf\")\n",
    "# Increase space between subplots to fit legend\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lns = {\n",
    "    \"Unembedding\": unembedding_lns,\n",
    "    \"Block 1 Pre-Attention\": block_1_attn_lns,\n",
    "    \"Block 1 Pre-MLP\": block_1_mlp_lns,\n",
    "    \"Block 2 Pre-Attention\": block_2_attn_lns,\n",
    "    \"Block 2 Pre-MLP\": block_2_mlp_lns,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, len(all_lns), figsize=(FULL_WIDTH * 2, FULL_HEIGHT ))\n",
    "\n",
    "def frac_eff_zero(w):\n",
    "    return (w.abs() < 1e-1).float().mean().detach().cpu().numpy()\n",
    "\n",
    "insets = []\n",
    "\n",
    "for i, (name, lns) in enumerate(list(all_lns.items())[1:] + list(all_lns.items())[:1]):\n",
    "    axes[0, i].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    sns.set_palette(sns.color_palette(\"viridis\", 64))\n",
    "    \n",
    "    axes[0, i].plot(steps, np.array([w.detach().cpu().numpy() for w, b in lns]), alpha=0.8, linewidth=0.5)\n",
    "    axes[1, i].plot(steps, np.array([b.detach().cpu().numpy() for w, b in lns]), alpha=0.8, linewidth=0.5)\n",
    "\n",
    "    w_means = np.array([w.detach().cpu().numpy() for w, b in lns]).mean(axis=-1)\n",
    "    b_means = np.array([b.detach().cpu().numpy() for w, b in lns]).mean(axis=-1)\n",
    "    w_stds = np.array([w.detach().cpu().numpy() for w, b in lns]).std(axis=-1)\n",
    "    b_stds = np.array([b.detach().cpu().numpy() for w, b in lns]).std(axis=-1)\n",
    "\n",
    "    axes[0, i].plot(steps, w_means, color=BRED)\n",
    "    axes[0, i].fill_between(steps, w_means - w_stds, w_means + w_stds, alpha=0.25, color=BRED)\n",
    "    axes[1, i].plot(steps, b_means, color=BRED)\n",
    "    axes[1, i].fill_between(steps, b_means - b_stds, b_means + b_stds, alpha=0.25, color=BRED)\n",
    "\n",
    "    axes[0, i].set_title(f\"{name} LN Weight\")\n",
    "    axes[1, i].set_title(f\"{name} LN Bias\")\n",
    "    \n",
    "    # axes[0, i].legend(labels=['Mean'], handles=handles, loc='upper left', frameon=False)\n",
    "    # axes[1, i].legend(labels=['Mean'], handles=handles, loc='upper left', frameon=False)\n",
    "    # axes[0, i].legend().remove()\n",
    "    # axes[1, i].legend().remove()\n",
    "\n",
    "    axinset0 = axes[0, i].inset_axes([0.10, 0.1, 0.3, 0.3])\n",
    "    axinset1 = axes[1, i].inset_axes([0.10, 0.1, 0.3, 0.3])\n",
    "\n",
    "    frac_w_eff_zero = np.array([frac_eff_zero(w) for w, b in lns])\n",
    "    frac_b_eff_zero = np.array([frac_eff_zero(b) for w, b in lns])\n",
    "    \n",
    "    axinset0.plot(steps, frac_w_eff_zero, color=BRED)\n",
    "    axinset1.plot(steps, frac_b_eff_zero, color=BRED)\n",
    "\n",
    "    insets.extend([axinset0, axinset1])\n",
    "    \n",
    "\n",
    "plot_transitions(axes, TRANSITIONS, colors=LR_TRANSITION_COLORS)\n",
    "plot_transitions(insets, TRANSITIONS, colors=LR_TRANSITION_COLORS)\n",
    "plt.tight_layout()\n",
    "\n",
    "for ax in [*axes.flatten(), *insets]:\n",
    "    ax.set_xscale('log')\n",
    "    # ax.set_yscale('symlog')\n",
    "    ax.set_xlim(100, 500000)\n",
    "\n",
    "for ax in insets:\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(\"% < 0.1\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([])  # Remove x-ticks\n",
    "    ax.set_xticklabels([])  # Remove x-tick labels\n",
    "    ax.grid(False)\n",
    "\n",
    "fig.savefig(FIGURES / \"lr-ln-all.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_patterns(df: pd.DataFrame, num_blocks: int, num_heads: int, num_tokens: int, title=None, save: Optional[str] = None, figsize=(20, 25), logx=False, logy=False, metric=\"mean\", label=\"Entropy\", full_block=True, full_head=True, y_axis=True):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(label + \"\\n\" + title)\n",
    "\n",
    "    num_cols = num_blocks * 2\n",
    "    num_rows = int(full_block) + int(full_head) + num_heads\n",
    "\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    axes = []\n",
    "\n",
    "    if full_block:\n",
    "        # Create subplot for mean entropy of first two blocks\n",
    "        ax0 = plt.subplot2grid((num_rows, num_cols), (0, 0), colspan=num_cols)\n",
    "        block_cmap = sns.color_palette(\"viridis\", num_blocks * 2)\n",
    "\n",
    "        for b in range(num_blocks):\n",
    "            for x_or_y in (1, 0):\n",
    "                ax0.plot(df.step, df[f\"block_{b}/{'x' if not x_or_y else 'y'}/{metric}\"], label=f\"Block {b + 1} {'xs' if not x_or_y else 'ys'}\", color=block_cmap[b+1-x_or_y])\n",
    "\n",
    "        ax0.set_title(\"Blocks\")\n",
    "        ax0.set_xlabel(\"Step, $t$\")\n",
    "        ax0.set_ylabel(label)\n",
    "        ax0.legend()\n",
    "\n",
    "        axes.append(ax0)\n",
    "\n",
    "    if full_head:\n",
    "        # Create subplots for each block, showing entropy in different heads\n",
    "        ax1 = [plt.subplot2grid((num_rows, num_cols), (int(full_block), i), colspan=1) for i in range(num_blocks * 2)]\n",
    "        head_cmap = sns.color_palette(\"viridis\", num_heads)\n",
    "            \n",
    "        for b in range(num_blocks):\n",
    "            for x_or_y in (1, 0):\n",
    "                _ax1 = ax1[2 * b + 1-x_or_y]\n",
    "                _ax1.set_title(f\"Block {b + 1} {'xs' if not x_or_y else 'ys'}\")\n",
    "                # _ax1.set_xlabel(\"Step, $t$\")\n",
    "                # _ax1.set_ylabel(label)\n",
    "                \n",
    "                for h in range(num_heads):\n",
    "                    series = df[f\"block_{b}/head_{h}/{'x' if not x_or_y else 'y'}/{metric}\"]\n",
    "                    _ax1.plot(df.step, series, label=f\"Head {h + 1}\", color=head_cmap[h])\n",
    "\n",
    "                _ax1.set_xticks([])\n",
    "                _ax1.set_xticklabels([])\n",
    "                \n",
    "        ax1[0].legend()\n",
    "        ax1[0].set_ylabel(label)\n",
    "\n",
    "        axes.extend(ax1)\n",
    "\n",
    "    # Create subplots for each head in each block, detailing entropy for each token\n",
    "    ax2 = [plt.subplot2grid((num_rows, num_cols), (i//(num_cols) + int(full_block) + int(full_head), i%(num_cols))) for i in range(num_heads * num_blocks * 2)]\n",
    "    ax_idx = 0\n",
    "\n",
    "    for b in range(num_blocks):\n",
    "        for x_or_y in (0, 1):\n",
    "            ax2[ax_idx].set_ylabel(label)\n",
    "            token_cmap = sns.color_palette(\"viridis\" if not x_or_y else \"magma\", num_tokens)\n",
    "            for h in range(num_heads):\n",
    "                _ax2 = ax2[ax_idx]\n",
    "                _ax2.set_title(f\"$b={b + 1}, h={h + 1}, {'x' if not x_or_y else 'y'}_k$\")\n",
    "                # ax2[ax_idx].set_xlabel(\"Step, $t$\")\n",
    "                # ax2[ax_idx].set_ylabel(label)\n",
    "\n",
    "                for t in range(int(x_or_y), num_tokens, 2):\n",
    "                    series = df[f\"block_{b}/head_{h}/token_{t}/{metric}\"]\n",
    "\n",
    "                    if y_axis:\n",
    "                        _ax2.axhline(0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "                    _ax2.plot(df.step, series, label=f\"Token {'x' if not x_or_y else 'y'} {t + 1}\", color=token_cmap[t])\n",
    "                    \n",
    "                _ax2.set_xticks([])\n",
    "                _ax2.set_xticklabels([])\n",
    "                ax_idx += 1\n",
    "\n",
    "    for ax in ax2[-4:]:\n",
    "        ax.set_xlabel(\"Step $t$\")\n",
    "        ax.set_xticks([10**i for i in range(6)]) + [500_000]\n",
    "        ax.set_xticklabels([10**i for i in range(6)]) + [500_000]\n",
    "\n",
    "    axes.extend(ax2)\n",
    "    for ax in axes:\n",
    "        if logx:\n",
    "            ax.set_xscale(\"log\")\n",
    "        if logy:\n",
    "            ax.set_yscale(\"log\")\n",
    "\n",
    "        ax.set_xlim(100, 500_000)\n",
    "\n",
    "    _plot_transitions(axes)\n",
    "\n",
    "\n",
    "    # ax2[0].legend()\n",
    "    # ax2[1].legend()\n",
    "    viridis = sns.color_palette(\"viridis\", 8)\n",
    "    magma = sns.color_palette(\"magma\", 8)\n",
    "    colors = [(viridis[c] if not x_or_y else magma[c]) for c in range(8) for x_or_y in (0, 1)]\n",
    "    labels = [f\"${'x' if not x_or_y else 'y'}_{i + 1}$\" for i in range(8) for x_or_y in (0, 1)]\n",
    "\n",
    "    # Handles (patches)\n",
    "    # handles = [mpatches.Line(color=colors[i], label=labels[i * 2 + x_or_y]) for i in range(8) for x_or_y in (0, 1)]\n",
    "    # Handles (lines)\n",
    "    handles = [plt.Line2D([0,0],[0,0], color=colors[i], label=labels[i]) for i in range(16)]\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.subplots_adjust(bottom=0.2)      \n",
    "    fig.legend(handles=handles, loc='lower center', ncol=8, frameon=False, bbox_to_anchor=(0.5, 0.01))\n",
    "\n",
    "    if save:\n",
    "        parent_dir = os.path.dirname(save)\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "        plt.savefig(save)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (title, metric) in enumerate(zip((\"$\\hat{H}_{k}^{(b, h)}$\", \"$V_{k}^{(b, h)}$\", \"$D_{k}^{(b, h)}$\", \"Self attention\", \"Previous token attention\", \"X tokens attention\", \"Hardness\"), (\"entropy_normalized\", \"variability\", \"distance\", \"self_attn\", \"prev_token_attn\", \"x_tokens_attn\", \"hardness\"))):\n",
    "    fig, axes = plot_attention_patterns(\n",
    "        attn_variabilities, \n",
    "        num_blocks=num_blocks, \n",
    "        num_heads=num_heads, \n",
    "        num_tokens=num_tokens-1, \n",
    "        save=False,\n",
    "        figsize=(FULL_WIDTH, 2.5 * FULL_HEIGHT),\n",
    "        logx=True,\n",
    "        metric=metric,\n",
    "        title=None, # run.config.to_latex(), \n",
    "        label=title,\n",
    "        full_block = False,\n",
    "        full_head = False,\n",
    "    )\n",
    "    for ax in axes:\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_ylim(10e-4, 1)\n",
    "    fig.savefig(FIGURES / (f\"lr/lr-attn-{metric}\" + \".pdf\"))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "metrics_to_labels = {\n",
    "    \"first_x\": r\"\\alpha_{k, 1}^{(b, h)}\",\n",
    "    \"first_y\": r\"\\alpha_{k, 2}^{(b, h)}\",\n",
    "    \"prev_token_attn\": r\"\\alpha_{k, k-1}^{(b, h)}\",\n",
    "    \"previous_x\": r\"\\alpha_{k, y_{k-1}}^{(b, ha)}\",\n",
    "    \"previous_y\": r\"\\alpha_{k, x_{k-1}}^{(b, h)}\",\n",
    "    \"x_tokens_attn\": r\"\\alpha_{k, x}^{(b, h)}\",\n",
    "    \"y_tokens_attn\": r\"\\alpha_{k, y}^{(b, h)}\",\n",
    "    \"self_attn\": r\"\\alpha_{k, k}^{(b, h)}\",\n",
    "}\n",
    "\n",
    "head_metrics = {\n",
    "    # \"b1h1x\": \"?\",\n",
    "    \"b1h1y\": \"x_tokens_attn\", # It's attending to x. For first y 100% on previous token. For others 0% on previous token but 100% on x\n",
    "    # \"b1h2x\": \"?\",\n",
    "    # \"b1h2y\": \"?\",\n",
    "    \"b1h3x\": \"y_tokens_attn\", # Maybe close to uniform?\n",
    "    \"b1h3y\": \"prev_token_attn\",\n",
    "    # \"b1h4x\": \"?\",\n",
    "    \"b1h4y\": \"y_tokens_attn\", # Almost a particular one?\n",
    "    # \"b2h1x\": \"?\",\n",
    "    # \"b2h1y\": \"?\",\n",
    "    # \"b2h2x\": \"?\",\n",
    "    # \"b2h2y\": \"?\",\n",
    "    # \"b2h3x\": \"?\",\n",
    "    \"b2h3y\": \"self_attn\", # relaxes in R3\n",
    "    # \"b2h4x\": \"?\",\n",
    "    # \"b2h4y\": \"?\",\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, len(head_metrics), figsize=(FULL_WIDTH * 1.5, FULL_HEIGHT))\n",
    "\n",
    "df = attn_variabilities\n",
    "colors = sns.color_palette(\"viridis\", 8)\n",
    "\n",
    "for i, ((id_, metric), ax) in enumerate(zip(head_metrics.items(), axes)):\n",
    "\n",
    "    try:\n",
    "        x_or_y = 1 if id_[4] == \"y\" else 0\n",
    "        for token in range(x_or_y, 15, 2):\n",
    "            ax.plot(df.step, df[f\"block_{int(id_[1])-1}/head_{int(id_[3])-1}/token_{token}/{metric}\"], color=colors[token])\n",
    "\n",
    "    except:\n",
    "        warnings.warn(f\"Failed to plot {id_} {metric}\")\n",
    "        \n",
    "    ax.set_title(f\"$b={id_[1]}, h={id_[3:5]}$\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlim(100, 500_000)\n",
    "    ax.set_xlabel(\"Step $t$\")\n",
    "    ax.set_ylabel(f\"${metrics_to_labels[metric]}$\")\n",
    "\n",
    "_plot_transitions(axes)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(FIGURES / \"lr/lr-attn-ids.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
