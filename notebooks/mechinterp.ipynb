{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanistic Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find induction heads where we know they exist. \n",
    "\n",
    "Take a GPT-2 or LLama model, visualize the attention patterns, and find the heads that seem to be doing induction. Additionally, see if you can automatically rank the heads by how much they attend to previous tokens like induction heads on synthetic samples `A B X_1 ... X_N A`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply the same two techniques to the ICL pretrained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/Projects/icl/.venv/lib/python3.9/site-packages/pydantic/_internal/_fields.py:127: UserWarning: Field \"model_seed\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from icl.analysis.utils import get_unique_run\n",
    "from devinterp.mechinterp.activations import ActivationProbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/Projects/devinfra/devinfra/utils/iterables.py:29: UserWarning: Number of steps in int_logspace is not 100, got 91.\n",
      "  warnings.warn(\n",
      "/Users/Jesse/Projects/devinfra/devinfra/utils/iterables.py:29: UserWarning: Number of steps in int_logspace is not 50, got 47.\n",
      "  warnings.warn(\n",
      "/Users/Jesse/Projects/icl/icl/baselines.py:165: UserWarning: The operator 'aten::_linalg_solve_ex.result' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  ws_hat = torch.linalg.solve(LHS, RHS)   # BKDD^-1 @ BKD1 -> B K D 1\n"
     ]
    }
   ],
   "source": [
    "run = get_unique_run(\n",
    "    \"../sweeps/small-sweep.yaml\", \n",
    "    task_config={\"num_tasks\": 1, \"num_layers\": 2},\n",
    "    optimizer_config={\"lr\": 0.001}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that we've loaded in the most recent model\n",
    "run.evaluator(run.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['token_sequence_transformer.token_embedding.weight',\n",
       " 'token_sequence_transformer.postn_embedding.weight',\n",
       " 'token_sequence_transformer.blocks.0.attention.causal_mask',\n",
       " 'token_sequence_transformer.blocks.0.attention.attention.weight',\n",
       " 'token_sequence_transformer.blocks.0.compute.0.weight',\n",
       " 'token_sequence_transformer.blocks.0.compute.0.bias',\n",
       " 'token_sequence_transformer.blocks.0.compute.2.weight',\n",
       " 'token_sequence_transformer.blocks.0.compute.2.bias',\n",
       " 'token_sequence_transformer.blocks.0.layer_norms.0.weight',\n",
       " 'token_sequence_transformer.blocks.0.layer_norms.0.bias',\n",
       " 'token_sequence_transformer.blocks.0.layer_norms.1.weight',\n",
       " 'token_sequence_transformer.blocks.0.layer_norms.1.bias',\n",
       " 'token_sequence_transformer.blocks.1.attention.causal_mask',\n",
       " 'token_sequence_transformer.blocks.1.attention.attention.weight',\n",
       " 'token_sequence_transformer.blocks.1.compute.0.weight',\n",
       " 'token_sequence_transformer.blocks.1.compute.0.bias',\n",
       " 'token_sequence_transformer.blocks.1.compute.2.weight',\n",
       " 'token_sequence_transformer.blocks.1.compute.2.bias',\n",
       " 'token_sequence_transformer.blocks.1.layer_norms.0.weight',\n",
       " 'token_sequence_transformer.blocks.1.layer_norms.0.bias',\n",
       " 'token_sequence_transformer.blocks.1.layer_norms.1.weight',\n",
       " 'token_sequence_transformer.blocks.1.layer_norms.1.bias',\n",
       " 'token_sequence_transformer.unembedding.0.weight',\n",
       " 'token_sequence_transformer.unembedding.0.bias',\n",
       " 'token_sequence_transformer.unembedding.1.weight',\n",
       " 'token_sequence_transformer.unembedding.1.bias']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(run.model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "figures = Path(\"../figures/M=1/across-x-basis-w=0\")\n",
    "\n",
    "output, activations_ = hooked_model.run_with_cache(x_trick, y_trick)\n",
    "\n",
    "def separate_attention(qkv, num_heads: int, batch_size: int, head_size: int, num_tokens: int):\n",
    "    return (qkv    # B T C @ C 3C  -> B T 3C\n",
    "        .view(batch_size, num_tokens, num_heads, 3*head_size)     #               -> B T H 3c\n",
    "        .transpose(-2, -3)      #               -> B H T 3c\n",
    "        .split(head_size, dim=-1)       #               -> (B H T c) * 3\n",
    "    )\n",
    "\n",
    "E = 64\n",
    "T = 16\n",
    "H = 4\n",
    "B = 4\n",
    "\n",
    "# separate_attention(activations_, num_heads=4, batch_size=4, head_size=64//4, num_tokens=16)\n",
    "\n",
    "def optionally_rotate(x, name,):\n",
    "    if len(x.shape) != 2:\n",
    "        raise ValueError(\"Tensor should have two dimensions.\")\n",
    "\n",
    "    if x.shape[0] > x.shape[1]:\n",
    "        return x.T, f\"{name}.T\"\n",
    "    \n",
    "    return x, name\n",
    "\n",
    "activations = {}\n",
    "activations[\"x\"] = x_trick\n",
    "activations[\"y\"] = y_trick\n",
    "activations.update(activations_)\n",
    "\n",
    "for i in range(4):\n",
    "    if not os.path.exists(figures / f\"{i}\"):\n",
    "        os.makedirs(figures / f\"{i}\")\n",
    "\n",
    "    for location, v in activations.items():\n",
    "        activation_slice = v[i] # Batch idx\n",
    "\n",
    "        if location.endswith(\"attention.attention\"):\n",
    "            q, k, v = separate_attention(v, num_heads=H, batch_size=B, head_size=E//H, num_tokens=T)\n",
    "            qk = q @ k.transpose(-2, -1)\n",
    "            q, k, qk, v = q[i], k[i], v[i], qk[i]\n",
    "            \n",
    "            fig, axs = plt.subplots(H, 4, figsize=(15, 15))\n",
    "\n",
    "            for j, (name, x) in enumerate(zip([\"Q\", \"K\", \"QK\", \"V\"], [q, k, qk, v])):\n",
    "                for h in range(H):\n",
    "                    ax = axs[h, j]\n",
    "                    ax.matshow(x[h].detach().to(\"cpu\").numpy())\n",
    "                    ax.set_title(f\"{h}.{name}\")\n",
    "\n",
    "            plt.suptitle(f\"{location}\")\n",
    "            plt.savefig(figures / f\"{i}/{location}.png\")\n",
    "            plt.show()\n",
    "        elif len(activation_slice.shape) == 2:\n",
    "            x, name = optionally_rotate(activation_slice, location)\n",
    "            plt.matshow(x.detach().to(\"cpu\").numpy())\n",
    "            plt.title(f\"{i} {name}\")\n",
    "            plt.savefig(figures / f\"{i}/{name}.png\")\n",
    "            plt.show()\n",
    "        elif len(activation_slice.shape) == 3:  # [heads, xs, ys]\n",
    "            heads, xs, ys = activation_slice.shape\n",
    "            fig, axs = plt.subplots(1, heads, figsize=(15, 15))\n",
    "            for j in range(heads):\n",
    "                ax = axs[j]\n",
    "                x, name = optionally_rotate(activation_slice[j], str(j))\n",
    "                ax.matshow(x.detach().to(\"cpu\").numpy())\n",
    "                ax.set_title(name)\n",
    "            plt.suptitle(f\"{location}.#\")\n",
    "            plt.savefig(figures / f\"{i}/{location}.png\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported number of dimensions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# List of folder paths containing images\n",
    "folder_paths = [figures / f\"{i}\" for i in range(4)]  # Add more folders as needed\n",
    "\n",
    "if not os.path.exists(figures / \"overview\"):\n",
    "    os.makedirs(figures/\"overview\")\n",
    "\n",
    "# Create a dictionary to store images by filename\n",
    "images_by_filename = {}\n",
    "\n",
    "# Load images from each folder and organize them by filename\n",
    "for folder_path in folder_paths:\n",
    "    filenames = [f for f in os.listdir(folder_path) if f.endswith('.png')]  # Change the extension as needed\n",
    "    for filename in filenames:\n",
    "        img = Image.open(os.path.join(folder_path, filename))\n",
    "        if filename in images_by_filename:\n",
    "            images_by_filename[filename].append(img)\n",
    "        else:\n",
    "            images_by_filename[filename] = [img]\n",
    "\n",
    "print(images_by_filename)\n",
    "\n",
    "# Create comparison images for each unique filename\n",
    "for filename, image_list in images_by_filename.items():\n",
    "    # Calculate the width and height of the result image\n",
    "    width = sum(img.width for img in image_list)\n",
    "    height = max(img.height for img in image_list)\n",
    "\n",
    "    # Create a new image for the comparison\n",
    "    result_image = Image.new('RGB', (width, height))\n",
    "\n",
    "    # Paste images side by side\n",
    "    x_offset = 0\n",
    "    for img in image_list:\n",
    "        result_image.paste(img, (x_offset, 0))\n",
    "        x_offset += img.width\n",
    "\n",
    "    # Display or save the result image\n",
    "    result_image.save(figures / f\"overview/{filename}\")  # You can replace this with result_image.save() to save the comparison images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import operator\n",
    "from typing import Callable, List\n",
    "\n",
    "def patch(module: nn.Module):\n",
    "    if isinstance(module, nn.ModuleList):\n",
    "        return PatchedList(module)\n",
    "    elif isinstance(module, nn.Sequential):\n",
    "        return PatchedSequential(module)\n",
    "    else:\n",
    "        return Patched(module)\n",
    "\n",
    "\n",
    "class Patched(nn.Module):\n",
    "    def __init__(self, module: nn.Module):\n",
    "        super().__init__()\n",
    "        self.__dict__[\"_current\"] = module\n",
    "        self.__dict__[\"_original\"] = module\n",
    "\n",
    "        for n, c in self._current.named_children():\n",
    "            print(n, c)\n",
    "            self.add_module(n, patch(c))\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self._current(*args, **kwargs)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            return self._current.__getattr__(name)\n",
    "        \n",
    "    def __setattr__(self, name, value):\n",
    "        current_value = getattr(self, name)\n",
    "\n",
    "        if isinstance(current_value, (Patched, PatchedList, PatchedSequential)):\n",
    "            current_value._current = value\n",
    "        else:\n",
    "            super().__setattr__(name, value)\n",
    "\n",
    "    def set(self, new_value):\n",
    "        self._current = new_value\n",
    "    \n",
    "    def reset(self):\n",
    "        self._current = self._original\n",
    "\n",
    "        for c in self._current.children():\n",
    "            if isinstance(c, (Patched, PatchedList, PatchedSequential)):\n",
    "                c.reset()\n",
    "\n",
    "    @property\n",
    "    def _current(self):\n",
    "        return self.__dict__[\"_current\"]\n",
    "    \n",
    "    @property\n",
    "    def _original(self):\n",
    "        return self.__dict__[\"original\"]\n",
    "\n",
    "\n",
    "class PatchedList(nn.Module):\n",
    "    def __init__(self, module_list: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        module_list = [patch(c) for c in module_list]\n",
    "        self.__dict__[\"_current\"] = module_list\n",
    "        self.__dict__[\"_original\"] = module_list\n",
    "\n",
    "        for i, module in module_list:\n",
    "            self.add_module(str(i), module)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        current_value = getattr(self, name)\n",
    "\n",
    "        if isinstance(current_value, (Patched, PatchedList, PatchedSequential)):\n",
    "            current_value._current = value\n",
    "        else:\n",
    "            super().__setattr__(name, value)\n",
    "\n",
    "    def __setitem__(self, index, value):\n",
    "        self[index].set(value)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._current[index]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self._current)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._current)\n",
    "\n",
    "    def set(self, new_value):\n",
    "        self._current = new_value\n",
    "    \n",
    "    def reset(self):\n",
    "        self._current = self._original\n",
    "\n",
    "        for c in self:\n",
    "            if isinstance(c, (Patched, PatchedList, PatchedSequential)):\n",
    "                c.reset()\n",
    "\n",
    "    @property\n",
    "    def _current(self):\n",
    "        return self.__dict__[\"_current\"]\n",
    "    \n",
    "    @property\n",
    "    def _original(self):\n",
    "        return self.__dict__[\"original\"]\n",
    "    \n",
    "class PatchedSequential(PatchedList):\n",
    "    def forward(self, x):\n",
    "        for layer in self:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def set_head_to(i, output):\n",
    "    def new_softmax(self, x):\n",
    "        y = nn.functional.softmax(x, dim=-1)\n",
    "        y[:, i, :, :] = output\n",
    "        return y \n",
    "    \n",
    "    return new_softmax\n",
    "\n",
    "def set_heads_to(mappings):\n",
    "    def new_softmax(self, x):\n",
    "        y = nn.functional.softmax(x, dim=-1)\n",
    "\n",
    "        for k, v in mappings.items():\n",
    "            y[:, k, :, :] = v\n",
    "        return y \n",
    "    \n",
    "    return new_softmax\n",
    "\n",
    "class Patch(nn.Module):\n",
    "    def __init__(self, callable: Callable):\n",
    "        self.callable = callable\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.callable(*args, **kwargs)\n",
    "\n",
    "\n",
    "def patch_(module: nn.Module, callable: Callable):\n",
    "    if isinstance(module, Hooked):\n",
    "        module._original = module._forward\n",
    "        module._forward = callable\n",
    "\n",
    "        def reset():\n",
    "            module._forward = module._original\n",
    "            del module._forward\n",
    "            del module.reset\n",
    "\n",
    "        module.reset = reset\n",
    "        return module\n",
    "\n",
    "    else:\n",
    "        module._original = module.forward\n",
    "        module.forward = callable\n",
    "\n",
    "        def reset():\n",
    "            module.forward = module._original\n",
    "            del module._forward\n",
    "            del module.reset\n",
    "\n",
    "        module.reset = reset\n",
    "        return module\n",
    "\n",
    "model = deepcopy(run.model)\n",
    "patched_model = model #  patch(model)\n",
    "hooked_patched_model = hook(patched_model) # hook(patch(run.model))\n",
    "\n",
    "def apply_binop_dicts(d1, d2, op):\n",
    "    return {\n",
    "        k: op(d1[k], d2[k]) for k in d1.keys()\n",
    "    }\n",
    "\n",
    "# probe = ActivationProbe(masked_model, \"token_sequence_transformer.blocks.0.attention.attention_softmax\")\n",
    "# probe.register_hook()\n",
    "_, activations = hooked_patched_model.run_with_cache(x_trick[:2], y_trick[:2])\n",
    "plot_activations([activations[\"token_sequence_transformer.blocks.0.attention.attention_softmax\"]])\n",
    "\n",
    "evals_1 = run.evaluator(hooked_patched_model)\n",
    "\n",
    "head_2 = torch.zeros((16, 16), device=\"mps\")\n",
    "for i in range(0,16, 2):\n",
    "    head_2[i, i] = 1\n",
    "\n",
    "for i in range(0, 16, 2):\n",
    "    head_2[i+1, i] = head_2[i+1, i+1] = 0.5\n",
    "\n",
    "\n",
    "head_3 = torch.zeros((16, 16), device=\"mps\")\n",
    "for i in range(15):\n",
    "    head_3[i+1, i] = 1\n",
    "head_3[0, 0] = 1\n",
    "\n",
    "layer = patch_(hooked_patched_model.token_sequence_transformer.blocks[0].attention.attention_softmax, set_heads_to({1: head_2, 2: head_3}))\n",
    "_, activations = hooked_patched_model.run_with_cache(x_trick[:2], y_trick[:2])\n",
    "plot_activations([activations[\"token_sequence_transformer.blocks.0.attention.attention_softmax\"]])\n",
    "\n",
    "evals_2 = run.evaluator(hooked_patched_model)\n",
    "\n",
    "apply_binop_dicts(evals_1, evals_2, lambda x, y: (y-x)/x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ready to investigate\n",
    "run.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from torchtyping import TensorType\n",
    "\n",
    "\n",
    "def get_attention(model, xs, ys):\n",
    "    num_layers = len(model.token_sequence_transformer.blocks)\n",
    "    probes = []\n",
    "\n",
    "    for b in range(num_layers):\n",
    "        probe = ActivationProbe(model, f\"token_sequence_transformer.blocks.{b}.attention.attention_softmax\")\n",
    "        probe.register_hook()\n",
    "        probes.append(probe)\n",
    "\n",
    "    # Run the model\n",
    "    model(xs, ys)\n",
    "\n",
    "    for probe in probes:\n",
    "        probe.unregister_hook()\n",
    "\n",
    "    # Get the activations\n",
    "    return [probe.activation for probe in probes]\n",
    "\n",
    "def plot_activations(activations: List[TensorType[\"batch\", \"heads\", \"tokens\", \"tokens\"]]):       \n",
    "    num_layers = len(activations)\n",
    "    num_samples, num_heads, num_tokens, _ = activations[0].shape\n",
    "\n",
    "    for sample_idx in range(num_samples):\n",
    "        # Create a new figure\n",
    "        plt.figure(figsize=(15, 4 * num_layers))\n",
    "\n",
    "        # Loop through each head\n",
    "        for layer_idx, activation in enumerate(activations):\n",
    "            for head_idx in range(num_heads):\n",
    "                head_activation = activation[sample_idx, head_idx].detach().cpu().numpy()\n",
    "\n",
    "                # Create a subplot for each head\n",
    "                ax = plt.subplot(num_layers, num_heads, layer_idx * num_heads + head_idx + 1)\n",
    "\n",
    "                # Plot the activation\n",
    "                ax.imshow(head_activation, cmap='viridis', aspect='auto')\n",
    "\n",
    "                # Add title and labels\n",
    "                ax.set_title(f'Layer {layer_idx + 1}, Head {head_idx + 1}')\n",
    "                ax.set_xlabel('Keys')\n",
    "                ax.set_ylabel('Queries')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def compose2(f, g):\n",
    "    return lambda *a, **kw: f(g(*a, **kw))\n",
    "\n",
    "def compose(*fs):\n",
    "    from functools import reduce\n",
    "    return reduce(compose2, fs)\n",
    "\n",
    "get_and_plot_activations = compose(plot_activations, get_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = run.pretrain_dist.task_distribution.tasks\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.tasks import apply_transformations\n",
    "\n",
    "# x_trick = torch.zeros((4, 8, 4))\n",
    "# x_trick[:, :, 0] = torch.arange(0, 8)\n",
    "# # y_trick = torch.zeros((1, 8, 1))\n",
    "# x_trick = x_trick.to(\"mps\")\n",
    "# y_trick = apply_transformations(ws, x_trick, run.pretrain_dist.std, device=\"mps\")\n",
    "\n",
    "x_trick = torch.zeros((4, 8, 4))\n",
    "for i in range(4):\n",
    "    x_trick[i, :, i] = torch.arange(0, 8)\n",
    "\n",
    "# y_trick = torch.zeros((1, 8, 1))\n",
    "x_trick = x_trick.to(\"mps\")\n",
    "y_trick = apply_transformations(ws[0].repeat(4), x_trick, run.pretrain_dist.std, device=\"mps\")\n",
    "\n",
    "for i in range(4):\n",
    "    plt.matshow(x_trick[i].T.detach().cpu().numpy())\n",
    "\n",
    "plt.matshow(y_trick.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs, ys = run.pretrain_dist.get_batch(8, 1)\n",
    "# get_and_plot_activations(run.model, xs=xs, ys=ys)\n",
    "get_and_plot_activations(run.model, xs=x_trick, ys=y_trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_2 = get_unique_run(\n",
    "    \"../sweeps/small-sweep.yaml\", \n",
    "    task_config={\"num_tasks\": 65536, \"num_layers\": 2},\n",
    "    optimizer_config={\"lr\": 0.001}\n",
    ")\n",
    "\n",
    "xs_2, ys_2 = run_2.pretrain_dist.get_batch(8, 1)\n",
    "get_and_plot_activations(run_2.model, xs=xs_2, ys=ys_2)\n",
    "get_and_plot_activations(run_2.model, xs=x_trick, ys=y_trick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_3 = get_unique_run(\n",
    "    \"../sweeps/small-sweep.yaml\", \n",
    "    task_config={\"num_tasks\": 64, \"num_layers\": 2},\n",
    "    optimizer_config={\"lr\": 0.001}\n",
    ")\n",
    "\n",
    "xs_3, ys_3 = run_3.pretrain_dist.get_batch(8, 1)\n",
    "get_and_plot_activations(run_3.model, xs=xs_3, ys=ys_3)\n",
    "get_and_plot_activations(run_3.model, xs=x_trick, ys=y_trick)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
