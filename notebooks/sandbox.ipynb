{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0.0:embed/token': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '0.1:embed/pos': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '1.0:block_0/ln_0': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '1.1.0:block_0/head_0': <function __main__.make_head_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '1.1.1:block_0/head_1': <function __main__.make_head_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '1.1.2:block_0/head_2': <function __main__.make_head_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '1.1.3:block_0/head_3': <function __main__.make_head_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '1.2:block_0/ln_1': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '1.3.0:block_0/mlp_0': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '1.3.1:block_0/mlp_1': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '2.0:block_1/ln_0': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '2.1.0:block_1/head_0': <function __main__.make_head_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '2.1.1:block_1/head_1': <function __main__.make_head_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '2.1.2:block_1/head_2': <function __main__.make_head_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '2.1.3:block_1/head_3': <function __main__.make_head_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '2.2:block_1/ln_1': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '2.3.0:block_1/mlp_0': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '2.3.1:block_1/mlp_1': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '3.0:unembed/ln': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>,\n",
       "  '3.1:unembed/linear': <function __main__.make_layer_accessor.<locals>.accessor(model: torch.nn.modules.module.Module) -> torch.Tensor>},\n",
       " {'0.0:embed/token': ('0.0:embed/token', '0.0:embed/token'),\n",
       "  '0.1:embed/pos': ('0.1:embed/pos', '0.1:embed/pos'),\n",
       "  '1.0:block_0/ln_0': ('1.0:block_0/ln_0', '1.0:block_0/ln_0'),\n",
       "  '1.1.0:block_0/head_0': ('1.1.0:block_0/head_0', '1.1.0:block_0/head_0'),\n",
       "  '1.1.1:block_0/head_1': ('1.1.1:block_0/head_1', '1.1.1:block_0/head_1'),\n",
       "  '1.1.2:block_0/head_2': ('1.1.2:block_0/head_2', '1.1.2:block_0/head_2'),\n",
       "  '1.1.3:block_0/head_3': ('1.1.3:block_0/head_3', '1.1.3:block_0/head_3'),\n",
       "  '1.2:block_0/ln_1': ('1.2:block_0/ln_1', '1.2:block_0/ln_1'),\n",
       "  '1.3.0:block_0/mlp_0': ('1.3.0:block_0/mlp_0', '1.3.0:block_0/mlp_0'),\n",
       "  '1.3.1:block_0/mlp_1': ('1.3.1:block_0/mlp_1', '1.3.1:block_0/mlp_1'),\n",
       "  '2.0:block_1/ln_0': ('2.0:block_1/ln_0', '2.0:block_1/ln_0'),\n",
       "  '2.1.0:block_1/head_0': ('2.1.0:block_1/head_0', '2.1.0:block_1/head_0'),\n",
       "  '2.1.1:block_1/head_1': ('2.1.1:block_1/head_1', '2.1.1:block_1/head_1'),\n",
       "  '2.1.2:block_1/head_2': ('2.1.2:block_1/head_2', '2.1.2:block_1/head_2'),\n",
       "  '2.1.3:block_1/head_3': ('2.1.3:block_1/head_3', '2.1.3:block_1/head_3'),\n",
       "  '2.2:block_1/ln_1': ('2.2:block_1/ln_1', '2.2:block_1/ln_1'),\n",
       "  '2.3.0:block_1/mlp_0': ('2.3.0:block_1/mlp_0', '2.3.0:block_1/mlp_0'),\n",
       "  '2.3.1:block_1/mlp_1': ('2.3.1:block_1/mlp_1', '2.3.1:block_1/mlp_1'),\n",
       "  '3.0:unembed/ln': ('3.0:unembed/ln', '3.0:unembed/ln'),\n",
       "  '3.1:unembed/linear': ('3.1:unembed/linear', '3.1:unembed/linear'),\n",
       "  '1.1.0:block_0/head_0-2.1.0:block_1/head_0': ('1.1.0:block_0/head_0',\n",
       "   '2.1.0:block_1/head_0'),\n",
       "  '1.1.0:block_0/head_0-2.1.1:block_1/head_1': ('1.1.0:block_0/head_0',\n",
       "   '2.1.1:block_1/head_1'),\n",
       "  '1.1.0:block_0/head_0-2.1.2:block_1/head_2': ('1.1.0:block_0/head_0',\n",
       "   '2.1.2:block_1/head_2'),\n",
       "  '1.1.0:block_0/head_0-2.1.3:block_1/head_3': ('1.1.0:block_0/head_0',\n",
       "   '2.1.3:block_1/head_3'),\n",
       "  '1.1.1:block_0/head_1-2.1.0:block_1/head_0': ('1.1.1:block_0/head_1',\n",
       "   '2.1.0:block_1/head_0'),\n",
       "  '1.1.1:block_0/head_1-2.1.1:block_1/head_1': ('1.1.1:block_0/head_1',\n",
       "   '2.1.1:block_1/head_1'),\n",
       "  '1.1.1:block_0/head_1-2.1.2:block_1/head_2': ('1.1.1:block_0/head_1',\n",
       "   '2.1.2:block_1/head_2'),\n",
       "  '1.1.1:block_0/head_1-2.1.3:block_1/head_3': ('1.1.1:block_0/head_1',\n",
       "   '2.1.3:block_1/head_3'),\n",
       "  '1.1.2:block_0/head_2-2.1.0:block_1/head_0': ('1.1.2:block_0/head_2',\n",
       "   '2.1.0:block_1/head_0'),\n",
       "  '1.1.2:block_0/head_2-2.1.1:block_1/head_1': ('1.1.2:block_0/head_2',\n",
       "   '2.1.1:block_1/head_1'),\n",
       "  '1.1.2:block_0/head_2-2.1.2:block_1/head_2': ('1.1.2:block_0/head_2',\n",
       "   '2.1.2:block_1/head_2'),\n",
       "  '1.1.2:block_0/head_2-2.1.3:block_1/head_3': ('1.1.2:block_0/head_2',\n",
       "   '2.1.3:block_1/head_3'),\n",
       "  '1.1.3:block_0/head_3-2.1.0:block_1/head_0': ('1.1.3:block_0/head_3',\n",
       "   '2.1.0:block_1/head_0'),\n",
       "  '1.1.3:block_0/head_3-2.1.1:block_1/head_1': ('1.1.3:block_0/head_3',\n",
       "   '2.1.1:block_1/head_1'),\n",
       "  '1.1.3:block_0/head_3-2.1.2:block_1/head_2': ('1.1.3:block_0/head_3',\n",
       "   '2.1.2:block_1/head_2'),\n",
       "  '1.1.3:block_0/head_3-2.1.3:block_1/head_3': ('1.1.3:block_0/head_3',\n",
       "   '2.1.3:block_1/head_3'),\n",
       "  '1.3.0:block_0/mlp_0-1.3.0:block_0/mlp_0': ('1.3.0:block_0/mlp_0',\n",
       "   '1.3.0:block_0/mlp_0'),\n",
       "  '1.3.1:block_0/mlp_1-1.3.1:block_0/mlp_1': ('1.3.1:block_0/mlp_1',\n",
       "   '1.3.1:block_0/mlp_1'),\n",
       "  '2.3.0:block_1/mlp_0-2.3.0:block_1/mlp_0': ('2.3.0:block_1/mlp_0',\n",
       "   '2.3.0:block_1/mlp_0'),\n",
       "  '2.3.1:block_1/mlp_1-2.3.1:block_1/mlp_1': ('2.3.1:block_1/mlp_1',\n",
       "   '2.3.1:block_1/mlp_1'),\n",
       "  '0.0:embed/token-3.1:unembed/linear': ('0.0:embed/token',\n",
       "   '3.1:unembed/linear')})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def make_layer_accessor(path: str):\n",
    "    def accessor(model: nn.Module) -> torch.Tensor:\n",
    "        layer = model\n",
    "        for name in path.split(\".\"):\n",
    "            layer = getattr(layer, name)\n",
    "\n",
    "        return layer.weight\n",
    "\n",
    "    return accessor\n",
    "\n",
    "\n",
    "# TODO: Check that this yields the correct weights\n",
    "def make_head_accessor(attn_layer: str, head: int, num_heads=2, embed_dim=4, head_size: Optional[int] = None):\n",
    "    \"\"\"Make a function to access the weights of a particular attention head.\"\"\"\n",
    "    head_size = embed_dim // num_heads if head_size is None else head_size\n",
    "    num_head_params = 3 * head_size * embed_dim\n",
    "\n",
    "    def accessor(model: nn.Module) -> torch.Tensor:\n",
    "        return make_layer_accessor(attn_layer)(model)[head * num_head_params: (head + 1) * num_head_params]\n",
    "\n",
    "    return accessor\n",
    "\n",
    "\n",
    "def _head_loc(l: int, h: int):\n",
    "    return f\"{1 + l}.1.{h}:block_{l}/head_{h}\"\n",
    "\n",
    "def _ln_loc(l: int, i: int):\n",
    "    return f\"{1 + l}.{2 * i}:block_{l}/ln_{i}\"\n",
    "\n",
    "def _mlp_loc(l: int, i: int):\n",
    "    return f\"{1 + l}.3.{i}:block_{l}/mlp_{i}\"\n",
    "\n",
    "def make_transformer_accessors(L=2, H=4):\n",
    "    assert L < 10, \"L must be less than 10\"\n",
    "\n",
    "    accessors = {\n",
    "        _head_loc(l, h): make_head_accessor(f\"token_sequence_transformer.blocks.{l}.attention.attention\", h)\n",
    "        for l in range(L) for h in range(H)\n",
    "    }\n",
    "\n",
    "    accessors.update({\n",
    "        _mlp_loc(l, i): make_layer_accessor(f\"token_sequence_transformer.blocks.{l}.compute.{2 * i}\")\n",
    "        for l in range(L) for i in range(2)\n",
    "    })\n",
    "\n",
    "    accessors.update({\n",
    "        _ln_loc(l, i): make_layer_accessor(f\"token_sequence_transformer.blocks.{l}.attention.layer_norms.{i}\")\n",
    "        for i in range(2) for l in range(L)\n",
    "    })\n",
    "\n",
    "    accessors.update({\n",
    "        \"0.0:embed/token\": make_layer_accessor(\"token_sequence_transformer.token_embedding\"),\n",
    "        \"0.1:embed/pos\": make_layer_accessor(\"token_sequence_transformer.postn_embedding\"),\n",
    "        f\"{1+L}.0:unembed/ln\": make_layer_accessor(\"token_sequence_transformer.unembedding.0\"),\n",
    "        f\"{1+L}.1:unembed/linear\": make_layer_accessor(\"token_sequence_transformer.unembedding.1\"),\n",
    "    })\n",
    "\n",
    "    # Sort\n",
    "    accessors = {k: v for k, v in sorted(accessors.items(), key=lambda x: x[0])}\n",
    "    \n",
    "    return accessors\n",
    "\n",
    "def make_transformer_accessors_and_interactions(L=2, H=4):\n",
    "    accessors = make_transformer_accessors(L, H)\n",
    "\n",
    "    # We want all within-layer covariances\n",
    "    paths = {key: (key, key) for key in accessors.keys()}\n",
    "\n",
    "    # We want between-head covariances for successive layers\n",
    "    if L >= 2:\n",
    "        for l in range(0, L-1):\n",
    "            for h1 in range(H):\n",
    "                for h2 in range(H):\n",
    "                    paths[f\"{_head_loc(l, h1)}-{_head_loc(l+1, h2)}\"] = (_head_loc(l, h1), _head_loc(l+1, h2))\n",
    "\n",
    "    # Let's check between mlp covariances within a single block\n",
    "    for l in range(L):\n",
    "        for i in range(2):\n",
    "            paths[f\"{_mlp_loc(l, i)}-{_mlp_loc(l, i)}\"] = (_mlp_loc(l, i), _mlp_loc(l, i))\n",
    "\n",
    "    # And let's check between embeds and unembeds\n",
    "    paths[f\"0.0:embed/token-{1+L}.1:unembed/linear\"] = (\"0.0:embed/token\", f\"{1+L}.1:unembed/linear\")\n",
    "\n",
    "    return accessors, paths\n",
    "\n",
    "make_transformer_accessors_and_interactions(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
