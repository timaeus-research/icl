{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLCT estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if os.environ.get(\"PJRT_DEVICE\") == \"TPU\":\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "    print(xm.get_xla_supported_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "from pprint import pp\n",
    "import yaml\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# from devinterp.evals import SamplerEvaluator\n",
    "from devinfra.io.storage import CheckpointerConfig, BaseStorageProvider\n",
    "from devinfra.evals import ModelEvaluator\n",
    "from devinfra.optim.optimizers import OptimizerConfig\n",
    "from devinfra.utils.seed import set_seed\n",
    "\n",
    "from icl.config import get_config\n",
    "from icl.evals import ICLEvaluator\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "# Warning block_idx in the dataframe starts at 1... sorry. \n",
    "cmap = sns.color_palette(\"viridis_r\", 16) # Light to dark\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sweep_configs(sweep_config_dicts: List[Dict], **kwargs):\n",
    "    for sweep_config_dict in sweep_config_dicts:\n",
    "        yield get_config(**sweep_config_dict, **kwargs)\n",
    "\n",
    "# Need to include the following because I was a dumbass and created the checkpoint names with a hash that didn't include all of the defualts.\n",
    "shared = {\"model_seed\": 0, \"pretrain_seed\": 1, \"true_seed\": 2, \"sampling_seed\": 3}\n",
    "layers4 = {\"task_size\": 4, \"max_examples\": 8, \"num_layers\": 4, \"num_heads\": 4, \"embed_size\": 64, \"mlp_size\": 64, \"noise_variance\": 0.125}\n",
    "configs = list(get_sweep_configs([{\"task_config\": {\"num_tasks\": 2**i, **layers4, **shared}, \"optimizer_config\": {\"lr\": 0.01}} for i in range(21)]))\n",
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointers = []\n",
    "\n",
    "for config in configs:\n",
    "    checkpointer = config.checkpointer_config.factory()\n",
    "    checkpointers.append(checkpointer)\n",
    "    print(repr(checkpointer) + \":\", str(tuple(checkpointer.file_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_at_step(config, step: int, checkpointer: Optional[BaseStorageProvider] = None):\n",
    "    if checkpointer is None:\n",
    "        checkpointer = config.checkpointer_config.factory()\n",
    "\n",
    "    model = config.task_config.model_factory()\n",
    "    model_state_dict = checkpointer.load_file(step)[\"model\"]\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model_at_last_checkpoint(config, checkpointer: Optional[BaseStorageProvider] = None):\n",
    "    if checkpointer is None:\n",
    "        checkpointer = config.checkpointer_config.factory()\n",
    "\n",
    "    model = config.task_config.model_factory()\n",
    "    model_state_dict = checkpointer[-1][\"model\"]\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "def eval_model_over_checkpoints(model, checkpointer: BaseStorageProvider, evaluator: ModelEvaluator, verbose=False):\n",
    "    steps = checkpointer.file_ids\n",
    "    evals = []\n",
    "\n",
    "    for step in steps:\n",
    "        model_state_dict = checkpointer.load_file(step)[\"model\"]\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        evals.append({**evaluator(model), \"step\": step})\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\n\")\n",
    "            print(f\"Step {step}\")\n",
    "            print(yaml.dump(evals[-1]))\n",
    "\n",
    "    return pd.DataFrame(evals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "config = configs[0]\n",
    "\n",
    "# initialise model\n",
    "model = load_model_at_last_checkpoint(config, checkpointers[0])\n",
    "\n",
    "# initialise 'pretraining' data source (for training on fixed task set)\n",
    "pretrain_dist = config.task_config.pretrain_dist_factory().to(device)\n",
    "\n",
    "# initialise 'true' data source (for evaluation, including unseen tasks)\n",
    "true_dist = config.task_config.true_dist_factory().to(device)\n",
    "\n",
    "# initialise evaluations\n",
    "evaluator = ICLEvaluator(\n",
    "    pretrain_dist=pretrain_dist,\n",
    "    true_dist=true_dist,\n",
    "    max_examples=config.task_config.max_examples,\n",
    "    eval_batch_size=config.eval_batch_size,\n",
    ")\n",
    "\n",
    "# Load model    \n",
    "# model.load_state_dict(checkpointers[0][-1][\"model\"])\n",
    "evaluator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtyping import TensorType\n",
    "from devinterp.mechinterp.activations import ActivationProbe\n",
    "from devinfra.utils.iterables import flatten_dict\n",
    "import pandas as pd\n",
    "\n",
    "def make_attention_entropy_evals(xs, ys, **paths):\n",
    "    \"\"\"\n",
    "    Each path should be a\n",
    "    \"\"\"\n",
    "    def get_attention_entropy(attn: TensorType[\"B\", \"H\", \"T\", \"T\"]):\n",
    "        log_attention = torch.where(attn > 0, torch.log(attn), torch.tensor(0.0).to(attn.device))\n",
    "    \n",
    "        # Compute entropy: -sum(p * log(p))\n",
    "        entropy = -torch.sum(attn * log_attention, dim=-1).mean(dim=0).squeeze(-1) # H T\n",
    "        \n",
    "        num_heads, num_tokens = entropy.shape\n",
    "\n",
    "        def get_head_attns(entropy):\n",
    "            results =  {\n",
    "                f\"token_{j}\": entropy[j].item()\n",
    "                for j in range(num_tokens)\n",
    "            }\n",
    "\n",
    "            results[\"mean\"] = entropy.mean().item()\n",
    "            return results\n",
    "\n",
    "        results = {\n",
    "            f\"head_{i}\": get_head_attns(entropy[i]) for i in range(num_heads)\n",
    "        }\n",
    "\n",
    "        results[\"mean\"] = entropy.mean().item()\n",
    "        return results\n",
    "\n",
    "    def eval_attention_entropy(model):\n",
    "        # Hook into the attention\n",
    "        probes = [ActivationProbe(model, path) for path in paths.values()]\n",
    "\n",
    "        # TODO: avoid this awful registering thing\n",
    "        for probe in probes:\n",
    "            probe.register_hook()\n",
    "\n",
    "        model(xs, ys)\n",
    "        \n",
    "        for probe in probes:\n",
    "            probe.unregister_hook()         \n",
    "\n",
    "        return flatten_dict({\n",
    "            k: get_attention_entropy(probe.activation)\n",
    "            for k, probe in zip(paths.keys(), probes)\n",
    "        })\n",
    "\n",
    "    return eval_attention_entropy\n",
    "\n",
    "attention_entropy_evals = make_attention_entropy_evals(\n",
    "    evaluator.pretrain_xs,\n",
    "    evaluator.pretrain_ys,\n",
    "    block_1=\"token_sequence_transformer.blocks.0.attention.attention_softmax\",\n",
    "    block_2=\"token_sequence_transformer.blocks.1.attention.attention_softmax\"\n",
    ")\n",
    "\n",
    "attention_entropy_evals(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_entropies_over_checkpoints = eval_model_over_checkpoints(model, checkpointer, attention_entropy_evals, verbose=True)\n",
    "attn_entropies_over_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(attn_entropies_over_checkpoints.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_patterns(df, title=\"\", save: Optional[str] = None):\n",
    "    fig = plt.figure(figsize=(20, 25))\n",
    "    plt.suptitle(title)\n",
    " \n",
    "\n",
    "    ax0 = plt.subplot2grid((6, 4), (0, 0), colspan=4)\n",
    " \n",
    "    # First plot: Block 1 vs 2 (mean), 4-column wide\n",
    "    ax0.plot(df.step, df[\"block_1/mean\"], label=\"block_1\")\n",
    "    ax0.plot(df.step, df[\"block_2/mean\"], label=\"block_2\")\n",
    "    ax0.set_title(\"Block 1 vs. 2\")\n",
    "    ax0.set_xlabel(\"Step\")\n",
    "    ax0.set_ylabel(\"Entropy\")\n",
    "    ax0.legend()\n",
    "\n",
    "    ax1 = [plt.subplot2grid((6, 4), (1, i*2), colspan=2) for i in range(2)]\n",
    "    for block_idx in range(2):\n",
    "        ax1[block_idx].set_title(f\"Block {block_idx + 1}\")\n",
    "        ax1[block_idx].set_xlabel(\"Step\")\n",
    "        ax1[block_idx].set_ylabel(\"Entropy\")\n",
    "        for head_idx in range(4):\n",
    "            series = df[f\"block_{block_idx+1}/head_{head_idx}/mean\"]\n",
    "            ax1[block_idx].plot(df.step, series, label=f\"Head {head_idx + 1}\")\n",
    "        ax1[block_idx].legend()\n",
    "\n",
    "    ax2 = [plt.subplot2grid((6, 4), (i//4 + 2, i%4)) for i in range(16)]\n",
    "    ax_idx = 0\n",
    "    for head_idx in range(4):\n",
    "        for block_idx in range(2):\n",
    "            for xs in (1, 0):\n",
    "                ax2[ax_idx].set_title(f\"Block {block_idx + 1} Head {head_idx + 1} {'X' if xs else 'Y'}\")\n",
    "                ax2[ax_idx].set_xlabel(\"Step\")\n",
    "                ax2[ax_idx].set_ylabel(\"Entropy\")\n",
    "                for token_idx in range(1-int(xs), 16, 2):\n",
    "                    series = df[f\"block_{block_idx+1}/head_{head_idx}/token_{token_idx}\"]\n",
    "                    ax2[ax_idx].plot(df.step, series, label=f\"Token {token_idx + 1}\", color=cmap[token_idx])\n",
    "                # ax2[ax_idx].set_xscale(\"log\")\n",
    "                # ax2[ax_idx].set_yscale(\"log\")\n",
    "                ax_idx += 1\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    if save:\n",
    "        parent_dir = os.path.dirname(save)\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "\n",
    "        plt.savefig(save)\n",
    "\n",
    "    plt.show()    \n",
    "\n",
    "plot_attention_patterns(attn_entropies_over_checkpoints, \"M=1\", save=\"../figures/M=1/attention_patterns.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations_over_models_over_checkpoints(configs, checkpointers):\n",
    "    dfs = []\n",
    "\n",
    "    for config, checkpointer in zip(configs, checkpointers):\n",
    "        num_tasks = config.task_config.num_tasks\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"=\" * 20 + f\" M={num_tasks} \" + \"=\" * 20)\n",
    "        model = load_model_at_last_checkpoint(config, checkpointer)\n",
    "\n",
    "        # initialise 'pretraining' data source (for training on fixed task set)\n",
    "        pretrain_dist = config.task_config.pretrain_dist_factory().to(device)\n",
    "\n",
    "        # initialise 'true' data source (for evaluation, including unseen tasks)\n",
    "        true_dist = config.task_config.true_dist_factory().to(device)\n",
    "\n",
    "        # initialise evaluations\n",
    "        evaluator = ICLEvaluator(\n",
    "            pretrain_dist=pretrain_dist,\n",
    "            true_dist=true_dist,\n",
    "            max_examples=config.task_config.max_examples,\n",
    "            eval_batch_size=config.eval_batch_size,\n",
    "        )\n",
    "             \n",
    "        attention_entropy_evals = make_attention_entropy_evals(\n",
    "            evaluator.pretrain_xs,\n",
    "            evaluator.pretrain_ys,\n",
    "            block_1=\"token_sequence_transformer.blocks.0.attention.attention_softmax\",\n",
    "            block_2=\"token_sequence_transformer.blocks.1.attention.attention_softmax\"\n",
    "        )\n",
    "        \n",
    "        attn_entropies_over_checkpoints = eval_model_over_checkpoints(model, checkpointer, attention_entropy_evals, verbose=False)\n",
    "        plot_attention_patterns(attn_entropies_over_checkpoints, f\"M={num_tasks}\", save=f\"../figures/M={num_tasks}/attention_entropies.png\")\n",
    "\n",
    "        dfs.append(attn_entropies_over_checkpoints)\n",
    "\n",
    "    return dfs\n",
    "\n",
    "dfs = plot_activations_over_models_over_checkpoints(configs[1:], checkpointers[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, config in zip(dfs, configs[1:]):\n",
    "    num_tasks = config.task_config.num_tasks\n",
    "    plot_attention_patterns(df, f\"M={num_tasks}\", save=f\"../figures/M={num_tasks}/attention_entropies.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dim reduction of weights and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from devinfra.utils.iterables import prepend_dict\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "Transform = Callable[[torch.Tensor], torch.Tensor]\n",
    "\n",
    "class ActivationProbe:\n",
    "    \"\"\"\n",
    "    A utility class to extract the activation value of a specific layer or neuron within a neural network.\n",
    "    \n",
    "    The location of the target is defined using a string that can specify the layer, channel, and spatial coordinates (y, x). The format allows flexibility in defining the location:\n",
    "    \n",
    "    - 'layer1.0.conv1': Targets the entire layer.\n",
    "    - 'layer1.0.conv1.3': Targets channel 3 in the specified layer.\n",
    "    - 'layer1.0.conv1.3.2.2': Targets channel 3, y-coordinate 2, and x-coordinate 2 in the specified layer.\n",
    "    - 'layer1.0.conv1.*': Targets all neurons in the specified layer.\n",
    "    \n",
    "    The class provides methods to register a forward hook into a PyTorch model to capture the activation of the specified target during model inference.\n",
    "    \n",
    "    Attributes:\n",
    "        model: The PyTorch model from which to extract the activation.\n",
    "        layer_location (List[str]): List of strings specifying the layer hierarchy.\n",
    "        neuron_location (List[int]): List of integers specifying the channel, y, and x coordinates.\n",
    "        activation: The value of the activation at the specified location.\n",
    "\n",
    "    Example:\n",
    "        model = ResNet18()\n",
    "        extractor = ActivationProbe(model, 'layer1.0.conv1.3')\n",
    "        handle = extractor.register_hook()\n",
    "        output = model(input_tensor)\n",
    "        print(extractor.activation)  # Prints the activation value\n",
    "\n",
    "    The wildcard '*' in neuron_location means that all neurons in the specified layer will be targeted.\n",
    "    For example, 'layer1.0.conv1.*' will capture activations for all neurons in the 'layer1.0.conv1' layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, location):\n",
    "        self.activation = None\n",
    "        self.model = model\n",
    "        location = location.split('.')\n",
    "\n",
    "        self.layer_location = []\n",
    "        self.neuron_location = []\n",
    "\n",
    "        # Get the target layer\n",
    "        self.layer = model\n",
    "        for part in location:\n",
    "            if part == \"\":\n",
    "                continue\n",
    "            if hasattr(self.layer, part):\n",
    "                self.layer_location.append(part)\n",
    "                self.layer = getattr(self.layer, part)\n",
    "            else:\n",
    "                if part == \"*\":\n",
    "                    self.neuron_location.append(...)\n",
    "                else:\n",
    "                    self.neuron_location.append(int(part))\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        if self.neuron_location:\n",
    "            # Assumes first index is over batch \n",
    "            self.activation = output[(..., *self.neuron_location)]\n",
    "        else:\n",
    "            self.activation = output\n",
    "\n",
    "    def register_hook(self):\n",
    "        self.handle = self.layer.register_forward_hook(self.hook_fn)\n",
    "        return self.handle\n",
    "    \n",
    "    def unregister_hook(self):\n",
    "        self.handle.remove()\n",
    "    \n",
    "\n",
    "    @contextmanager\n",
    "    def watch(self):\n",
    "        handle = self.register_hook()\n",
    "        yield\n",
    "        handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, Union, Dict\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from devinterp.mechinterp.activations import ActivationProbe\n",
    "\n",
    "# m = deepcopy(model)\n",
    "# model = deepcopy(m)\n",
    "\n",
    "class ActivationsReducer:\n",
    "    activation_samples: Dict[str, Union[list, np.ndarray]]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xs: torch.Tensor,\n",
    "        ys: torch.Tensor,\n",
    "        activation_paths: Optional[Dict[str, str]] = None, \n",
    "        dr_method: Callable = None, \n",
    "    ):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.activation_samples = defaultdict(list)\n",
    "        self.targets = activation_paths or model.named_modules()\n",
    "        self.dr_method = dr_method\n",
    "        \n",
    "    def eval(self, model):\n",
    "        assert not len(self.activation_samples.values()) or isinstance(next(iter(self.activation_samples.values())), list), \"Cannot run eval() after running freeze()\"\n",
    "\n",
    "        probes = [ActivationProbe(model, path) for path in self.targets.values()]\n",
    "        \n",
    "        for probe in probes:\n",
    "            probe.register_hook()\n",
    "        \n",
    "        model(self.xs, self.ys)\n",
    "        \n",
    "        for probe in probes:\n",
    "            probe.unregister_hook()\n",
    "            \n",
    "        for name, probe in zip(self.targets.keys(), probes):\n",
    "            activation = probe.activation.detach().cpu().numpy()\n",
    "            activation = activation.reshape(-1)  \n",
    "            self.activation_samples[name].append(activation)\n",
    "\n",
    "    def run(self, model, checkpointer):\n",
    "        for step in tqdm(steps, desc=\"Iterating over checkpoints\"):\n",
    "            model_state_dict = checkpointer.load_file(step)[\"model\"]\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            self.eval(model)\n",
    "\n",
    "    def freeze(self):\n",
    "        self.activation_samples = {k: np.array(v) for k, v in self.activation_samples.items()}\n",
    "            \n",
    "    def transform(self):\n",
    "        assert isinstance(next(iter(self.activation_samples.values())), np.ndarray), \"Must run freeze() before fit()\"\n",
    "        return {k: self.dr_method(v) for k, v in self.activation_samples.items()} if self.dr_method else self.activation_samples\n",
    "\n",
    "    def run_transform(self, model, checkpointer):\n",
    "        self.run(model, checkpointer)\n",
    "        self.freeze()\n",
    "        return self.transform()\n",
    "\n",
    "\n",
    "def apply_pca(samples: np.ndarray, num_components=2):\n",
    "    pca = PCA(n_components=num_components)\n",
    "    transformed_samples = pca.fit_transform(samples)\n",
    "    return pca, transformed_samples\n",
    "\n",
    "activation_reducer = ActivationsReducer(\n",
    "    evaluator.pretrain_xs,\n",
    "    evaluator.pretrain_ys,\n",
    "    # {\"ys\": \"\"},\n",
    "    {\"y\": \"7.0\"},\n",
    "    dr_method=apply_pca\n",
    ")\n",
    "\n",
    "steps = checkpointer.file_ids\n",
    "activation_reducer.run(model, checkpointer)\n",
    "activation_reducer.freeze()\n",
    "dr_samples_pca, dr_samples = activation_reducer.transform()[\"y\"] \n",
    "#activation_reducer.dr_method(activation_reducer.activation_samples[\"ys\"])# activation_reducer.reduce()\n",
    "samples = activation_reducer.activation_samples[\"y\"]\n",
    "samples_last_half = samples[len(samples)//2:, :]\n",
    "pca_last_half = PCA(n_components=2)\n",
    "transformed_samples_last_half = pca_last_half.fit_transform(samples_last_half)\n",
    "steps_last_half = checkpointer.file_ids[len(samples)//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples.shape[1] // evaluator.pretrain_xs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to plot sample evolution with color linear in steps and rescale samples\n",
    "def plot_sample_evolution_with_inset(steps, samples, explained_variance, title=\"Sample Evolution in 2D Plane\", num_points_to_label=10, save: Optional[str] = None, ax: Optional = None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    # Main plot\n",
    "    sc = ax.scatter(samples[:, 0], samples[:, 1], c=steps, cmap='viridis', s=50, alpha=0.6)\n",
    "    plt.colorbar(sc, ax=ax, label='Steps')\n",
    "    \n",
    "    # Label some points\n",
    "    total_samples = len(samples)\n",
    "    step = total_samples // num_points_to_label\n",
    "    for i in range(0, total_samples, step):\n",
    "        sample_step = steps[i]\n",
    "        ax.text(samples[i, 0], samples[i, 1], str(sample_step), fontsize=12, ha='right', va='bottom')\n",
    "        \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Inset for explained variance at the bottom right corner with slight transparency\n",
    "    axins = ax.inset_axes([0.7, 0.05, 0.25, 0.25])  # x, y, width, height\n",
    "    axins.bar(range(len(explained_variance)), explained_variance, alpha=0.5)\n",
    "    axins.set_title('Explained Variance')\n",
    "    axins.set_xlabel('Component')\n",
    "    axins.set_ylabel('Variance')\n",
    "    axins.patch.set_alpha(0.5)\n",
    "    \n",
    "    if save:\n",
    "        parent_dir = os.path.dirname(save)\n",
    "\n",
    "        if not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir)\n",
    "\n",
    "        plt.savefig(save)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "plt.suptitle(\"M=1\")\n",
    "plot_sample_evolution_with_inset(checkpointer.file_ids, dr_samples, dr_samples_pca.explained_variance_ratio_, title=\"All checkpoints\", ax=axes[0])\n",
    "plot_sample_evolution_with_inset(steps_last_half, samples_last_half, pca_last_half.explained_variance_ratio_, title=\"Last half of checkpoints\", save=f\"../figures/M=1/M1_y_pca.png\", ax=axes[1])\n",
    "plt.show()\n",
    "\n",
    "# Example usage could be similar to before, with the option to specify immediate_dr\n",
    "# immediate_dr = SomeRandomProjectionFunction\n",
    "# weight_eval, apply_weight_dr = make_weight_dr_evals({\"layer1\": \"layer1.weight\"}, dr_method, immediate_dr)\n",
    "# activation_eval, apply_activation_dr = make_activation_dr_evals({\"act1\": \"layer1\"}, dr_method, immediate_dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_dict(d: dict, prefix: str, delimiter=\".\"):\n",
    "    return {f\"{prefix}{delimiter}{k}\": v for k, v in d.items()}\n",
    "\n",
    "def plot_pcas_over_models_over_checkpoints(configs, checkpointers, paths):\n",
    "    pca_results = []\n",
    "\n",
    "    def apply_pca(samples: np.ndarray, num_components=2):\n",
    "        pca = PCA(n_components=num_components)\n",
    "        transformed_samples = pca.fit_transform(samples)\n",
    "        return pca, transformed_samples\n",
    "\n",
    "    for config, checkpointer in zip(configs, checkpointers):\n",
    "        num_layers = config.task_config.num_layers\n",
    "        num_tasks = config.task_config.num_tasks\n",
    "        print(\"\\n\\n\")\n",
    "        print(\"=\" * 20 + f\" M={num_tasks} \" + \"=\" * 20)\n",
    "        model = load_model_at_last_checkpoint(config, checkpointer)\n",
    "\n",
    "        # initialise 'pretraining' data source (for training on fixed task set)\n",
    "        pretrain_dist = config.task_config.pretrain_dist_factory().to(device)\n",
    "\n",
    "        # initialise 'true' data source (for evaluation, including unseen tasks)\n",
    "        true_dist = config.task_config.true_dist_factory().to(device)\n",
    "\n",
    "        # initialise evaluations\n",
    "        evaluator = ICLEvaluator(\n",
    "            pretrain_dist=pretrain_dist,\n",
    "            true_dist=true_dist,\n",
    "            max_examples=config.task_config.max_examples,\n",
    "            eval_batch_size=config.eval_batch_size,\n",
    "        )\n",
    "        \n",
    "        activation_reducer = ActivationsReducer(\n",
    "            evaluator.pretrain_xs,\n",
    "            evaluator.pretrain_ys,\n",
    "            paths,\n",
    "            dr_method=apply_pca\n",
    "        )\n",
    "\n",
    "        steps = checkpointer.file_ids\n",
    "\n",
    "        local_pca_results = {\n",
    "            \"num_tasks\": num_tasks,\n",
    "        }\n",
    "\n",
    "        transformed_samples = activation_reducer.run_transform(model, checkpointer)\n",
    "\n",
    "        for path in paths.keys():\n",
    "            pca, samples = transformed_samples[path]\n",
    "            samples_full = activation_reducer.activation_samples[path]\n",
    "            samples_last_half = samples_full[len(samples_full) //2:, :]\n",
    "            steps_last_half = checkpointer.file_ids[len(samples)//2:]\n",
    "            pca_last_half, transformed_samples_last_half = apply_pca(samples_last_half)\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "            plt.suptitle(f\"M={num_tasks}\")\n",
    "            plot_sample_evolution_with_inset(checkpointer.file_ids, samples, pca.explained_variance_ratio_, title=\"All checkpoints\", ax=axes[0])\n",
    "            plot_sample_evolution_with_inset(steps_last_half, samples_last_half, pca_last_half.explained_variance_ratio_, title=\"Last half of checkpoints\", save=f\"../figures/L={num_layers}_M={num_tasks}_{path}_pca.png\", ax=axes[1])\n",
    "            plt.show()\n",
    "\n",
    "            local_pca_results.update(prepend_dict({\n",
    "                \"explained_variance\": pca.explained_variance_ratio_,\n",
    "                \"samples\": samples,\n",
    "                \"explained_variance_last_half\": pca_last_half.explained_variance_ratio_,\n",
    "                \"samples_last_half\": samples_last_half\n",
    "            }, prefix=path, delimiter=\"/\"))\n",
    "\n",
    "\n",
    "        pca_results.append(local_pca_results)\n",
    "\n",
    "\n",
    "    return pca_results\n",
    "\n",
    "# pca_results = plot_pcas_over_models_over_checkpoints(configs[1:], checkpointers[1:])\n",
    "# pca_results = plot_pcas_over_models_over_checkpoints(configs[12:], checkpointers[12:], paths={path: \"token_sequence_transformer\"})\n",
    "pca_results = plot_pcas_over_models_over_checkpoints(configs, checkpointers, paths={\"ys\": \"\", \"y\": \"7.0\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLCTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from boto3.s3 import NoSuchKey\n",
    "from devinfra.evals import RepeatEvaluator\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def eval_rlct(model: nn.Module):\n",
    "    xs, ys = evaluator.pretrain_xs, evaluator.pretrain_ys\n",
    "    trainset = torch.utils.data.TensorDataset(xs, ys)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(xs))\n",
    "\n",
    "    optimizer_kwargs = dict(\n",
    "        lr=1e-5,\n",
    "        noise_level=1.0,\n",
    "        weight_decay=3e-7,\n",
    "        elasticity=10.0,\n",
    "        temperature=\"adaptive\",\n",
    "        num_samples=len(xs),\n",
    "    )\n",
    "    return {\n",
    "        \"rlct\": estimate_rlct(\n",
    "            model,\n",
    "            trainloader,\n",
    "            F.mse_loss,\n",
    "            \"sgld\",\n",
    "            optimizer_kwargs,\n",
    "            num_draws=20,\n",
    "            num_chains=8,\n",
    "            num_burnin_steps=0,\n",
    "            num_steps_bw_draws=1,\n",
    "            cores=8,\n",
    "            pbar=False,\n",
    "            device=\"xla\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "eval_rlcts = RepeatEvaluator(eval_rlct, 5)\n",
    "eval_rlcts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eval_models_at_step(step=-1):\n",
    "    evals = {}\n",
    "\n",
    "    for i, _checkpointer in enumerate(checkpointers):\n",
    "        checkpointer = _checkpointer.providers[0]\n",
    "        print(\"-\" * 20 + f\" M {2 ** i} \" + \"-\" * 20)\n",
    "\n",
    "        if step == -1:\n",
    "            step = checkpointer.file_ids[step]        \n",
    "\n",
    "        try:\n",
    "            model.load_state_dict(checkpointer.load_file(step)[\"model\"])  # Load last checkpoint\n",
    "        except Exception as e:\n",
    "            # TODO: Figure out where to find NoSuchKey\n",
    "            print(f\"Step {step} not found for checkpoint {i}. Skipping...\", e, step)\n",
    "            continue\n",
    "        \n",
    "        sampler = ICLSampler(\n",
    "            model, \n",
    "            evaluator.pretrain_xs, \n",
    "            evaluator.pretrain_ys,    \n",
    "            sampler_config\n",
    "        )\n",
    "\n",
    "        rlct_evaluator = SamplerEvaluator.create_rlct_evaluator(sampler)\n",
    "        _evals = []\n",
    "\n",
    "        def wipe():\n",
    "            nonlocal sampler\n",
    "            nonlocal rlct_evaluator\n",
    "\n",
    "            del sampler\n",
    "            del rlct_evaluator\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        try:\n",
    "\n",
    "            for _ in trange(10):\n",
    "                _evals.append(rlct_evaluator(model, None, None))\n",
    "                print(_evals[-1])\n",
    "\n",
    "            evals[2 ** i] = _evals\n",
    "            print(evals)\n",
    "        except Exception as e:\n",
    "            wipe()\n",
    "            raise e      \n",
    "\n",
    "    return evals\n",
    "\n",
    "evals = eval_models_at_step(81632)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keys = sorted(list(evals.keys()))\n",
    "means = [np.mean(evals[k]) for k in keys]\n",
    "std_devs = [np.std(evals[k]) for k in keys]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(keys, means, yerr=std_devs, fmt='o-', capsize=5, label=\"Evaluation Metrics\")\n",
    "plt.xlabel(\"Number of Tasks\")\n",
    "plt.ylabel(r\"$\\hat\\lambda$\")\n",
    "plt.title(\"Steps = 81632\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "all_steps = sorted(list(reduce(lambda x, y: x | y, [set(c.file_ids) for c in checkpointers], set())))\n",
    "all_steps = all_steps[::2]\n",
    "print(all_steps, len(all_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models_at_multiple_steps(steps):\n",
    "    evals = {}\n",
    "\n",
    "    for step in steps:\n",
    "        print(\"=\" * 20 + f\" Step {step} \" + \"=\" * 20)\n",
    "        evals[step] = eval_models_at_step(step)\n",
    "\n",
    "    return evals\n",
    "\n",
    "evals_over_time = eval_models_at_multiple_steps(all_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "line, = ax.plot([], [], 'o-', label=\"Evaluation Metrics\")\n",
    "ax.set_xlabel(\"Num tasks\")\n",
    "ax.set_ylabel(r\"$\\hat\\lambda$\")\n",
    "ax.set_title(\"RLCT estimates over time\")\n",
    "ax.grid(True)\n",
    "ax.legend()\n",
    "\n",
    "# Initialize data\n",
    "x_data = []\n",
    "y_data = []\n",
    "y_err = []\n",
    "\n",
    "# Update function for animation\n",
    "def update(step):\n",
    "    global x_data, y_data, y_err\n",
    "\n",
    "    evals = evals_over_time.get(step, {})\n",
    "    keys = sorted(list(evals.keys()))\n",
    "    means = [np.mean(evals[k]) for k in keys]\n",
    "    std_devs = [np.std(evals[k]) for k in keys]\n",
    "\n",
    "    x_data = keys\n",
    "    y_data = means\n",
    "    y_err = std_devs\n",
    "\n",
    "    line.set_data(x_data, y_data)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    ax.errorbar(x_data, y_data, yerr=y_err, fmt='o-', capsize=5, label=\"Evaluation Metrics (Step {})\".format(step))\n",
    "\n",
    "# Create animation\n",
    "ani = FuncAnimation(fig, update, frames=sorted(evals_over_time.keys()), repeat=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from devinterp.utils import flatten_dict\n",
    "\n",
    "def wandb_run_to_df(run):\n",
    "    # Assuming 'run' is a wandb.Api().run() object\n",
    "    config = flatten_dict(run.config, flatten_lists=True)\n",
    "    history = run.history()\n",
    "\n",
    "    for k, v in config.items():\n",
    "        history[k] = v\n",
    "\n",
    "    return pd.DataFrame.from_dict(history)\n",
    "\n",
    "def wandb_runs_to_df(*runs):\n",
    "    # Multiple wandb run objects\n",
    "    df_list = [wandb_run_to_df(run) for run in runs]\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def wandb_sweep_to_df(sweep):\n",
    "    # Assuming 'sweep' is a wandb.Api().sweep() object\n",
    "   return wandb_runs_to_df(*sweep.runs)\n",
    "\n",
    "def wandb_sweeps_to_df(*sweeps):\n",
    "    # Multiple wandb sweep objects\n",
    "    df_list = [wandb_sweep_to_df(sweep) for sweep in sweeps]\n",
    "    return pd.concat(df_list)\n",
    "\n",
    "def wandb_run_id_to_df(run_id: str):\n",
    "    api = wandb.Api()\n",
    "    run = api.run(f\"project_path/{run_id}\")  # Replace 'project_path' with your actual project path\n",
    "    return wandb_run_to_df(run)\n",
    "\n",
    "def wandb_sweep_id_to_df(sweep_id: str, entity=\"devinterp\", project=\"icl\"):\n",
    "    api = wandb.Api()\n",
    "    sweep = api.sweep(f\"{entity}/{project}/{sweep_id}\")  # Replace 'project_path' with your actual project path\n",
    "    return wandb_sweep_to_df(sweep)\n",
    "\n",
    "def wandb_sweep_ids_to_df(sweep_ids: str, entity=\"devinterp\", project=\"icl\"):\n",
    "    api = wandb.Api()\n",
    "    sweeps = [api.sweep(f\"{entity}/{project}/{sweep_id}\") for sweep_id in sweep_ids.split(\",\")]  # Replace 'project_path' with your actual project path\n",
    "    return wandb_sweeps_to_df(*sweeps)\n",
    "\n",
    "df = wandb_sweep_ids_to_df(\"xksoyrhb\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = df[\"analysis_config/lr\"].unique()\n",
    "num_draws = df[\"analysis_config/num_draws\"].unique()\n",
    "elasticities = df[\"analysis_config/elasticity\"].unique()\n",
    "\n",
    "lrs, num_draws, elasticities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "for (lr, _num_draws, elasticity) in product(lrs, num_draws, elasticities):\n",
    "    _df = df[\n",
    "        (df[\"analysis_config/lr\"] == lr) &\n",
    "        (df[\"analysis_config/num_draws\"] == _num_draws) &\n",
    "        (df[\"analysis_config/elasticity\"] == elasticity)\n",
    "    ]\n",
    "    _df = _df.sort_values(by=\"task_config/num_tasks\")  # Ensure data is sorted by x-values\n",
    "\n",
    "    # Values for plotting\n",
    "    x_vals = _df[\"task_config/num_tasks\"]\n",
    "    y_means = _df[\"rlct/mean\"]\n",
    "    y_stds = _df[\"rlct/std\"]\n",
    "\n",
    "    # Create a figure and a plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot mean values as a line\n",
    "    plt.plot(x_vals, y_means, 'o-', label=\"RLCTs\", linewidth=2)\n",
    "    \n",
    "    # Add shaded area for error\n",
    "    plt.fill_between(x_vals, y_means - y_stds, y_means + y_stds, color='gray', alpha=0.4)\n",
    "    \n",
    "    # Labels and scales\n",
    "    plt.xlabel(\"Number of Tasks\")\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel(r\"$\\hat\\lambda$\")\n",
    "    \n",
    "    plt.title(f\"$\\\\eta={lr}, n_\\\\mathrm{{draws}}={_num_draws}, \\\\gamma={elasticity}$\")\n",
    "\n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Initialize the figure\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "fig.tight_layout(pad=6.0)\n",
    "\n",
    "# Create a mapping for lrs and num_draws to the grid\n",
    "lr_to_row = {lr: i for i, lr in enumerate(sorted(set(lrs)))}\n",
    "num_draws_to_col = {nd: i for i, nd in enumerate(sorted(set(num_draws)))}\n",
    "\n",
    "for (lr, _num_draws, elasticity) in product(lrs, num_draws, elasticities):\n",
    "    _df = df[\n",
    "        (df[\"analysis_config/lr\"] == lr) &\n",
    "        (df[\"analysis_config/num_draws\"] == _num_draws) &\n",
    "        (df[\"analysis_config/elasticity\"] == elasticity)\n",
    "    ]\n",
    "    _df = _df.sort_values(by=\"task_config/num_tasks\")  # Ensure data is sorted by x-values\n",
    "    \n",
    "    # Select subplot\n",
    "    ax = axes[lr_to_row[lr], num_draws_to_col[_num_draws]]\n",
    "\n",
    "    # Values for plotting\n",
    "    x_vals = _df[\"task_config/num_tasks\"]\n",
    "    y_means = _df[\"rlct/mean\"]\n",
    "    y_stds = _df[\"rlct/std\"]\n",
    "\n",
    "    # Plot mean values as a line\n",
    "    ax.plot(x_vals, y_means, 'o-', label=f\"RLCTs, $\\gamma={elasticity}$\", linewidth=2)\n",
    "    \n",
    "    # Add shaded area for error\n",
    "    ax.fill_between(x_vals, y_means - y_stds, y_means + y_stds, color='gray', alpha=0.4)\n",
    "\n",
    "    # Labels and scales\n",
    "    ax.set_xlabel(\"Number of Tasks\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_ylabel(r\"$\\hat\\lambda$\")\n",
    "    \n",
    "    # Title for each subplot\n",
    "    ax.set_title(f\"$\\eta={lr}, n_\\mathrm{{draws}}={_num_draws}, \\gamma={elasticity}$\")\n",
    "    \n",
    "    # Show legend\n",
    "    ax.legend()\n",
    "\n",
    "# Show the complete figure\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
