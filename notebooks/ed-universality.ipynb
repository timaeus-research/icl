{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Essential dynamics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tqdm\n",
                "import seaborn as sns\n",
                "from sklearn.decomposition import PCA\n",
                "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
                "import matplotlib.patches as mpatches\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from matplotlib.patches import FancyArrowPatch\n",
                "from mpl_toolkits.mplot3d import proj3d\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from sklearn.decomposition import PCA\n",
                "from torch.nn import functional as F\n",
                "from sklearn.manifold import TSNE\n",
                "import gc\n",
                "import itertools\n",
                "from scipy.ndimage import gaussian_filter1d\n",
                "import plotly.graph_objects as go\n",
                "from plotly.subplots import make_subplots\n",
                "import plotly.offline as pyo\n",
                "import numpy as np\n",
                "import tqdm\n",
                "from infra.utils.iterables import int_linspace\n",
                "from copy import deepcopy\n",
                "from pathlib import Path\n",
                "\n",
                "# import sys\n",
                "# del sys.modules['icl.figures.colors']\n",
                "# del sys.modules['icl.figures.notation']\n",
                "\n",
                "from devinterp.slt.forms import get_osculating_circle\n",
                "from icl.analysis.utils import get_unique_run\n",
                "from icl.constants import ANALYSIS, FIGURES, SWEEPS, DATA\n",
                "from icl.figures.notation import str_d_dlogt, str_d_dt, str_dlog_dlogt\n",
                "from icl.figures.colors import (\n",
                "    plot_transitions,\n",
                "    gen_transition_colors,\n",
                "    get_transition_type,\n",
                "    PRIMARY,\n",
                "    SECONDARY,\n",
                "    TERTIARY,\n",
                "    BRED,\n",
                "    BBLUE,\n",
                "    BRED,\n",
                "    BGREEN,\n",
                ")\n",
                "from icl.constants import DEVICE\n",
                "\n",
                "# from devinterp.slt.forms import\n",
                "sns.set_style(\"white\")\n",
                "DEVICE\n",
                "\n",
                "NUM_TASKS = \"inf\"\n",
                "NUM_LAYERS = 2\n",
                "MAX_LR = 0.003\n",
                "MODEL_SEEDS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
                "\n",
                "steps = int_linspace(0, 500_000, 10_000)[::2]\n",
                "\n",
                "\n",
                "plt.rcParams[\"figure.dpi\"] = 300"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_models_and_optimizers(run, steps, model_id):\n",
                "    if os.path.exists(Path(\"../checkpoints\") / f\"{model_id}-models.pt\"):\n",
                "        print(\"Loading models from disk\")\n",
                "        models = torch.load(Path(\"../checkpoints\") / f\"{model_id}-models.pt\")\n",
                "        optimizer_state_dicts = torch.load(\n",
                "            Path(\"../checkpoints\") / f\"{model_id}-optimizer_state_dicts.pt\"\n",
                "        )\n",
                "\n",
                "    else:\n",
                "        print(\"Retrieving models from AWS\")\n",
                "        # Let's generate these same plots and also look at their evolution.\n",
                "        models = []\n",
                "        optimizer_state_dicts = []\n",
                "\n",
                "        for step in tqdm.tqdm(steps):\n",
                "            checkpoint = run.checkpointer.load_file(step)\n",
                "\n",
                "            m = deepcopy(run.model)\n",
                "            m.load_state_dict(checkpoint[\"model\"])\n",
                "            models.append(m)\n",
                "            optimizer_state_dicts.append(checkpoint[\"optimizer\"])\n",
                "\n",
                "        print(\"Saving models to disk\")\n",
                "        torch.save(models, Path(\"../checkpoints\") / f\"{model_id}-models.pt\")\n",
                "        torch.save(\n",
                "            optimizer_state_dicts,\n",
                "            Path(\"../checkpoints\") / f\"{model_id}-optimizer_state_dicts.pt\",\n",
                "        )\n",
                "    \n",
                "    return models, optimizer_state_dicts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "from icl.regression.model import to_token_sequence, from_predicted_token_sequence\n",
                "\n",
                "K = 16\n",
                "B = 1024\n",
                "D = 4\n",
                "\n",
                "def get_tokens(run, batch_size, max_examples, seed=0, include_x_and_y=False):\n",
                "    torch.manual_seed(seed)\n",
                "\n",
                "    xs, ys = run.pretrain_dist.get_batch(max_examples, batch_size, return_ws=False)\n",
                "    tokens = to_token_sequence(xs, ys)\n",
                "\n",
                "    if include_x_and_y:\n",
                "        return tokens, xs, ys\n",
                "\n",
                "    return tokens\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_y_outputs(models, xs, ys, model_id, force_reeval=False):\n",
                "    B, K, D = xs.shape\n",
                "\n",
                "    outputs = np.zeros((len(models), K * B), dtype=np.float32)\n",
                "\n",
                "    if not os.path.exists(DATA / f\"{model_id}-outputs-y-only.pkl\") or force_reeval:\n",
                "        print(\"Computing outputs\")\n",
                "        for i, model in enumerate(tqdm.tqdm(models, desc=\"Computing outputs\")):\n",
                "            with torch.no_grad():\n",
                "                output = model(xs, ys).flatten()\n",
                "                outputs[i, :] = output.cpu().numpy()\n",
                "\n",
                "        with open(DATA / f\"{model_id}-outputs-y-only.pkl\", \"wb\") as f:\n",
                "            pickle.dump(outputs, f)\n",
                "    else:\n",
                "        print(\"Loading y outputs from disk\")\n",
                "        with open(DATA / f\"{model_id}-outputs-y-only.pkl\", \"rb\") as f:\n",
                "            outputs = pickle.load(f)\n",
                "\n",
                "    return outputs\n",
                "\n",
                "def get_outputs(models, tokens, model_id, force_reeval=False):\n",
                "    B, K, D = tokens.shape\n",
                "    K = K // 2  \n",
                "    D = D - 1 \n",
                "\n",
                "    outputs = np.zeros((len(models), K * B * (D + 1) * 2), dtype=np.float32)\n",
                "\n",
                "    if not os.path.exists(DATA / f\"{model_id}-outputs.pkl\") or force_reeval:\n",
                "        print(\"Computing outputs\")\n",
                "        for i, model in enumerate(tqdm.tqdm(models, desc=\"Computing outputs\")):\n",
                "            with torch.no_grad():\n",
                "                output = model.token_sequence_transformer(tokens).flatten()\n",
                "                outputs[i, :] = output.cpu().numpy()\n",
                "\n",
                "        with open(DATA / f\"{model_id}-outputs.pkl\", \"wb\") as f:\n",
                "            pickle.dump(outputs, f)\n",
                "    else:\n",
                "        print(\"Loading outputs from disk\")\n",
                "        with open(DATA / f\"{model_id}-outputs.pkl\", \"rb\") as f:\n",
                "            outputs = pickle.load(f)\n",
                "\n",
                "    return outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_pca_and_reduced(outputs, model_id, n_components=30, force_reeval=False):\n",
                "    if not os.path.exists(DATA / f\"{model_id}-pca.pkl\") or force_reeval:\n",
                "        print(\"Computing PCA\")\n",
                "        pca = PCA(n_components=n_components)\n",
                "        pca.fit(outputs)\n",
                "        reduced = pca.transform(outputs)\n",
                "        with open(DATA / f\"{model_id}-pca.pkl\", \"wb\") as f:\n",
                "            pickle.dump((pca, reduced), f)\n",
                "    else:\n",
                "        print(\"Loading PCA from disk\")\n",
                "        with open(DATA / f\"{model_id}-pca.pkl\", \"rb\") as f:\n",
                "            pca, reduced = pickle.load(f)\n",
                "    return pca, reduced"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List, TypedDict\n",
                "import yaml\n",
                "\n",
                "class FormDict(TypedDict):\n",
                "    name: str\n",
                "    components: List[float]\n",
                "\n",
                "def get_forms(model_id) -> List[FormDict]:\n",
                "    if os.path.exists(DATA / f\"{model_id}-forms.yaml\"):\n",
                "        print(\"Loading forms from disk\")\n",
                "        with open(DATA / f\"{model_id}-forms.yaml\", \"r\") as f:\n",
                "            forms = yaml.safe_load(f)\n",
                "    else:\n",
                "        print(\"Computing forms\")\n",
                "        forms = []\n",
                "        \n",
                "    return forms"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.express as px\n",
                "from sklearn.decomposition import PCA\n",
                "import seaborn as sns\n",
                "\n",
                "cmap = sns.color_palette(\"Spectral\", as_cmap=True)\n",
                "color_indices = np.linspace(0, 1, len(steps))\n",
                "colors = np.array([cmap(c) for c in color_indices])\n",
                "\n",
                "def to_color_string(color):\n",
                "    # return (256 * color[0], 256 * color[1], 256 * color[2], color[3])\n",
                "    return f\"rgb({int(256 * color[0])}, {int(256 * color[1])}, {int(256 * color[2])}, {color[3]})\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_ed(pca, reduced, reduced_smooth, forms, model_id, form_cmap='rainbow', evolute_cmap='Spectral', num_components=3, title=\"\", slug=\"pca.html\"):\n",
                "    labels = {\n",
                "        str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
                "        for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
                "    }\n",
                "\n",
                "    subplot_titles = []\n",
                "    fig = make_subplots(rows=num_components, cols=num_components, subplot_titles=subplot_titles)\n",
                "\n",
                "    if isinstance(form_cmap, str):\n",
                "        form_cmap = sns.color_palette(form_cmap, as_cmap=True)\n",
                "    if isinstance(evolute_cmap, str):\n",
                "        evolute_cmap = sns.color_palette(evolute_cmap, as_cmap=True)\n",
                "\n",
                "    form_colors = np.array([to_color_string(form_cmap(c)) for c in np.linspace(0, 1, len(forms))])   \n",
                "    evolute_colors = np.array([to_color_string(evolute_cmap(c)) for c in np.linspace(0, 1, len(reduced_smooth)-4)])\n",
                "\n",
                "    for i, j in tqdm.tqdm(itertools.product(range(num_components), range(num_components)), total=num_components ** 2): \n",
                "        row, col = i + 1, j + 1\n",
                "            \n",
                "        ymin, ymax = (\n",
                "            reduced[:, i].min(),\n",
                "            reduced[:, i].max(),\n",
                "        )\n",
                "        xmin, xmax = (\n",
                "            reduced[:, j].min(),\n",
                "            reduced[:, j].max(),\n",
                "        )\n",
                "\n",
                "        # Forms\n",
                "        for f, form in enumerate(forms):\n",
                "            if form[j] is not None:\n",
                "                # Vertical line\n",
                "                fig.add_shape(\n",
                "                    type=\"line\",\n",
                "                    x0=form[j],\n",
                "                    y0=ymin * 1.25,\n",
                "                    x1=form[j],\n",
                "                    y1=ymax * 1.25,\n",
                "                    line=dict(color=form_colors[f], width=1),\n",
                "                    row=row,\n",
                "                    col=col,\n",
                "                )\n",
                "            if form[i] is not None:\n",
                "                # Horizontal line\n",
                "                fig.add_shape(\n",
                "                    type=\"line\",\n",
                "                    x0=xmin * 1.25,\n",
                "                    y0=form[i],\n",
                "                    x1=xmax * 1.25,\n",
                "                    y1=form[i],\n",
                "                    line=dict(color=form_colors[f], width=1),\n",
                "                    row=row,\n",
                "                    col=col,\n",
                "                )\n",
                "\n",
                "        ts = np.array(range(2, len(reduced_smooth) - 2))\n",
                "        centers = np.zeros((len(ts), 2))\n",
                "\n",
                "        # Circles\n",
                "        for ti, t in enumerate(ts):\n",
                "            center, radius = get_osculating_circle(\n",
                "                reduced_smooth[:, (j, i)], t\n",
                "            )\n",
                "            # if ti % 16 == 0:\n",
                "            #     # This seems to be cheaper than directly plotting a circle\n",
                "            #     circle = go.Scatter(\n",
                "            #         x=center[0] + radius * np.cos(np.linspace(0, 2 * np.pi, 100)),\n",
                "            #         y=center[1] + radius * np.sin(np.linspace(0, 2 * np.pi, 100)),\n",
                "            #         mode=\"lines\",\n",
                "            #         line=dict(color=\"rgba(0.1, 0.1, 1, 0.05)\", width=1),\n",
                "            #         showlegend=False,\n",
                "            #     )\n",
                "            #     fig.add_trace(circle, row=row, col=col)\n",
                "\n",
                "            centers[ti] = center\n",
                "\n",
                "        # Centers\n",
                "        fig.add_trace(\n",
                "            go.Scatter(\n",
                "                x=centers[:, 0],\n",
                "                y=centers[:, 1],\n",
                "                mode=\"markers\",\n",
                "                marker=dict(size=2, symbol=\"x\", color=evolute_colors),\n",
                "                name=\"Centers\",\n",
                "            ),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "        # Original samples\n",
                "        # fig.add_trace(\n",
                "        #     go.Scatter(\n",
                "        #         x=reduced[:, j],\n",
                "        #         y=reduced[:, i],\n",
                "        #         mode=\"markers\",\n",
                "        #         marker=dict(color=colors, size=3),\n",
                "        #         showlegend=False,\n",
                "        #     ),\n",
                "        #     row=row,\n",
                "        #     col=col,\n",
                "        # )\n",
                "\n",
                "        # Smoothed trajectory\n",
                "        fig.add_trace(\n",
                "            go.Scatter(\n",
                "                x=reduced_smooth[:, j],\n",
                "                y=reduced_smooth[:, i],\n",
                "                mode=\"lines\",\n",
                "                line=dict(color=\"black\", width=2),\n",
                "                showlegend=False,\n",
                "            ),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "        if j == 0:\n",
                "            fig.update_yaxes(title_text=labels[str(i)], row=row, col=col)\n",
                "\n",
                "        fig.update_xaxes(title_text=labels[str(j)], row=row, col=col)\n",
                "\n",
                "        fig.update_xaxes(\n",
                "            range=(xmin * 1.25, xmax * 1.25),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "        fig.update_yaxes(\n",
                "            range=(ymin * 1.25, ymax * 1.25),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "    fig.update_layout(width=2500, height=2500)  # Adjust the size as needed\n",
                "    fig.update_layout(title_text=title, showlegend=False)\n",
                "\n",
                "    # Save as html\n",
                "    pyo.plot(fig, filename=str(FIGURES / model_id / slug))\n",
                "    # fig.write_image(str(FIGURES / model_id / \"pca.png\"))\n",
                "\n",
                "    return fig"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": [
                "num_checkpoints_total = 5000\n",
                "num_downsample = 50\n",
                "num_checkpoints = num_checkpoints_total // num_downsample\n",
                "num_seeds = 10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/10 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retrieving run...\n",
                        "Retrieved run.\n",
                        "Loading models from disk\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/10 [00:33<?, ?it/s]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[56], line 33\u001b[0m\n\u001b[1;32m     22\u001b[0m run \u001b[38;5;241m=\u001b[39m get_unique_run(\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mstr\u001b[39m(SWEEPS \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression/training-runs/L2H4Minf.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     24\u001b[0m     task_config\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     optimizer_config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: MAX_LR},\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved run.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m models, optimizer_state_dicts \u001b[38;5;241m=\u001b[39m \u001b[43mget_models_and_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m tokens, xs, ys \u001b[38;5;241m=\u001b[39m get_tokens(run, B, K, seed\u001b[38;5;241m=\u001b[39mTOKENS_SEED, include_x_and_y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens generated from seed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOKENS_SEED\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mget_models_and_optimizers\u001b[0;34m(run, steps, model_id)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-models.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading models from disk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     models \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-models.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     optimizer_state_dicts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m      6\u001b[0m         Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-optimizer_state_dicts.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "File \u001b[0;32m~/Projects/icl/.venv/lib/python3.9/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
                        "File \u001b[0;32m~/Projects/icl/.venv/lib/python3.9/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "File \u001b[0;32m~/Projects/icl/.venv/lib/python3.9/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
                        "File \u001b[0;32m~/Projects/icl/.venv/lib/python3.9/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
                        "File \u001b[0;32m~/Projects/icl/.venv/lib/python3.9/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "File \u001b[0;32m~/Projects/icl/.venv/lib/python3.9/site-packages/torch/serialization.py:192\u001b[0m, in \u001b[0;36m_mps_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mps_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/Projects/icl/.venv/lib/python3.9/site-packages/torch/storage.py:128\u001b[0m, in \u001b[0;36m_StorageBase.mps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# steps\n",
                "\n",
                "from icl.analysis.smoothing import gaussian_filter1d_variable_sigma\n",
                "\n",
                "TOKENS_SEED = 0\n",
                "\n",
                "num_checkpoints_total = 5000\n",
                "num_downsample = 10\n",
                "num_checkpoints = num_checkpoints_total // num_downsample\n",
                "num_seeds = 10\n",
                "\n",
                "combined_outputs = np.zeros((num_seeds * num_checkpoints, 81920))\n",
                "combined_y_outputs = np.zeros((num_seeds * num_checkpoints, 8192 * 2))\n",
                "\n",
                "for model_seed in tqdm.tqdm(MODEL_SEEDS):\n",
                "    model_id = f\"L2H4Minf{model_seed}\"\n",
                "\n",
                "    os.makedirs(str(FIGURES / model_id), exist_ok=True)\n",
                "    os.makedirs(str(DATA / model_id), exist_ok=True)\n",
                "\n",
                "    print(\"Retrieving run...\")\n",
                "    run = get_unique_run(\n",
                "        str(SWEEPS / \"regression/training-runs/L2H4Minf.yaml\"),\n",
                "        task_config={\n",
                "            \"num_tasks\": NUM_TASKS,\n",
                "            \"num_layers\": NUM_LAYERS,\n",
                "            \"model_seed\": model_seed,\n",
                "        },\n",
                "        optimizer_config={\"lr\": MAX_LR},\n",
                "    )\n",
                "    print(\"Retrieved run.\")\n",
                "\n",
                "    models, optimizer_state_dicts = get_models_and_optimizers(run, steps, model_id)\n",
                "    tokens, xs, ys = get_tokens(run, B, K, seed=TOKENS_SEED, include_x_and_y=True)\n",
                "    print(f\"Tokens generated from seed {TOKENS_SEED} with shape {tokens.shape}\")\n",
                "\n",
                "    outputs, y_outputs = get_outputs(models, tokens, model_id, force_reeval=False), get_y_outputs(models, xs, ys, model_id, force_reeval=False)\n",
                "\n",
                "    print(f\"Outputs shape: {outputs.shape}\")\n",
                "    combined_outputs[model_seed * num_checkpoints: (model_seed + 1) * num_checkpoints, :] = outputs[::num_downsample, :]\n",
                "    combined_y_outputs[model_seed * num_checkpoints: (model_seed + 1) * num_checkpoints, :] = y_outputs[::num_downsample, :]\n",
                "    # pca, reduced = get_pca_and_reduced(outputs, model_id, n_components=30, force_reeval=False)\n",
                "\n",
                "    # start, end = 0.1, 300\n",
                "    # reduced_smooth = gaussian_filter1d_variable_sigma(reduced, np.linspace(start, end, len(reduced)), axis=0)\n",
                "\n",
                "    # forms = get_forms(model_id)\n",
                "    # num_forms = len(forms)\n",
                "    # form_cmap = sns.color_palette(\"rainbow\", as_cmap=True)\n",
                "\n",
                "    # fig = plot_ed(pca, reduced, reduced_smooth, forms, model_id, num_components=8, title=model_id)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/10 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 10%|█         | 1/10 [00:00<00:06,  1.46it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 20%|██        | 2/10 [00:01<00:06,  1.21it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Computing outputs\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Computing outputs: 100%|██████████| 5000/5000 [00:34<00:00, 143.46it/s]\n",
                        " 30%|███       | 3/10 [00:37<01:57, 16.79s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Outputs shape: (5000, 81920)\n",
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 40%|████      | 4/10 [00:38<01:02, 10.46s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 50%|█████     | 5/10 [00:38<00:34,  6.95s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 60%|██████    | 6/10 [00:41<00:21,  5.31s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 70%|███████   | 7/10 [00:42<00:11,  3.97s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 80%|████████  | 8/10 [00:43<00:06,  3.22s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 90%|█████████ | 9/10 [00:44<00:02,  2.50s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n"
                    ]
                }
            ],
            "source": [
                "# steps\n",
                "\n",
                "from icl.analysis.smoothing import gaussian_filter1d_variable_sigma\n",
                "\n",
                "TOKENS_SEED = 0\n",
                "\n",
                "num_checkpoints_total = 5000\n",
                "num_downsample = 10\n",
                "num_checkpoints = num_checkpoints_total // num_downsample\n",
                "num_seeds = 10\n",
                "\n",
                "combined_outputs = np.zeros((num_seeds * num_checkpoints, 81920))\n",
                "combined_y_outputs = np.zeros((num_seeds * num_checkpoints, 8192 * 2))\n",
                "\n",
                "for model_seed in tqdm.tqdm(MODEL_SEEDS):\n",
                "    model_id = f\"L2H4Minf{model_seed}\"\n",
                "\n",
                "    os.makedirs(str(FIGURES / model_id), exist_ok=True)\n",
                "    os.makedirs(str(DATA / model_id), exist_ok=True)\n",
                "\n",
                "    tokens, xs, ys = get_tokens(run, B, K, seed=TOKENS_SEED, include_x_and_y=True)\n",
                "    print(f\"Tokens generated from seed {TOKENS_SEED} with shape {tokens.shape}\")\n",
                "\n",
                "    outputs, y_outputs = get_outputs(models, tokens, model_id, force_reeval=False), get_y_outputs(models, xs, ys, model_id, force_reeval=False)\n",
                "\n",
                "    print(f\"Outputs shape: {outputs.shape}\")\n",
                "    combined_outputs[model_seed * num_checkpoints: (model_seed + 1) * num_checkpoints, :] = outputs[::num_downsample, :]\n",
                "    combined_y_outputs[model_seed * num_checkpoints: (model_seed + 1) * num_checkpoints, :] = y_outputs[::num_downsample, :]\n",
                "    # pca, reduced = get_pca_and_reduced(outputs, model_id, n_components=30, force_reeval=False)\n",
                "\n",
                "    # start, end = 0.1, 300\n",
                "    # reduced_smooth = gaussian_filter1d_variable_sigma(reduced, np.linspace(start, end, len(reduced)), axis=0)\n",
                "\n",
                "    # forms = get_forms(model_id)\n",
                "    # num_forms = len(forms)\n",
                "    # form_cmap = sns.color_palette(\"rainbow\", as_cmap=True)\n",
                "\n",
                "    # fig = plot_ed(pca, reduced, reduced_smooth, forms, model_id, num_components=8, title=model_id)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing Combined PCA\n"
                    ]
                }
            ],
            "source": [
                "if not os.path.exists(DATA / f\"combined-pca.pkl\") or True:\n",
                "    print(\"Computing Combined PCA\")\n",
                "    pca = PCA(n_components=8)\n",
                "    pca.fit(combined_outputs)\n",
                "    reduced = pca.transform(combined_outputs)\n",
                "    with open(DATA / f\"{model_id}-pca.pkl\", \"wb\") as f:\n",
                "        pickle.dump((pca, reduced), f)\n",
                "else:\n",
                "    print(\"Loading PCA from disk\")\n",
                "    with open(DATA / f\"combined-pca.pkl\", \"rb\") as f:\n",
                "        pca, reduced = pickle.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "((1000, 5), (5000, 5))"
                        ]
                    },
                    "execution_count": 33,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "reduced.shap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 64/64 [00:00<00:00, 204.63it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 251.47it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 251.16it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 248.12it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 251.81it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 257.36it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 259.82it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 260.65it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 256.20it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 256.25it/s]\n"
                    ]
                }
            ],
            "source": [
                "start, end = 0.1 / num_downsample, 300 / num_downsample\n",
                "reduced_smooth = np.zeros_like(reduced)\n",
                "\n",
                "for i in range(10):\n",
                "    reduced_smooth[i * num_checkpoints: (i + 1) * num_checkpoints] = gaussian_filter1d_variable_sigma(reduced[i * num_checkpoints: (i + 1) * num_checkpoints], np.linspace(start, end, num_checkpoints), axis=0)\n",
                "\n",
                "num_components = 8\n",
                "\n",
                "evolute_cmap = ''\n",
                "fig = make_subplots(rows=num_components, cols=num_components, subplot_titles=subplot_titles)\n",
                "\n",
                "for s in range(num_seeds):\n",
                "    _reduced = reduced[s * num_checkpoints: (s + 1) * num_checkpoints]\n",
                "    _reduced_smooth = reduced_smooth[s * num_checkpoints: (s + 1) * num_checkpoints]\n",
                "    color = sns.color_palette(\"Spectral\", as_cmap=True)(s / num_seeds)\n",
                "    color = to_color_string(color)\n",
                "\n",
                "    labels = {\n",
                "        str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
                "        for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
                "    }\n",
                "\n",
                "    subplot_titles = []\n",
                "    \n",
                "    for i, j in tqdm.tqdm(itertools.product(range(num_components), range(num_components)), total=num_components ** 2): \n",
                "        row, col = i + 1, j + 1\n",
                "            \n",
                "        ymin, ymax = (\n",
                "            reduced[:, i].min(),\n",
                "            reduced[:, i].max(),\n",
                "        )\n",
                "        xmin, xmax = (\n",
                "            reduced[:, j].min(),\n",
                "            reduced[:, j].max(),\n",
                "        )\n",
                "\n",
                "        ts = np.array(range(2, len(_reduced_smooth) - 2))\n",
                "        centers = np.zeros((len(ts), 2))\n",
                "\n",
                "        # Original samples\n",
                "        # fig.add_trace(\n",
                "        #     go.Scatter(\n",
                "        #         x=reduced[:, j],\n",
                "        #         y=reduced[:, i],\n",
                "        #         mode=\"markers\",\n",
                "        #         marker=dict(color=colors, size=3),\n",
                "        #         showlegend=False,\n",
                "        #     ),\n",
                "        #     row=row,\n",
                "        #     col=col,\n",
                "        # )\n",
                "\n",
                "        # Smoothed trajectory\n",
                "        fig.add_trace(\n",
                "            go.Scatter(\n",
                "                x=_reduced_smooth[:, j],\n",
                "                y=_reduced_smooth[:, i],\n",
                "                mode=\"lines\",\n",
                "                line=dict(color=color, width=2),\n",
                "                showlegend=False,\n",
                "            ),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "        fig.add_trace(\n",
                "            go.Scatter(\n",
                "                x=_reduced_smooth[:1, j],\n",
                "                y=_reduced_smooth[:1, i],\n",
                "                mode=\"markers\",\n",
                "                marker=dict(color=color),\n",
                "                showlegend=False,\n",
                "            ),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "\n",
                "        if j == 0:\n",
                "            fig.update_yaxes(title_text=labels[str(i)], row=row, col=col)\n",
                "\n",
                "        fig.update_xaxes(title_text=labels[str(j)], row=row, col=col)\n",
                "\n",
                "        fig.update_xaxes(\n",
                "            range=(xmin * 1.25, xmax * 1.25),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "        fig.update_yaxes(\n",
                "            range=(ymin * 1.25, ymax * 1.25),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "    fig.update_layout(width=2500, height=2500)  # Adjust the size as needed\n",
                "    fig.update_layout(title_text=f\"Combined LR PCA ({num_seeds} seeds, {num_checkpoints} checkpoints per seed, all outputs)\", showlegend=False)\n",
                "\n",
                "# Save as html\n",
                "pyo.plot(fig, filename=str(FIGURES / \"combined-pca.html\"))\n",
                "fig.write_image(str(FIGURES / \"combined-pca.png\"))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Computing Combined PCA\n"
                    ]
                }
            ],
            "source": [
                "if not os.path.exists(DATA / f\"combined-pca-y-only.pkl\") or True:\n",
                "    print(\"Computing Combined PCA\")\n",
                "    y_pca = PCA(n_components=8)\n",
                "    y_pca.fit(combined_y_outputs)\n",
                "    y_reduced = y_pca.transform(combined_y_outputs)\n",
                "    with open(DATA / f\"{model_id}-pca-y-only.pkl\", \"wb\") as f:\n",
                "        pickle.dump((y_pca, y_reduced), f)\n",
                "else:\n",
                "    print(\"Loading PCA from disk\")\n",
                "    with open(DATA / f\"combined-pca-y-only.pkl\", \"rb\") as f:\n",
                "        y_pca, y_reduced = pickle.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 64/64 [00:00<00:00, 232.62it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 253.09it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 255.75it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 254.92it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 259.14it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 258.83it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 259.73it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 262.66it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 255.99it/s]\n",
                        "100%|██████████| 64/64 [00:00<00:00, 244.48it/s]\n"
                    ]
                }
            ],
            "source": [
                "start, end = 0.1 / num_downsample, 300 / num_downsample\n",
                "y_reduced_smooth = np.zeros_like(y_reduced)\n",
                "\n",
                "for i in range(10):\n",
                "    y_reduced_smooth[i * num_checkpoints: (i + 1) * num_checkpoints] = gaussian_filter1d_variable_sigma(y_reduced[i * num_checkpoints: (i + 1) * num_checkpoints], np.linspace(start, end, num_checkpoints), axis=0)\n",
                "\n",
                "num_components = 8\n",
                "\n",
                "evolute_cmap = ''\n",
                "fig = make_subplots(rows=num_components, cols=num_components, subplot_titles=subplot_titles)\n",
                "\n",
                "for s in range(num_seeds):\n",
                "    _y_reduced = y_reduced[s * num_checkpoints: (s + 1) * num_checkpoints]\n",
                "    _y_reduced_smooth = y_reduced_smooth[s * num_checkpoints: (s + 1) * num_checkpoints]\n",
                "    color = sns.color_palette(\"Spectral\", as_cmap=True)(s / num_seeds)\n",
                "    color = to_color_string(color)\n",
                "\n",
                "    labels = {\n",
                "        str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
                "        for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
                "    }\n",
                "\n",
                "    subplot_titles = []\n",
                "    \n",
                "    for i, j in tqdm.tqdm(itertools.product(range(num_components), range(num_components)), total=num_components ** 2): \n",
                "        row, col = i + 1, j + 1\n",
                "            \n",
                "        ymin, ymax = (\n",
                "            y_reduced[:, i].min(),\n",
                "            y_reduced[:, i].max(),\n",
                "        )\n",
                "        xmin, xmax = (\n",
                "            y_reduced[:, j].min(),\n",
                "            y_reduced[:, j].max(),\n",
                "        )\n",
                "\n",
                "        ts = np.array(range(2, len(_y_reduced_smooth) - 2))\n",
                "        centers = np.zeros((len(ts), 2))\n",
                "\n",
                "        # Original samples\n",
                "        # fig.add_trace(\n",
                "        #     go.Scatter(\n",
                "        #         x=y_reduced[:, j],\n",
                "        #         y=y_reduced[:, i],\n",
                "        #         mode=\"markers\",\n",
                "        #         marker=dict(color=colors, size=3),\n",
                "        #         showlegend=False,\n",
                "        #     ),\n",
                "        #     row=row,\n",
                "        #     col=col,\n",
                "        # )\n",
                "\n",
                "        # Smoothed trajectory\n",
                "        fig.add_trace(\n",
                "            go.Scatter(\n",
                "                x=_y_reduced_smooth[:, j],\n",
                "                y=_y_reduced_smooth[:, i],\n",
                "                mode=\"lines\",\n",
                "                line=dict(color=color, width=2),\n",
                "                showlegend=False,\n",
                "            ),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "        fig.add_trace(\n",
                "            go.Scatter(\n",
                "                x=_y_reduced_smooth[:1, j],\n",
                "                y=_y_reduced_smooth[:1, i],\n",
                "                mode=\"markers\",\n",
                "                marker=dict(color=color),\n",
                "                showlegend=False,\n",
                "            ),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "        if j == 0:\n",
                "            fig.update_yaxes(title_text=labels[str(i)], row=row, col=col)\n",
                "\n",
                "        fig.update_xaxes(title_text=labels[str(j)], row=row, col=col)\n",
                "\n",
                "        fig.update_xaxes(\n",
                "            range=(xmin * 1.25, xmax * 1.25),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "        fig.update_yaxes(\n",
                "            range=(ymin * 1.25, ymax * 1.25),\n",
                "            row=row,\n",
                "            col=col,\n",
                "        )\n",
                "\n",
                "    fig.update_layout(width=2500, height=2500)  # Adjust the size as needed\n",
                "    fig.update_layout(title_text=f\"Combined LR PCA ({num_seeds} seeds, {num_checkpoints} checkpoints per seed, all outputs)\", showlegend=False)\n",
                "\n",
                "# Save as html\n",
                "pyo.plot(fig, filename=str(FIGURES / \"combined-pca-y-only.html\"))\n",
                "fig.write_image(str(FIGURES / \"combined-pca-y-only.png\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Finite Ms"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/6 [00:00<?, ?it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 163840), (5000, 16384)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 17%|█▋        | 1/6 [00:01<00:05,  1.20s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 163840), (5000, 16384)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 33%|███▎      | 2/6 [00:02<00:04,  1.23s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 163840), (5000, 16384)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 50%|█████     | 3/6 [00:04<00:05,  1.69s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 163840), (5000, 16384)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 67%|██████▋   | 4/6 [00:06<00:03,  1.62s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n",
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 163840), (5000, 16384)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 83%|████████▎ | 5/6 [00:07<00:01,  1.53s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokens generated from seed 0 with shape torch.Size([1024, 32, 5])\n",
                        "Loading outputs from disk\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        " 83%|████████▎ | 5/6 [00:08<00:01,  1.65s/it]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading outputs from disk\n",
                        "Outputs shape: (5000, 81920), (5000, 16384)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                },
                {
                    "ename": "ValueError",
                    "evalue": "could not broadcast input array from shape (500,81920) into shape (500,163840)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[79], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m outputs, y_outputs \u001b[38;5;241m=\u001b[39m get_outputs(models, tokens, model_id, force_reeval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), get_y_outputs(models, xs, ys, model_id, force_reeval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputs shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_outputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mcombined_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_checkpoints\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_checkpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m outputs[::num_downsample, :]\n\u001b[1;32m     55\u001b[0m combined_y_outputs[i \u001b[38;5;241m*\u001b[39m num_checkpoints: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m num_checkpoints, :] \u001b[38;5;241m=\u001b[39m y_outputs[::num_downsample, :]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# pca, reduced = get_pca_and_reduced(outputs, model_id, n_components=30, force_reeval=False)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# start, end = 0.1, 300\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# fig = plot_ed(pca, reduced, reduced_smooth, forms, model_id, num_components=8, title=model_id)\u001b[39;00m\n",
                        "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (500,81920) into shape (500,163840)"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "from icl.analysis.smoothing import gaussian_filter1d_variable_sigma\n",
                "\n",
                "MODEL_SEED = 0\n",
                "NUM_TASKS = list(2 ** np.arange(0, 21))\n",
                "TOKENS_SEED = 0\n",
                "\n",
                "num_checkpoints_total = 5000\n",
                "num_downsample = 10\n",
                "num_checkpoints = num_checkpoints_total // num_downsample\n",
                "num_diff_tasks = len(NUM_TASKS)\n",
                "\n",
                "combined_outputs = np.zeros((num_diff_tasks * num_checkpoints, 81920 * 2))\n",
                "combined_y_outputs = np.zeros((num_diff_tasks * num_checkpoints, 8192 * 2))\n",
                "\n",
                "for i, num_tasks in enumerate(tqdm.tqdm(NUM_TASKS[:5] + ['inf'])):\n",
                "    model_id = f\"L2H4M{num_tasks}\"\n",
                "\n",
                "    if num_tasks == 'inf':\n",
                "        model_id += '0'\n",
                "\n",
                "\n",
                "    os.makedirs(str(FIGURES / model_id), exist_ok=True)\n",
                "    os.makedirs(str(DATA / model_id), exist_ok=True)\n",
                "    \n",
                "    # if num_tasks == 'inf':\n",
                "    #     run = get_unique_run(\n",
                "    #         str(SWEEPS / \"regression/training-runs/L2H4Minf.yaml\"),\n",
                "    #         task_config={\n",
                "    #             \"num_layers\": NUM_LAYERS,\n",
                "    #             \"model_seed\": MODEL_SEED,\n",
                "    #         },\n",
                "    #         optimizer_config={\"lr\": MAX_LR},\n",
                "    #     )\n",
                "    # else:\n",
                "    #     run = get_unique_run(\n",
                "    #         str(SWEEPS / \"regression/training-runs/L2H4Mfin.yaml\"),\n",
                "    #         task_config={\n",
                "    #             \"num_tasks\": num_tasks,\n",
                "    #             \"num_layers\": NUM_LAYERS,\n",
                "    #             \"model_seed\": MODEL_SEED,\n",
                "    #         },\n",
                "    #         optimizer_config={\"lr\": MAX_LR},\n",
                "    #     )\n",
                "    # print(\"Retrieved run.\")\n",
                "\n",
                "    # models, optimizer_state_dicts = get_models_and_optimizers(run, steps, model_id)\n",
                "\n",
                "    tokens, xs, ys = get_tokens(run, B, K, seed=TOKENS_SEED, include_x_and_y=True)\n",
                "    print(f\"Tokens generated from seed {TOKENS_SEED} with shape {tokens.shape}\")\n",
                "\n",
                "    outputs, y_outputs = get_outputs(models, tokens, model_id, force_reeval=False), get_y_outputs(models, xs, ys, model_id, force_reeval=False)\n",
                "\n",
                "    print(f\"Outputs shape: {outputs.shape}, {y_outputs.shape}\")\n",
                "    combined_outputs[i * num_checkpoints: (i + 1) * num_checkpoints, :] = outputs[::num_downsample, :]\n",
                "    combined_y_outputs[i * num_checkpoints: (i + 1) * num_checkpoints, :] = y_outputs[::num_downsample, :]\n",
                "    # pca, reduced = get_pca_and_reduced(outputs, model_id, n_components=30, force_reeval=False)\n",
                "\n",
                "    # start, end = 0.1, 300\n",
                "    # reduced_smooth = gaussian_filter1d_variable_sigma(reduced, np.linspace(start, end, len(reduced)), axis=0)\n",
                "\n",
                "    # forms = get_forms(model_id)\n",
                "    # num_forms = len(forms)\n",
                "    # form_cmap = sns.color_palette(\"rainbow\", as_cmap=True)\n",
                "\n",
                "    # fig = plot_ed(pca, reduced, reduced_smooth, forms, model_id, num_components=8, title=model_id)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.15"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
