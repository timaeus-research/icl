{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import datasets\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import HookedTransformerConfig\n",
    "from transformer_lens.utils import lm_cross_entropy_loss\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.language.model import get_model_cfg\n",
    "from icl.language.utils import load_hf_checkpoint\n",
    "\n",
    "model_cfgs = {}\n",
    "model_cfgs[1] = get_model_cfg(num_layers=1)\n",
    "model_cfgs[2] = get_model_cfg(num_layers=2)\n",
    "\n",
    "model = HookedTransformer(model_cfgs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'oknMswoztTPaAVreBrWy/dsir-pile-100k'\n",
    "dataset_col_name = 'contents'\n",
    "\n",
    "dataset = datasets.load_dataset(dataset_name,\n",
    "                                split='train')\n",
    "tokens_dataset = tokenize_and_concatenate(dataset,\n",
    "                                         model.tokenizer,\n",
    "                                         streaming=False,\n",
    "                                         max_length=model.cfg.n_ctx,\n",
    "                                         column_name=dataset_col_name,\n",
    "                                         add_bos_token=True,\n",
    "                                         num_proc=12)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(tokens_dataset,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=False)\n",
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self):\n",
    "        self.sample = []\n",
    "        self.special_token = None\n",
    "        self.first_idx = None\n",
    "        self.second_idx = None\n",
    "        self.attn_idx = None\n",
    "\n",
    "\n",
    "def generate_synthetic_prefix_matching_sample(context_length, d_vocab):\n",
    "    bos_token = d_vocab - 1\n",
    "    special_token = random.randint(0, d_vocab - 2)\n",
    "\n",
    "    # choose locations for the first and second instances of the special token\n",
    "    first_idx = random.randint(1, context_length // 2 - 1)\n",
    "    second_idx = random.randint(context_length // 2 + 1, context_length - 1)\n",
    "    # this is where we want our head to attend to\n",
    "    attn_idx = first_idx + 1\n",
    "\n",
    "    sample = Sample()\n",
    "    for idx in range(context_length):\n",
    "        if idx == 0:\n",
    "            sample.sample.append(bos_token)\n",
    "        elif idx == first_idx or idx == second_idx:\n",
    "            sample.sample.append(special_token)\n",
    "        else:\n",
    "            # draw until you don't get the special token\n",
    "            rand_token = special_token\n",
    "            while rand_token == special_token:\n",
    "                rand_token = random.randint(0, d_vocab - 2)\n",
    "            sample.sample.append(rand_token)\n",
    "\n",
    "    sample.sample = torch.unsqueeze(torch.tensor(sample.sample, dtype=torch.int64), 0)\n",
    "    sample.special_token = special_token\n",
    "    sample.first_idx = first_idx\n",
    "    sample.second_idx = second_idx\n",
    "    sample.attn_idx = attn_idx\n",
    "    return sample\n",
    "\n",
    "\n",
    "def get_prefix_attns(model, n_layers, sample):\n",
    "    '''Assumes batch size 1'''\n",
    "    _, cache = model.run_with_cache(sample.sample)\n",
    "    prefix_attns = [[] for _ in range(n_layers)]\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        layer_cache = cache[f'blocks.{layer}.attn.hook_pattern'][0]\n",
    "        for attn_head in layer_cache:\n",
    "            attn_pattern = attn_head[sample.second_idx]\n",
    "            prefix_attn = attn_pattern[sample.attn_idx]\n",
    "            prefix_attns[layer].append(prefix_attn.cpu().item())\n",
    "    return prefix_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "SEED = 0\n",
    "D_VOCAB = 5000\n",
    "NUM_SAMPLES = 10000\n",
    "SAMPLE_LEN = 128\n",
    "\n",
    "MAX_CHECKPOINT = 50000\n",
    "\n",
    "def get_checkpoint_prefix_score(step, n_layers, d_vocab, num_samples, sample_len):\n",
    "    model = load_hf_checkpoint(step, n_layers=n_layers)\n",
    "    random.seed(SEED)\n",
    "    all_prefix_attns = []\n",
    "    for _ in tqdm(range(num_samples)):\n",
    "        sample = generate_synthetic_prefix_matching_sample(sample_len, d_vocab)\n",
    "        prefix_attns = get_prefix_attns(model, n_layers, sample)\n",
    "        all_prefix_attns.append(prefix_attns)\n",
    "    all_prefix_attns = np.array(all_prefix_attns)\n",
    "    return all_prefix_attns.mean(axis=0)\n",
    "\n",
    "L1_prefix_scores = []\n",
    "L2_prefix_scores = []\n",
    "for step in range(0, MAX_CHECKPOINT + 1, 100):\n",
    "    L1_scores = get_checkpoint_prefix_score(step, 1, D_VOCAB, NUM_SAMPLES, SAMPLE_LEN)\n",
    "    L1_prefix_scores.append(L1_scores)\n",
    "    L2_scores = get_checkpoint_prefix_score(step, 2, D_VOCAB, NUM_SAMPLES, SAMPLE_LEN)\n",
    "    L2_prefix_scores.append(L2_scores)\n",
    "\n",
    "L1_prefix_scores = np.array(L1_prefix_scores)\n",
    "L2_prefix_scores = np.array(L2_prefix_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous token matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self):\n",
    "        self.sample = []\n",
    "\n",
    "def generate_synthetic_prev_matching_sample(context_length, d_vocab):\n",
    "    bos_token = d_vocab - 1\n",
    "\n",
    "    sample_len = random.randint(context_length // 4, context_length)\n",
    "    sample = [bos_token] + [random.randint(0, d_vocab - 2) for _ in range(sample_len - 1)]\n",
    "    return sample\n",
    "\n",
    "def get_prev_attns(model, n_layers, sample):\n",
    "    '''Assumes batch size 1'''\n",
    "    sample = torch.tensor(sample, device=model.cfg.device)\n",
    "    _, cache = model.run_with_cache(sample)\n",
    "    prev_attns = [[] for _ in range(n_layers)]\n",
    "    for layer in range(n_layers):\n",
    "        layer_cache = cache[f'blocks.{layer}.attn.hook_pattern'][0]\n",
    "        for attn_head in layer_cache:\n",
    "            attn_pattern = attn_head[-1]\n",
    "            prev_attn = attn_pattern[-2]\n",
    "            prev_attns[layer].append(prev_attn.cpu().item())\n",
    "    return prev_attns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "D_VOCAB = 5000\n",
    "NUM_SAMPLES = 10_000\n",
    "MAX_SAMPLE_LEN = 64\n",
    "\n",
    "MAX_CHECKPOINT = 50_000\n",
    "\n",
    "def get_checkpoint_prev_score(step, n_layers):\n",
    "    model = load_hf_checkpoint(step, n_layers=n_layers)\n",
    "    random.seed(0)\n",
    "    all_prev_attns = []\n",
    "    for _ in tqdm(range(NUM_SAMPLES)):\n",
    "        sample = generate_synthetic_prev_matching_sample(MAX_SAMPLE_LEN, D_VOCAB)\n",
    "        prev_attns = get_prev_attns(model, n_layers, sample)\n",
    "        all_prev_attns.append(prev_attns)\n",
    "    all_prev_attns = np.array(all_prev_attns)\n",
    "    return all_prev_attns.mean(axis=0)\n",
    "\n",
    "L1_prev_scores = []\n",
    "L2_prev_scores = []\n",
    "x_steps = list(range(0, MAX_CHECKPOINT + 1, 100))\n",
    "\n",
    "for step in x_steps:\n",
    "    L1_scores = get_checkpoint_prev_score(step, n_layers=1)\n",
    "    L1_prev_scores.append(L1_scores)\n",
    "    L2_scores = get_checkpoint_prev_score(step, n_layers=2)\n",
    "    L2_prev_scores.append(L2_scores)\n",
    "\n",
    "L1_prefix_scores = np.array(L1_prefix_scores)\n",
    "L2_prefix_scores = np.array(L2_prefix_scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
