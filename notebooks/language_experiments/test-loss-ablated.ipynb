{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import datasets\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import HookedTransformerConfig\n",
    "from transformer_lens.utils import lm_cross_entropy_loss\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.language.model import get_model_cfg\n",
    "from icl.language.utils import load_hf_checkpoint\n",
    "\n",
    "model_cfgs = {}\n",
    "model_cfgs[1] = get_model_cfg(num_layers=1)\n",
    "model_cfgs[2] = get_model_cfg(num_layers=2)\n",
    "\n",
    "model = HookedTransformer(model_cfgs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'oknMswoztTPaAVreBrWy/dsir-pile-100k'\n",
    "dataset_col_name = 'contents'\n",
    "\n",
    "dataset = datasets.load_dataset(dataset_name,\n",
    "                                split='train')\n",
    "tokens_dataset = tokenize_and_concatenate(dataset,\n",
    "                                         model.tokenizer,\n",
    "                                         streaming=False,\n",
    "                                         max_length=model.cfg.n_ctx,\n",
    "                                         column_name=dataset_col_name,\n",
    "                                         add_bos_token=True,\n",
    "                                         num_proc=12)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(tokens_dataset,\n",
    "                                          batch_size=32,\n",
    "                                          shuffle=False)\n",
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_POSITIONS = [0, 1, 2, 3, 4, 5, 6, 7, 10, 20, 30, 50, 100, 200, 300, 500, 1000]\n",
    "NUM_BATCHES = 313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from collections import defaultdict\n",
    "from transformer_lens.utils import get_act_name\n",
    "\n",
    "def pos_ablation_hook(value, hook):\n",
    "  value[:, :] = 0.\n",
    "\n",
    "def ablated_logits(model, tokens):\n",
    "  return model.run_with_hooks(tokens,\n",
    "                              fwd_hooks=[(get_act_name('pos_embed'),\n",
    "                                          pos_ablation_hook)])\n",
    "\n",
    "def compute_losses(model, data_loader, loss_dict, ablate_pos=False):\n",
    "  batch_count = 0\n",
    "  device = model.cfg.device\n",
    "  checkpoint_losses = defaultdict(list)\n",
    "  for batch in tqdm(data_loader, total=NUM_BATCHES):\n",
    "    if batch_count >= NUM_BATCHES:\n",
    "      break\n",
    "    batch_count += 1\n",
    "    tokens = batch['tokens'].to(device)\n",
    "    if ablate_pos:\n",
    "      logits = ablated_logits(model, tokens).detach()\n",
    "    else:\n",
    "      logits = model(tokens).detach()\n",
    "    losses = lm_cross_entropy_loss(logits, tokens, per_token=True)\n",
    "    losses = einops.reduce(losses, 'batch pos -> pos', 'mean')\n",
    "    mean_loss = einops.reduce(losses, 'pos ->', 'mean')\n",
    "    checkpoint_losses[-1].append(mean_loss.item()) # save mean loss at -1\n",
    "    for pos in LOSS_POSITIONS:\n",
    "      checkpoint_losses[pos].append(losses[pos].item())\n",
    "  for pos in [-1] + LOSS_POSITIONS:\n",
    "    loss_dict[pos].append(np.mean(checkpoint_losses[pos]))\n",
    "  return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def pos_color(pos):\n",
    "  val = np.log(pos+1) / np.log(1024)\n",
    "  return cm.viridis(val)\n",
    "\n",
    "def plot_losses(loss_dict):\n",
    "  x_steps = np.array(list(range(0, 50001, 100))) + 1\n",
    "  for pos in LOSS_POSITIONS:\n",
    "    vals = loss_dict[pos]\n",
    "    color = pos_color(pos)\n",
    "    plt.plot(x_steps[:len(vals)], vals, alpha=0.4, color=color)\n",
    "  vals = loss_dict[-1]\n",
    "  color = 'tab:red'\n",
    "  plt.plot(x_steps[:len(vals)], vals, color=color)\n",
    "  plt.axvline(x=800, linestyle=':', color='black', alpha=0.5)\n",
    "  plt.axvline(x=6500, linestyle=':', color='black', alpha=0.5)\n",
    "  plt.axvline(x=8500, linestyle=':', color='black', alpha=0.5)\n",
    "  plt.xscale('log')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "x_steps = list(range(0, 50001, 100))\n",
    "\n",
    "L1_losses = defaultdict(list)\n",
    "L1_ablated_losses = defaultdict(list)\n",
    "\n",
    "# one layer model\n",
    "for step in x_steps:\n",
    "  model = load_hf_checkpoint(step, n_layers=1)\n",
    "  L1_losses = compute_losses(model, data_loader, L1_losses, ablate_pos=False)\n",
    "  L1_ablated_losses = compute_losses(model, data_loader, L1_ablated_losses, ablate_pos=True)\n",
    "  \n",
    "\n",
    "L2_losses = defaultdict(list)\n",
    "L2_ablated_losses = defaultdict(list)\n",
    "\n",
    "# two layer model\n",
    "for step in x_steps:\n",
    "  model = load_hf_checkpoint(step, n_layers=1)\n",
    "  L2_losses = compute_losses(model, data_loader, L2_losses, ablate_pos=False)\n",
    "  L2_ablated_losses = compute_losses(model, data_loader, L2_ablated_losses, ablate_pos=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your files somewhere when done!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
