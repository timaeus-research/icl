{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import HookedTransformerConfig\n",
    "from transformer_lens.utils import lm_cross_entropy_loss\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icl.language.model import get_model_cfg\n",
    "from icl.language.utils import load_hf_checkpoint\n",
    "\n",
    "model_cfgs = {}\n",
    "model_cfgs[1] = get_model_cfg(num_layers=1)\n",
    "model_cfgs[2] = get_model_cfg(num_layers=2)\n",
    "\n",
    "model = HookedTransformer(model_cfgs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_models = {}\n",
    "L2_models = {}\n",
    "\n",
    "for step in tqdm(range(0, 50_001, 100)):\n",
    "    L1_models[step] = load_hf_checkpoint(step, n_layers=1)\n",
    "    L2_models[step] = load_hf_checkpoint(step, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model weight norms\n",
    "\n",
    "L1_norms = []\n",
    "for step in tqdm(range(0, 50001, 100)):\n",
    "  model = L1_models[step]\n",
    "  norm = np.sum([np.sum((p.detach().cpu().numpy() ** 2)) for p in model.parameters()]) ** 0.5\n",
    "  L1_norms.append(norm)\n",
    "\n",
    "L2_norms = []\n",
    "for step in tqdm(range(0, 50001, 100)):\n",
    "  model = L2_models[step]\n",
    "  norm = np.sum([np.sum((p.detach().cpu().numpy() ** 2)) for p in model.parameters()]) ** 0.5\n",
    "  L2_norms.append(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca on model positional embedding weights\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def pca_model_weights(model):\n",
    "  W_pos = model.pos_embed.W_pos.detach().cpu().numpy()\n",
    "\n",
    "  pca = PCA(n_components=3)\n",
    "  pca.fit(W_pos)\n",
    "  data = pca.transform(W_pos)\n",
    "  pca_results = {\n",
    "    'pca': pca,\n",
    "    'fit_data': data,\n",
    "    'ex_var': pca.explained_variance_ratio_,\n",
    "  }\n",
    "  return pca_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_results = []\n",
    "L2_results = []\n",
    "\n",
    "for step in range(0, 50001, 100):\n",
    "  L1_results.append(pca_model_weights(L1_models[step]))\n",
    "  L2_results.append(pca_model_weights(L2_models[step]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 norm of positional embedding weights\n",
    "def pos_embed_magnitudes(model):\n",
    "  W_pos = model.pos_embed.W_pos.detach().cpu().numpy()\n",
    "  magnitudes = []\n",
    "  for i in range(1024):\n",
    "    magnitudes.append(np.linalg.norm(W_pos[i, :]))\n",
    "  return np.array(magnitudes)\n",
    "\n",
    "L1_mags = []\n",
    "L2_mags = []\n",
    "for step in range(0, 50001, 100):\n",
    "  L1_mags.append(pos_embed_magnitudes(L1_models[step]))\n",
    "  L2_mags.append(pos_embed_magnitudes(L2_models[step]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
