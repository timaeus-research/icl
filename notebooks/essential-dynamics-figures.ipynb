{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Essential dynamics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv();"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import tqdm\n",
                "import seaborn as sns\n",
                "from sklearn.decomposition import PCA\n",
                "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
                "import matplotlib.patches as mpatches\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from matplotlib.patches import FancyArrowPatch\n",
                "from mpl_toolkits.mplot3d import proj3d\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from sklearn.decomposition import PCA\n",
                "from torch.nn import functional as F\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "# import sys\n",
                "# del sys.modules['icl.figures.colors']\n",
                "# del sys.modules['icl.figures.notation']\n",
                "\n",
                "from icl.analysis.utils import get_unique_run\n",
                "from icl.constants import ANALYSIS, FIGURES, SWEEPS\n",
                "from icl.figures.notation import str_d_dlogt, str_d_dt, str_dlog_dlogt\n",
                "from icl.figures.colors import plot_transitions, gen_transition_colors, get_transition_type, PRIMARY, SECONDARY, TERTIARY, BRED, BBLUE, BRED, BGREEN\n",
                "from icl.constants import DEVICE\n",
                "\n",
                "MODEL_ID = \"Minf1\"\n",
                "LLC_RUN_ID = \"tkudtu1b\"  # TODO: Actually for m = 14\n",
                "\n",
                "sns.set_style('white')\n",
                "DEVICE\n",
                "\n",
                "plt.rcParams['figure.dpi'] = 300"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "NUM_TASKS = 'inf'\n",
                "NUM_LAYERS = 2\n",
                "MAX_LR = 0.003\n",
                "MODEL_SEED = 1\n",
                "\n",
                "# shorthands\n",
                "BATCH_SIZE = 8192\n",
                "K = 8\n",
                "D = 4\n",
                "\n",
                "run = get_unique_run(\n",
                "    str(SWEEPS / \"training-runs/L2H4Minf-1.yaml\"), \n",
                "    task_config={\"num_tasks\": NUM_TASKS, \"num_layers\": NUM_LAYERS, \"model_seed\": MODEL_SEED},\n",
                "    optimizer_config={\"lr\": MAX_LR}\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from infra.utils.iterables import int_linspace\n",
                "from copy import deepcopy\n",
                "from pathlib import Path \n",
                "\n",
                "steps = int_linspace(0, 500_000, 50_000)[::10]\n",
                "steps"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Let's generate these same plots and also look at their evolution. \n",
                "models = []\n",
                "optimizer_state_dicts = []\n",
                "\n",
                "for step in tqdm.tqdm(steps):\n",
                "    checkpoint = run.checkpointer.load_file(step)\n",
                "\n",
                "    m = deepcopy(run.model)\n",
                "    m.load_state_dict(checkpoint[\"model\"])\n",
                "    models.append(m)\n",
                "    optimizer_state_dicts.append(checkpoint[\"optimizer\"])\n",
                "\n",
                "torch.save(models, Path('../checkpoints') / f\"{MODEL_ID}-models.pt\")\n",
                "torch.save(optimizer_state_dicts, Path('../checkpoints') / f\"{MODEL_ID}-optimizer_state_dicts.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(Path('../checkpoints') / f\"{MODEL_ID}-models.pt\", 'rb') as f:\n",
                "    models = torch.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import colorsys\n",
                "from matplotlib.colors import LinearSegmentedColormap\n",
                "\n",
                "TRANSITIONS = [\n",
                "    (0, 1500, 'R1'),\n",
                "    (1500, 40_000, 'R2'),\n",
                "    #(7_500, 40000, 'R3'),\n",
                "    (40000, 320000, 'R3'),\n",
                "    #(120000, 320000, 'R5'),\n",
                "    (320000, 500000, 'R4'),\n",
                "]\n",
                "\n",
                "# def gen_transition_colors(types):\n",
                "#     \"\"\"Generates a palette for transition colors. Orange-flavored for Type A. Blue-flavored for Type B.\"\"\"\n",
                "#     num_type_a = sum([t == \"A\" for t in types])\n",
                "#     num_type_b = sum([t == \"B\" for t in types])\n",
                "#     num_other = sum([t == \"Other\" for t in types])\n",
                "\n",
                "#     type_a_palette = sns.color_palette(\"Oranges_r\", num_type_a)\n",
                "#     type_b_palette = sns.color_palette(\"Blues_r\", num_type_b)\n",
                "#     other_palette = sns.color_palette(\"Greys_r\", num_other)\n",
                "\n",
                "#     palette = []\n",
                "\n",
                "#     for t in types:\n",
                "#         if t == \"A\":\n",
                "#             palette.append(type_a_palette.pop())\n",
                "#         elif t == \"B\":\n",
                "#             palette.append(type_b_palette.pop())\n",
                "#         else:\n",
                "#             palette.append(other_palette.pop())\n",
                "\n",
                "#     return palette\n",
                "\n",
                "\n",
                "def increase_saturation(rgb, saturation_factor):\n",
                "    # Convert RGB to HSV\n",
                "    hsv = colorsys.rgb_to_hsv(*rgb)\n",
                "    \n",
                "    # Increase saturation by the given factor, making sure it stays in [0, 1]\n",
                "    new_s = min(max(hsv[1] * saturation_factor, 0), 1)\n",
                "    \n",
                "    # Convert back to RGB\n",
                "    new_rgb = colorsys.hsv_to_rgb(hsv[0], new_s, hsv[2])\n",
                "    return new_rgb\n",
                "\n",
                "\n",
                "def increase_contrast(rgb, contrast_factor):\n",
                "    # Midpoint\n",
                "    midpoint = 128.0 / 255\n",
                "    \n",
                "    # Increase contrast\n",
                "    new_rgb = [(0.5 + contrast_factor * (component - 0.5)) for component in rgb]\n",
                "    \n",
                "    # Clip to the range [0, 1]\n",
                "    new_rgb = [min(max(component, 0), 1) for component in new_rgb]\n",
                "    return new_rgb\n",
                "\n",
                "\n",
                "\n",
                "transition_types = [get_transition_type(t) for t in TRANSITIONS]\n",
                "transition_colors = gen_transition_colors(transition_types)\n",
                "\n",
                "transition_colors = [increase_saturation(rgb, 2) for rgb in transition_colors]\n",
                "transition_colors = [increase_contrast(rgb, 2) for rgb in transition_colors]\n",
                "\n",
                "transitions_cmap = LinearSegmentedColormap.from_list(\"transitions\", transition_colors)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from icl.analysis.evals import ICLEvaluator\n",
                "\n",
                "FORCE_REEVAL = True\n",
                "BATCH_SIZE = 1024\n",
                "\n",
                "evaluator = ICLEvaluator(\n",
                "    pretrain_dist=run.pretrain_dist,\n",
                "    true_dist=run.true_dist,\n",
                "    max_examples=run.config.task_config.max_examples,\n",
                "    eval_batch_size=BATCH_SIZE,\n",
                "    seed=run.config.task_config.true_seed,   \n",
                ")\n",
                "\n",
                "if os.path.exists(ANALYSIS / f\"{MODEL_ID}_evals_over_time.csv\") and not FORCE_REEVAL:\n",
                "    print(\"Loading evals from file\")\n",
                "    evals_over_time_df = pd.read_csv(ANALYSIS / f\"{MODEL_ID}_evals_over_time.csv\")\n",
                "    evals_over_time = evals_over_time_df.to_dict(\"records\")\n",
                "else:\n",
                "    print(\"Evaluating models\")\n",
                "    evals_over_time = [{**evaluator(model), \"step\": step, \"model_seed\": 5} for step, model in zip(steps, tqdm.tqdm(models))]\n",
                "    evals_over_time_df = pd.DataFrame(evals_over_time)\n",
                "    evals_over_time_df.to_csv(ANALYSIS / f\"{MODEL_ID}_evals_over_time.csv\")\n",
                "\n",
                "evals_over_time_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "with open(ANALYSIS / f\"{MODEL_ID}_evals_over_time.pkl\", 'wb') as f:\n",
                "    pickle.dump(evals_over_time, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "from typing import Optional\n",
                "\n",
                "def get_transition_indices(steps, transitions):\n",
                "    transition_indices = []\n",
                "    for step in steps:\n",
                "        # Find the index of the transition that the current step falls into\n",
                "        index = next((i for i, transition in enumerate(transitions) if transition[0] <= step < transition[1]), None)\n",
                "        transition_indices.append(index if index is not None else -1)\n",
                "\n",
                "    return transition_indices\n",
                "\n",
                "def get_nearest_step(step):\n",
                "    idx = np.argmin(np.abs(np.array(steps) - step))\n",
                "    return steps[idx]\n",
                "\n",
                "def plot_explained_variance(pca, title=\"Explained Variance\", ax: Optional[plt.Axes] = None, num_pca_components=None):\n",
                "    num_pca_components = num_pca_components or len(pca.explained_variance_ratio_)\n",
                "\n",
                "    if ax is None:\n",
                "        fig, ax = plt.subplots(figsize=(15, 8))\n",
                "\n",
                "    ax.bar(range(num_pca_components), pca.explained_variance_ratio_[:num_pca_components])\n",
                "\n",
                "    for i, ratio in enumerate(pca.explained_variance_ratio_[:num_pca_components]):\n",
                "        ax.text(i, ratio, f\"{ratio:.2f}\", fontsize=12, ha='center', va='bottom')\n",
                "\n",
                "    ax.set_title(title)\n",
                "    ax.set_xlabel('PC')\n",
                "    ax.set_ylabel('Explained Variance')\n",
                "\n",
                "    ax.set_xticks(range(num_pca_components), range(1, num_pca_components + 1))\n",
                "\n",
                "\n",
                "def plot_multiple_slices(steps, samples, pca, transitions, highlighted_steps=None, connect_dots=False, palette='tab10', alpha=0.8, save=False, line_color=\"auto\"):\n",
                "    transition_idxs = get_transition_indices(steps, transitions)\n",
                "    # transition_idxs = [(0 if i != 4 else 1) for i in transition_idxs]\n",
                "\n",
                "    # for i in range(1, 5):\n",
                "    #     transition_idxs[-i] = 10  \n",
                "\n",
                "    if highlighted_steps is None:\n",
                "        highlighted_steps = list(map(get_nearest_step, [t[0] for t in transitions][1:]))\n",
                "\n",
                "    num_pca_components = samples.shape[-1]\n",
                "    \n",
                "    # Create a single row of subplots\n",
                "    num_pca_combos = (num_pca_components * (num_pca_components-1)) // 2\n",
                "    \n",
                "    fig, axes = plt.subplots(1, num_pca_combos + 1, figsize=(20, 4))\n",
                "    # fig.suptitle(title)\n",
                "\n",
                "    # Ensure ax is iterable by converting to a list if there's only one subplot\n",
                "    if num_pca_components == 2:\n",
                "        axes = [axes]\n",
                "\n",
                "    I = 0\n",
                "    for i in range(1, num_pca_components):\n",
                "        for j in range(i):\n",
                "\n",
                "            if connect_dots:\n",
                "                axes[I].plot(samples[:, i], samples[:, j], c='black', alpha=0.2)\n",
                "\n",
                "            # sc = axes[I].scatter(samples[:, i], samples[:, j], c=transition_idxs, cmap=cmap, s=50, alpha=alpha)\n",
                "            sns.scatterplot(x=samples[:, i], y=samples[:, j], hue=transition_idxs, palette=palette, s=50, alpha=alpha, ax=axes[I], legend=False)\n",
                "            axes[I].set_xlabel(f'PC {i}')\n",
                "            axes[I].set_ylabel(f'PC {j}')\n",
                "            axes[I].set_title(f'PC {i} vs PC {j}')\n",
                "\n",
                "            # Label some points\n",
                "            total_samples = len(samples)\n",
                "            for step in highlighted_steps:\n",
                "                k = steps.index(step)  # Find the index of the highlighted step\n",
                "                axes[I].text(samples[k, i], samples[k, j], str(step), fontsize=8, ha='right', va='bottom', alpha=0.8)\n",
                "\n",
                "            I += 1\n",
                "\n",
                "    plot_explained_variance(pca, ax=axes[-1], num_pca_components=num_pca_components)\n",
                "    # for I in range( num_pca_combos):\n",
                "    #     axes[I].axis('off')\n",
                "            \n",
                "    # Colorbar for the last plot\n",
                "    # cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])  # Adjust as necessary\n",
                "        # plt.colorbar(sc, cax=cbar_ax, label='Milestones')\n",
                "\n",
                "    cmap = sns.palettes.color_palette(palette, n_colors=len(transitions) + 1)\n",
                "\n",
                "    # Plot the legend on the first subplot on the left\n",
                "    legend_ax = axes[0]\n",
                "    scatter_proxy = [plt.Line2D([0], [0], linestyle='none', marker='o', alpha=alpha, color=cmap[i]) for i in range(len(transitions))]\n",
                "    legend_labels = [label for _, _, label in transitions]\n",
                "    legend_ax.legend(scatter_proxy, legend_labels, loc='center', ncol=1, frameon=False, bbox_to_anchor=(-0.5, 0.5), title='Developmental Stages')\n",
                "    # legend_ax.set_title()\n",
                "\n",
                "    plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust the right side to make room for the colorbar\n",
                "\n",
                "    if save:\n",
                "        parent_dir = os.path.dirname(save)\n",
                "        if not os.path.exists(parent_dir):\n",
                "            os.makedirs(parent_dir)\n",
                "        plt.savefig(save)\n",
                "\n",
                "\n",
                "    fig.set_facecolor('white')\n",
                "    \n",
                "# Usage of the function\n",
                "# Call the function with your data and the list of highlighted steps\n",
                "# plot_multiple_slices(steps, samples, pca, highlighted_steps=[100, 1000, 10000], title=\"Your Title\", num_points_to_label=10, save=\"path/to/save.png\", connect_dots=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Do the PCAs again. \n",
                "from typing import Dict, Iterable, Tuple\n",
                "from sklearn.decomposition import PCA\n",
                "from collections import defaultdict\n",
                "from icl.analysis.hooks import hook\n",
                "import numpy as np\n",
                "from icl.analysis.utils import map_evals_over_checkpoints, get_unique_run\n",
                "from icl.train import Run\n",
                "from infra.utils.tensors import convert_tensor, ReturnTensor\n",
                "from matplotlib import colors as mcolors\n",
                "\n",
                "\n",
                "def extract_activations_over_checkpoints(models: Iterable[nn.Module], xs, ys, *paths, return_type: ReturnTensor=\"np\"):\n",
                "    def eval_activations(model):\n",
                "        model.to(DEVICE)\n",
                "        xs.to(model.device)\n",
                "        ys.to(model.device)\n",
                "        hooked_model = hook(model, *paths)\n",
                "        outputs, activations = hooked_model.run_with_cache(xs, ys)\n",
                "        activations[\"\"] = outputs\n",
                "        return {k: convert_tensor(v, return_type) for k, v in activations.items() if (k in paths or k == \"\") and v is not None}\n",
                "    \n",
                "    for model in models:\n",
                "        yield eval_activations(model)\n",
                "\n",
                "\n",
                "def get_vectorized_activations_trace(models: Iterable[nn.Module], xs, ys, *paths, normalize=False):\n",
                "    evals: Dict[str, list] = defaultdict(list)\n",
                "    \n",
                "    for activations in extract_activations_over_checkpoints(models, xs, ys, *paths):\n",
                "        for path, activation in activations.items():\n",
                "            if normalize:\n",
                "                activation = activation / np.linalg.norm(activation)\n",
                "\n",
                "            evals[path].append(activation)\n",
                "\n",
                "    return {\n",
                "        k: np.array(v) for k, v in evals.items()\n",
                "    }\n",
                "\n",
                "def get_pca_activations_trace(models: Iterable[nn.Module], xs, ys, *paths, num_components=3, normalize=False) -> Dict[str, Tuple[PCA, np.ndarray]]:\n",
                "    results = {}\n",
                "\n",
                "    for path, activations in tqdm.tqdm(get_vectorized_activations_trace(models, xs, ys, *paths, normalize=normalize).items(), total=len(models)):\n",
                "        pca = PCA(n_components=num_components)\n",
                "        activations_reduced = pca.fit_transform(activations)\n",
                "        results[path] = pca, activations_reduced\n",
                "\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_outputs = []\n",
                "only_xs = []\n",
                "only_ys = []\n",
                "all_gradients = []\n",
                "\n",
                "all_running_grads = []\n",
                "all_running_grads_squared = []\n",
                "\n",
                "\n",
                "def get_exp_avg_sq_grads(optimizer_state_dict):\n",
                "    return np.concatenate([g[\"exp_avg_sq\"].cpu().numpy().flatten() for g in optimizer_state_dict[\"state\"].values()])\n",
                "\n",
                "def get_exp_avg_grads(optimizer_state_dict):\n",
                "    return np.concatenate([g[\"exp_avg\"].cpu().numpy().flatten() for g in optimizer_state_dict[\"state\"].values()])\n",
                "\n",
                "\n",
                "# Outputs of token sequence transformer\n",
                "outputs = []\n",
                "only_x_outputs = []\n",
                "only_y_preds = []\n",
                "\n",
                "xs, ys = run.evaluator.pretrain_xs, run.evaluator.pretrain_ys\n",
                "\n",
                "activations_trace = get_vectorized_activations_trace(tqdm.tqdm(models), xs, ys, 'token_sequence_transformer', normalize=False)\n",
                "print({k: v.shape for k, v in activations_trace.items()})\n",
                "\n",
                "activations = activations_trace['token_sequence_transformer']\n",
                "vec_activations = activations.reshape(len(activations), -1)\n",
                "outputs.append(vec_activations)\n",
                "print(activations.shape)\n",
                "only_x_outputs.append(activations[:, :, ::2, :].reshape(len(vec_activations), -1))\n",
                "\n",
                "y_preds = activations_trace[\"\"]\n",
                "only_y_preds.append(y_preds.reshape(len(y_preds), -1))\n",
                "\n",
                "all_outputs.append(np.concatenate(outputs, axis=1))\n",
                "only_xs.append(np.concatenate(only_x_outputs, axis=1))\n",
                "only_ys.append(np.concatenate(only_y_preds, axis=1))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# all_weights = []\n",
                "\n",
                "# # Weights & Gradients\n",
                "# gradients = []\n",
                "weights = []\n",
                "\n",
                "def get_weights_vector(model):\n",
                "    return np.concatenate([param.detach().cpu().numpy().flatten() for name, param in model.named_parameters() if param is not None])\n",
                "\n",
                "def get_gradients_vector(model):\n",
                "    return np.concatenate([param.grad.detach().cpu().numpy().flatten() for name, param in model.named_parameters() if param.grad is not None])\n",
                "\n",
                "for model in tqdm.tqdm(models[::20]):\n",
                "    # model.to(DEVICE)\n",
                "    # train_xs_1.to(DEVICE)\n",
                "    # train_ys_1.to(DEVICE)\n",
                "    # model.train()\n",
                "    # model.zero_grad()\n",
                "    # train_ys_1_pred = model(train_xs_1, train_ys_1)\n",
                "    # loss = F.mse_loss(train_ys_1_pred, train_ys_1)\n",
                "    # loss.backward()\n",
                "\n",
                "    # gradients.append(get_gradients_vector(model))\n",
                "    weights.append(get_weights_vector(model))\n",
                "\n",
                "# all_gradients.append(np.array(gradients))\n",
                "all_weights = np.array(weights)\n",
                "\n",
                "# # Optimizer states\n",
                "# running_grads = []\n",
                "# running_grads_squared = []\n",
                "# for opt_state_dict in opt_state_dicts:\n",
                "#     running_grads.append(get_exp_avg_grads(opt_state_dict))\n",
                "#     running_grads_squared.append(get_exp_avg_sq_grads(opt_state_dict))\n",
                "\n",
                "# all_running_grads.append(np.array(running_grads))\n",
                "# all_running_grads_squared.append(np.array(running_grads_squared))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "\n",
                "def plot_essential_dynamics_grid(steps, all_samples, transitions, palette='tab10', save=False, figsize=(20, 4), num_pca_components=3, max_step=None, normalize=False, labels=None, special=[]):\n",
                "    num_samples = len(all_samples)  \n",
                "\n",
                "    # Create a single row of subplots\n",
                "    num_pca_combos = (num_pca_components * (num_pca_components-1)) // 2\n",
                "    fig, all_axes = plt.subplots(num_samples, num_pca_combos + 1, figsize=figsize)\n",
                "    \n",
                "    if num_samples == 1:\n",
                "        all_axes = [all_axes]\n",
                "\n",
                "    labels = labels or [f\"Model {i+1}\" for i in range(num_samples)]\n",
                "\n",
                "    for samples_idx, _samples in enumerate(tqdm.tqdm(all_samples, desc=\"Plotting...\")):\n",
                "        if max_step is not None:\n",
                "            max_step_idx = steps.index(max_step)\n",
                "            _samples = _samples[:max_step_idx, :]\n",
                "        if normalize:\n",
                "            _samples = _samples / np.linalg.norm(_samples, axis=1, keepdims=True)\n",
                "\n",
                "        pca = PCA(n_components=10 * num_pca_components)\n",
                "        samples = pca.fit_transform(_samples)   \n",
                "\n",
                "        axes = all_axes[samples_idx]\n",
                "\n",
                "        # Ensure ax is iterable by converting to a list if there's only one subplot\n",
                "        if num_pca_components == 2:\n",
                "            axes = [axes]\n",
                "\n",
                "        special_transforms = [\n",
                "            pca.transform(s) for s, _ in special\n",
                "        ]\n",
                "\n",
                "        I = 0\n",
                "        for i in range(1, num_pca_components):\n",
                "            for j in range(i):\n",
                "                sns.scatterplot(x=samples[:, i], y=samples[:, j], ax=axes[I], alpha=0.5, color=\"gray\", s=10, legend=False)\n",
                "                for k, (start, end, stage) in enumerate(transitions):\n",
                "                    start_idx = steps.index(start)\n",
                "                    end_idx = steps.index(end) + 1\n",
                "                        \n",
                "                    # sc = axes[I].scatter(samples[:, i], samples[:, j], c=transition_idxs, cmap=cmap, s=50, alpha=alpha)\n",
                "                    axes[I].plot(samples[start_idx:end_idx, i], samples[start_idx:end_idx, j])\n",
                "\n",
                "                for s, (_, settings) in zip(special_transforms, special):\n",
                "                    style = settings.get(\"style\", 'r+')\n",
                "                    axes[I].plot(s[:, i], s[:, j], style)\n",
                "\n",
                "                if not transitions:\n",
                "                    axes[I].plot(samples[:, i], samples[:, j])\n",
                "\n",
                "                axes[I].set_xlabel(f'PC {i+1}')\n",
                "                axes[I].set_ylabel(f'PC {j+1}')\n",
                "                axes[I].set_title(f'PC {j+1} vs PC {i+1}')\n",
                "\n",
                "                I += 1\n",
                "\n",
                "        axes[0].set_ylabel(f\"{labels[samples_idx]}\\n\\nPC 1\")\n",
                "\n",
                "        plot_explained_variance(pca, ax=axes[-1], num_pca_components=num_pca_components)\n",
                "\n",
                "    # cmap = sns.palettes.color_palette(palette, n_colors=len(transitions) + 1)\n",
                "    # Plot the legend on the first subplot on the left\n",
                "    # legend_ax = axes[0]\n",
                "    # scatter_proxy = [plt.Line2D([0], [0], linestyle='none', marker='o', alpha=alpha, color=cmap[i]) for i in range(len(transitions))]\n",
                "    # legend_labels = [label for _, _, label in transitions]\n",
                "    # legend_ax.legend(scatter_proxy, legend_labels, loc='center', ncol=1, frameon=False, bbox_to_anchor=(-0.5, 0.5), title='Developmental Stages')\n",
                "    # legend_ax.set_title()\n",
                "\n",
                "    # plt.tight_layout(rect=[0, 0, 0.9, 1])  # Adjust the right side to make room for the colorbar\n",
                "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
                "\n",
                "    if transitions:\n",
                "        # Create an axis for the legend\n",
                "        legend_ax = fig.add_axes([0.1, -0.03, 0.95, 0.05])  # Adjust these values as needed\n",
                "\n",
                "        # Create a list of handles for the legend\n",
                "        handles = [plt.Line2D([0], [0], color=sns.color_palette(palette)[i], linestyle='-') for i in range(len(transitions))]\n",
                "        labels = [label for _, _, label in transitions]\n",
                "\n",
                "        # Add legend to the new axis\n",
                "        legend_ax.legend(handles, labels, loc='center', ncol=len(labels), frameon=False)\n",
                "        legend_ax.axis('off')  # Turn off axis lines and labels\n",
                "\n",
                "    if save:\n",
                "        parent_dir = os.path.dirname(save)\n",
                "        if not os.path.exists(parent_dir):\n",
                "            os.makedirs(parent_dir)\n",
                "\n",
                "        with open(save + \".pkl\", \"wb\") as f:\n",
                "            pickle.dump((pca,samples), f)\n",
                "\n",
                "        plt.savefig(save + \".png\")\n",
                "\n",
                "    fig.set_facecolor('white')\n",
                "\n",
                "    return fig"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CLOSEST_TRANSITIONS = [\n",
                "    (get_nearest_step(t0), get_nearest_step(t1), label) for t0, t1, label in TRANSITIONS\n",
                "]\n",
                "print(CLOSEST_TRANSITIONS)\n",
                "steps[10], steps[100], steps[500], steps[1000]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_outputs[0].shape, only_x_outputs[0].shape, only_y_preds.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"../data/ED-proxy.pkl\", \"wb\") as f:\n",
                "    pickle.dump(all_outputs, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "save= \"../data/LMinf1-behavioral-ED\"\n",
                "\n",
                "_all_outputs = only_ys # [all_outputs[0][:, ]] \n",
                "\n",
                "# zero_prediction = np.zeros_like(_all_outputs[0][0:1])\n",
                "special = [\n",
                "    # (zero_prediction, {\"title\": \"Task prior\"}),\n",
                "    # (first_xs.detach().cpu().numpy(), {\"title\": \"First x\", \"style\": \"b+\"}),           \n",
                "    # (prev_xs.detach().cpu().numpy(), {\"title\": \"Previous x\", \"style\": \"g+\"}),\n",
                "    # (cumulative_xs.detach().cpu().numpy(), {\"title\": \"Cumulative x\", \"style\": \"o+\"}),\n",
                "]\n",
                "\n",
                "truncated_transitions = CLOSEST_TRANSITIONS[:1]\n",
                "truncated_transitions[-1] = (0, steps[10-1], 'A')\n",
                "print(truncated_transitions)\n",
                "\n",
                "fig = plot_essential_dynamics_grid(steps[:10], [_all_outputs[0][:10]], truncated_transitions, num_pca_components=4, figsize=(20, 3), special=special)        \n",
                "fig.suptitle(\"First 1k Steps\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "s = 30\n",
                "truncated_transitions = CLOSEST_TRANSITIONS[:2]\n",
                "truncated_transitions[-1] = (1200, steps[s-1], 'B')\n",
                "print(truncated_transitions)\n",
                "\n",
                "fig = plot_essential_dynamics_grid(steps[:s], [_all_outputs[0][:s]], truncated_transitions, num_pca_components=4, figsize=(20, 3), special=special)        \n",
                "fig.suptitle(f\"First {int(s//10)}k Steps\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "truncated_transitions = CLOSEST_TRANSITIONS[:2]\n",
                "\n",
                "truncated_transitions[-1] = (1200, steps[100], 'B')\n",
                "print(truncated_transitions)\n",
                "\n",
                "fig = plot_essential_dynamics_grid(steps[:101], [_all_outputs[0][:101]], truncated_transitions, num_pca_components=4, figsize=(20, 3), special=special)        \n",
                "fig.suptitle(\"First 10k Steps\")\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "truncated_transitions = CLOSEST_TRANSITIONS[:4]\n",
                "truncated_transitions[-1] = (60001, steps[1000], 'D')\n",
                "print(truncated_transitions)\n",
                "\n",
                "# fig = plot_essential_dynamics_grid(steps[:1001], [_all_outputs[0][:1001]], truncated_transitions, num_pca_components=4, figsize=(20, 3), special=special)        \n",
                "# fig.suptitle(\"First 100k Steps\")\n",
                "# plt.tight_layout()\n",
                "# plt.show()\n",
                "\n",
                "fig = plot_essential_dynamics_grid(\n",
                "    steps, _all_outputs, CLOSEST_TRANSITIONS, num_pca_components=4, figsize=(20, 4), \n",
                "    special=special, \n",
                "    save=save\n",
                ")        \n",
                "fig.suptitle(\"All Steps\")\n",
                "plt.tight_layout() \n",
                "plt.show()\n",
                "# plot_multiple_slices(\n",
                "#     steps[:MAX_PCA_STEP], \n",
                "#     exp_avg_grads_reduced[:, :3], \n",
                "#     pca_1, \n",
                "#     TRANSITIONS[:4],\n",
                "#     connect_dots=True, \n",
                "#     save=None,\n",
                "#     # cmap=transitions_cmap\n",
                "# )\n",
                "# plt.suptitle(\"Essential Dynamics of Exponentially Averaged Square Gradients\")\n",
                "# plt.tight_layout()\n",
                "# plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "fig = plot_essential_dynamics_grid(\n",
                "    steps, [all_weights], CLOSEST_TRANSITIONS, num_pca_components=4, figsize=(20, 4), \n",
                "    special=special, \n",
                "    save=\"../data/Minf1-ED-weight\"\n",
                ")        \n",
                "# fig.suptitle(\"All Steps\")\n",
                "plt.tight_layout() "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.15"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
